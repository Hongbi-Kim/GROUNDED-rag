{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a2b37b67",
      "metadata": {},
      "source": [
        "# 02 Build Graph Payload (Hierarchical Nodes)\n",
        "\n",
        "- Source scope: `001823(건축법)`, `002118(건축법 시행령)`\n",
        "- Node hierarchy: `Law -> Article -> Paragraph`\n",
        "- Reference edge: `REF` (target can be Law / Article / Paragraph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe3f1af",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "SOURCE_LAW_IDS = {'001823', '002118'}\n",
        "\n",
        "\n",
        "def resolve_processed_root() -> Path:\n",
        "    candidates = [\n",
        "        Path('notebooks/research_mvp/data/processed'),\n",
        "        Path('../research_mvp/data/processed'),\n",
        "        Path('research_mvp/data/processed'),\n",
        "    ]\n",
        "    for c in candidates:\n",
        "        if c.exists():\n",
        "            return c\n",
        "    raise FileNotFoundError('processed root not found')\n",
        "\n",
        "\n",
        "PROCESSED = resolve_processed_root()\n",
        "OUT_DIR = Path('notebooks/graphrag_mvp/data')\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CHUNKS_DIR = PROCESSED / 'chunks'\n",
        "REF_MAP_DIR = PROCESSED / 'chunks_golden' / 'chunk_ref_map'\n",
        "\n",
        "print('PROCESSED =', PROCESSED)\n",
        "print('CHUNKS_DIR =', CHUNKS_DIR, CHUNKS_DIR.exists())\n",
        "print('REF_MAP_DIR =', REF_MAP_DIR, REF_MAP_DIR.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b1e39a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_source_chunks(chunks_dir: Path, source_law_ids: set[str]):\n",
        "    rows = []\n",
        "    for f in sorted(chunks_dir.glob('*_chunks.json')):\n",
        "        arr = json.loads(f.read_text(encoding='utf-8'))\n",
        "        for x in arr:\n",
        "            law_id = str(x.get('law_id', '')).strip()\n",
        "            if law_id in source_law_ids:\n",
        "                rows.append(x)\n",
        "    return rows\n",
        "\n",
        "\n",
        "def load_ref_map_for_sources(ref_map_dir: Path, source_law_ids: set[str]):\n",
        "    out = {}\n",
        "    for law_id in source_law_ids:\n",
        "        p = ref_map_dir / f'{law_id}.json'\n",
        "        if p.exists():\n",
        "            obj = json.loads(p.read_text(encoding='utf-8'))\n",
        "            if isinstance(obj, dict):\n",
        "                out.update(obj)\n",
        "    return out\n",
        "\n",
        "\n",
        "chunks = load_source_chunks(CHUNKS_DIR, SOURCE_LAW_IDS)\n",
        "ref_map = load_ref_map_for_sources(REF_MAP_DIR, SOURCE_LAW_IDS)\n",
        "\n",
        "print('source chunks:', len(chunks))\n",
        "print('ref map keys:', len(ref_map))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c41f8cf6",
      "metadata": {},
      "outputs": [],
      "source": [
        "CIRCLED_NUM_MAP = {\n",
        "    '①': '1', '②': '2', '③': '3', '④': '4', '⑤': '5',\n",
        "    '⑥': '6', '⑦': '7', '⑧': '8', '⑨': '9', '⑩': '10',\n",
        "}\n",
        "\n",
        "\n",
        "def norm_para_num(v) -> str:\n",
        "    s = str(v or '').strip()\n",
        "    if not s:\n",
        "        return '0'\n",
        "    if s in CIRCLED_NUM_MAP:\n",
        "        return CIRCLED_NUM_MAP[s]\n",
        "    digits = ''.join(ch for ch in s if ch.isdigit())\n",
        "    return digits if digits else s\n",
        "\n",
        "\n",
        "def normalize_text(v) -> str:\n",
        "    return str(v or '').strip()\n",
        "\n",
        "\n",
        "def normalize_name_key(name: str) -> str:\n",
        "    # law_id 미확보 ref 법령을 위한 안정 키\n",
        "    s = normalize_text(name)\n",
        "    if not s:\n",
        "        return ''\n",
        "    return 'name::' + s.lower().replace(' ', '_')\n",
        "\n",
        "\n",
        "def make_article_key(law_key: str, article_num: str, article_sub: str = '0') -> str:\n",
        "    sub = normalize_text(article_sub) or '0'\n",
        "    return f\"{law_key}:{normalize_text(article_num)}:{sub}\"\n",
        "\n",
        "\n",
        "def make_paragraph_key(law_key: str, article_num: str, article_sub: str, para_num: str) -> str:\n",
        "    return f\"{law_key}:{normalize_text(article_num)}:{normalize_text(article_sub) or '0'}:{norm_para_num(para_num)}\"\n",
        "\n",
        "\n",
        "def normalize_ref_articles(value):\n",
        "    # article 값이 int/str/list일 수 있음\n",
        "    if value is None:\n",
        "        return []\n",
        "    if isinstance(value, list):\n",
        "        out=[]\n",
        "        for x in value:\n",
        "            s=normalize_text(x)\n",
        "            if s:\n",
        "                out.append(s)\n",
        "        return out\n",
        "    s = normalize_text(value)\n",
        "    return [s] if s else []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95125c1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "nodes = {}\n",
        "edges = []\n",
        "\n",
        "\n",
        "def upsert_node(node_id: str, label: str, props: dict):\n",
        "    if node_id not in nodes:\n",
        "        nodes[node_id] = {'id': node_id, 'label': label, 'props': dict(props)}\n",
        "    else:\n",
        "        # 빈값은 기존값 보존, 신규 유효값만 채움\n",
        "        old = nodes[node_id]['props']\n",
        "        for k,v in props.items():\n",
        "            if k not in old or old.get(k) in [None, '']:\n",
        "                old[k] = v\n",
        "\n",
        "\n",
        "def add_edge(src: str, rel: str, dst: str, props=None):\n",
        "    edges.append({'from': src, 'type': rel, 'to': dst, 'props': props or {}})\n",
        "\n",
        "\n",
        "def ensure_law_node(law_key: str, law_id: str, law_name: str, law_type: str = ''):\n",
        "    upsert_node(f'LAW:{law_key}', 'Law', {\n",
        "        'law_key': law_key,\n",
        "        'law_id': normalize_text(law_id),\n",
        "        'law_name': normalize_text(law_name),\n",
        "        'law_type': normalize_text(law_type),\n",
        "    })\n",
        "\n",
        "\n",
        "def ensure_article_node(law_key: str, law_id: str, law_name: str, article_num: str, article_sub: str = '0', article_title: str = '', content: str = ''):\n",
        "    akey = make_article_key(law_key, article_num, article_sub)\n",
        "    upsert_node(f'ART:{akey}', 'Article', {\n",
        "        'article_key': akey,\n",
        "        'law_key': law_key,\n",
        "        'law_id': normalize_text(law_id),\n",
        "        'law_name': normalize_text(law_name),\n",
        "        'article_num': normalize_text(article_num),\n",
        "        'article_sub': normalize_text(article_sub) or '0',\n",
        "        'article_title': normalize_text(article_title),\n",
        "        'content': normalize_text(content),\n",
        "    })\n",
        "    add_edge(f'LAW:{law_key}', 'HAS_ARTICLE', f'ART:{akey}')\n",
        "    return akey\n",
        "\n",
        "\n",
        "def ensure_paragraph_node(law_key: str, law_id: str, law_name: str, article_num: str, article_sub: str, para_num: str, content: str = '', is_placeholder: bool = False):\n",
        "    pkey = make_paragraph_key(law_key, article_num, article_sub, para_num)\n",
        "    upsert_node(f'PARA:{pkey}', 'Paragraph', {\n",
        "        'paragraph_key': pkey,\n",
        "        'law_key': law_key,\n",
        "        'law_id': normalize_text(law_id),\n",
        "        'law_name': normalize_text(law_name),\n",
        "        'article_num': normalize_text(article_num),\n",
        "        'article_sub': normalize_text(article_sub) or '0',\n",
        "        'paragraph_num': norm_para_num(para_num),\n",
        "        'content': normalize_text(content),\n",
        "        'is_ref_placeholder': bool(is_placeholder),\n",
        "    })\n",
        "    akey = make_article_key(law_key, article_num, article_sub)\n",
        "    add_edge(f'ART:{akey}', 'HAS_PARAGRAPH', f'PARA:{pkey}')\n",
        "    return pkey\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "114f1539",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) source 법령 본문으로 Law/Article/Paragraph 생성\n",
        "law_key_by_name = {}\n",
        "law_key_by_id = {}\n",
        "\n",
        "for c in chunks:\n",
        "    law_id = normalize_text(c.get('law_id'))\n",
        "    law_name = normalize_text(c.get('law_name'))\n",
        "    law_type = normalize_text(c.get('law_type'))\n",
        "    article_num = normalize_text(c.get('article_num'))\n",
        "    article_sub = normalize_text(c.get('article_sub')) or '0'\n",
        "\n",
        "    law_key = law_id\n",
        "    law_key_by_id[law_id] = law_key\n",
        "    if law_name:\n",
        "        law_key_by_name[law_name] = law_key\n",
        "\n",
        "    ensure_law_node(law_key, law_id, law_name, law_type)\n",
        "    ensure_article_node(\n",
        "        law_key=law_key,\n",
        "        law_id=law_id,\n",
        "        law_name=law_name,\n",
        "        article_num=article_num,\n",
        "        article_sub=article_sub,\n",
        "        article_title=normalize_text(c.get('article_title')),\n",
        "        content=normalize_text(c.get('content')),\n",
        "    )\n",
        "\n",
        "    paras = c.get('paragraphs', [])\n",
        "    if isinstance(paras, list) and paras:\n",
        "        for p in paras:\n",
        "            if not isinstance(p, dict):\n",
        "                continue\n",
        "            ensure_paragraph_node(\n",
        "                law_key=law_key,\n",
        "                law_id=law_id,\n",
        "                law_name=law_name,\n",
        "                article_num=article_num,\n",
        "                article_sub=article_sub,\n",
        "                para_num=normalize_text(p.get('num')) or '0',\n",
        "                content=normalize_text(p.get('content')),\n",
        "                is_placeholder=False,\n",
        "            )\n",
        "    else:\n",
        "        ensure_paragraph_node(\n",
        "            law_key=law_key,\n",
        "            law_id=law_id,\n",
        "            law_name=law_name,\n",
        "            article_num=article_num,\n",
        "            article_sub=article_sub,\n",
        "            para_num='0',\n",
        "            content=normalize_text(c.get('content')),\n",
        "            is_placeholder=False,\n",
        "        )\n",
        "\n",
        "print('after source graph -> nodes:', len(nodes), 'edges:', len(edges))\n",
        "print('known laws by id:', len(law_key_by_id), 'by name:', len(law_key_by_name))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6b4f84d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) REF 생성 (target level: Law / Article / Paragraph)\n",
        "\n",
        "def pick_law_key_from_ref(ref_law_name: str, src_law_key: str):\n",
        "    name = normalize_text(ref_law_name)\n",
        "    if not name:\n",
        "        return src_law_key, '', ''\n",
        "    if name in law_key_by_name:\n",
        "        key = law_key_by_name[name]\n",
        "        return key, key, name\n",
        "    # source 스코프 밖 법령 -> synthetic law_key 생성\n",
        "    skey = normalize_name_key(name)\n",
        "    return skey, '', name\n",
        "\n",
        "\n",
        "ref_edges = 0\n",
        "for c in chunks:\n",
        "    src_law_key = normalize_text(c.get('law_id'))\n",
        "    src_law_id = normalize_text(c.get('law_id'))\n",
        "    src_law_name = normalize_text(c.get('law_name'))\n",
        "    src_article = normalize_text(c.get('article_num'))\n",
        "    src_sub = normalize_text(c.get('article_sub')) or '0'\n",
        "    src_art_key = make_article_key(src_law_key, src_article, src_sub)\n",
        "\n",
        "    refs = ref_map.get(make_article_key(src_law_key, src_article, src_sub), {}) or {}\n",
        "    internal_refs = refs.get('internal_refs', []) if isinstance(refs.get('internal_refs', []), list) else []\n",
        "    external_refs = refs.get('external_refs', []) if isinstance(refs.get('external_refs', []), list) else []\n",
        "\n",
        "    for scope, ref_list in [('internal', internal_refs), ('external', external_refs)]:\n",
        "        for r in ref_list:\n",
        "            if not isinstance(r, dict):\n",
        "                continue\n",
        "\n",
        "            law_key, target_law_id_known, target_law_name = pick_law_key_from_ref(r.get('law_name', ''), src_law_key)\n",
        "            if not target_law_name:\n",
        "                target_law_name = src_law_name\n",
        "\n",
        "            # law node 보장\n",
        "            ensure_law_node(\n",
        "                law_key=law_key,\n",
        "                law_id=target_law_id_known,\n",
        "                law_name=target_law_name,\n",
        "                law_type='',\n",
        "            )\n",
        "\n",
        "            article_candidates = normalize_ref_articles(r.get('article'))\n",
        "            para_val = normalize_text(r.get('paragraph')) or '0'\n",
        "\n",
        "            if not article_candidates:\n",
        "                # 법-only ref -> target: Law\n",
        "                add_edge(\n",
        "                    f'ART:{src_art_key}',\n",
        "                    'REF',\n",
        "                    f'LAW:{law_key}',\n",
        "                    {\n",
        "                        'scope': scope,\n",
        "                        'raw': normalize_text(r.get('raw')),\n",
        "                        'item': normalize_text(r.get('item')),\n",
        "                        'target_level': 'law',\n",
        "                    },\n",
        "                )\n",
        "                ref_edges += 1\n",
        "                continue\n",
        "\n",
        "            for art in article_candidates:\n",
        "                # article node 보장(내용 미확보시 빈값)\n",
        "                tgt_art_key = ensure_article_node(\n",
        "                    law_key=law_key,\n",
        "                    law_id=target_law_id_known,\n",
        "                    law_name=target_law_name,\n",
        "                    article_num=art,\n",
        "                    article_sub=normalize_text(r.get('article_sub')) or '0',\n",
        "                    article_title='',\n",
        "                    content='',\n",
        "                )\n",
        "\n",
        "                if para_val and para_val != '0':\n",
        "                    # paragraph ref\n",
        "                    tgt_para_key = ensure_paragraph_node(\n",
        "                        law_key=law_key,\n",
        "                        law_id=target_law_id_known,\n",
        "                        law_name=target_law_name,\n",
        "                        article_num=art,\n",
        "                        article_sub=normalize_text(r.get('article_sub')) or '0',\n",
        "                        para_num=para_val,\n",
        "                        content='',\n",
        "                        is_placeholder=True,\n",
        "                    )\n",
        "                    add_edge(\n",
        "                        f'ART:{src_art_key}',\n",
        "                        'REF',\n",
        "                        f'PARA:{tgt_para_key}',\n",
        "                        {\n",
        "                            'scope': scope,\n",
        "                            'raw': normalize_text(r.get('raw')),\n",
        "                            'item': normalize_text(r.get('item')),\n",
        "                            'target_level': 'paragraph',\n",
        "                        },\n",
        "                    )\n",
        "                else:\n",
        "                    # article ref\n",
        "                    add_edge(\n",
        "                        f'ART:{src_art_key}',\n",
        "                        'REF',\n",
        "                        f'ART:{tgt_art_key}',\n",
        "                        {\n",
        "                            'scope': scope,\n",
        "                            'raw': normalize_text(r.get('raw')),\n",
        "                            'item': normalize_text(r.get('item')),\n",
        "                            'target_level': 'article',\n",
        "                        },\n",
        "                    )\n",
        "                ref_edges += 1\n",
        "\n",
        "print('ref edges added:', ref_edges)\n",
        "print('final nodes:', len(nodes), 'final edges:', len(edges))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1b93a4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_payload = {\n",
        "    'meta': {\n",
        "        'source_law_ids': sorted(SOURCE_LAW_IDS),\n",
        "        'node_labels': ['Law', 'Article', 'Paragraph'],\n",
        "        'edge_types': ['HAS_ARTICLE', 'HAS_PARAGRAPH', 'REF'],\n",
        "    },\n",
        "    'nodes': list(nodes.values()),\n",
        "    'edges': edges,\n",
        "    'stats': {\n",
        "        'node_count': len(nodes),\n",
        "        'edge_count': len(edges),\n",
        "    }\n",
        "}\n",
        "\n",
        "out_path = OUT_DIR / 'legal_graph_payload_ref_only.json'\n",
        "out_path.write_text(json.dumps(graph_payload, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "print('saved:', out_path)\n",
        "print('stats:', graph_payload['stats'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75d82cd1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa126dab",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39a9eb79",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "966780df",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1ae50bb",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c145539",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}