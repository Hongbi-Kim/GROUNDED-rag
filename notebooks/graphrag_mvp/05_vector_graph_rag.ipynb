{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05 Vector + Graph RAG (NAVER)\n",
        "\n",
        "`langchain_naver` 기반으로 Vector + Graph RAG를 실험합니다.\n",
        "- Embedding: `ClovaXEmbeddings(model='bge-m3')`\n",
        "- Generation: `ChatClovaX(model='HCX-005')`\n",
        "- Graph: Neo4j `Paragraph` 노드 + `REF` 관계\n",
        "\n",
        "실행 순서:\n",
        "1. 연결/모델 초기화\n",
        "2. Paragraph 임베딩 저장 (`embedding` 속성)\n",
        "3. vector index 생성\n",
        "4. vector + ref 검색\n",
        "5. Clova 답변 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa979b48",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from neo4j import GraphDatabase\n",
        "from langchain_naver import ClovaXEmbeddings, ChatClovaX\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "URI = os.getenv('NEO4J_URI', '').strip()\n",
        "USER = os.getenv('NEO4J_USER', 'neo4j').strip()\n",
        "PWD = os.getenv('NEO4J_PASSWORD', '').strip()\n",
        "DB = os.getenv('NEO4J_DATABASE', 'neo4j').strip()\n",
        "\n",
        "if not URI:\n",
        "    raise ValueError('NEO4J_URI is empty')\n",
        "if not PWD:\n",
        "    raise ValueError('NEO4J_PASSWORD is empty')\n",
        "\n",
        "driver = GraphDatabase.driver(URI, auth=(USER, PWD))\n",
        "driver.verify_connectivity()\n",
        "# print('neo4j connected:', URI, 'db:', DB)\n",
        "\n",
        "embedder = ClovaXEmbeddings(model='bge-m3')\n",
        "llm = ChatClovaX(model='HCX-005', temperature=0.0, max_tokens=1200)\n",
        "# print('models ready: bge-m3 + HCX-005')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5423981d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_query(cypher: str, **params):\n",
        "    records, summary, keys = driver.execute_query(cypher, database_=DB, **params)\n",
        "    return records, summary, keys\n",
        "\n",
        "\n",
        "records, _, _ = run_query('MATCH (p:Paragraph) RETURN count(p) AS c')\n",
        "print('Paragraph count:', records[0]['c'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d4f8976",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1) 임베딩 대상 조회 (resume-safe)\n",
        "\n",
        "LIMIT_PARAGRAPHS = None   # 테스트: 100\n",
        "ONLY_MISSING = True       # True면 embedding 없는 노드만 처리\n",
        "\n",
        "# property key가 아직 없을 때 경고를 줄이기 위한 분기\n",
        "prop_records, _, _ = run_query('CALL db.propertyKeys() YIELD propertyKey RETURN collect(propertyKey) AS keys')\n",
        "prop_keys = set(prop_records[0]['keys'] or [])\n",
        "\n",
        "base_q = 'MATCH (p:Paragraph) WHERE p.content IS NOT NULL AND trim(p.content) <> \"\" '\n",
        "if ONLY_MISSING and 'embedding' in prop_keys:\n",
        "    base_q += 'AND p.embedding IS NULL '\n",
        "base_q += 'RETURN p.paragraph_key AS paragraph_key, p.content AS content '\n",
        "if LIMIT_PARAGRAPHS is not None:\n",
        "    base_q += 'LIMIT $n'\n",
        "\n",
        "params = {'n': LIMIT_PARAGRAPHS} if LIMIT_PARAGRAPHS is not None else {}\n",
        "records, summary, _ = run_query(base_q, **params)\n",
        "rows = [r.data() for r in records]\n",
        "\n",
        "print('embedding property exists:', 'embedding' in prop_keys)\n",
        "print('target paragraphs:', len(rows), 'time(ms)=', summary.result_available_after)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f836bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional) Graph reset cell (주의)\n",
        "# 기본값 False: 실수 방지\n",
        "RESET_GRAPH = False\n",
        "\n",
        "if RESET_GRAPH:\n",
        "    # 이 프로젝트 스키마(Document/Article/Paragraph + REF)만 삭제\n",
        "    run_query('MATCH (a:Paragraph)-[r:REF]->(b:Paragraph) DELETE r')\n",
        "    run_query('MATCH (n:Paragraph) DETACH DELETE n')\n",
        "    run_query('MATCH (n:Article) DETACH DELETE n')\n",
        "    run_query('MATCH (n:Document) DETACH DELETE n')\n",
        "\n",
        "    # 인덱스 정리(있을 때만)\n",
        "    try:\n",
        "        run_query('DROP INDEX paragraph_embedding_index IF EXISTS')\n",
        "    except Exception as e:\n",
        "        print('index drop warning:', e)\n",
        "\n",
        "    print('Graph reset done.')\n",
        "else:\n",
        "    print('RESET_GRAPH=False -> skip reset')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b31ae310",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2) 임베딩 생성 + 저장 (429 retry + skip existing)\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "SLEEP_SEC = 10\n",
        "MAX_RETRIES = 3\n",
        "BASE_BACKOFF = 1.0\n",
        "\n",
        "\n",
        "def is_rate_limited(err: Exception) -> bool:\n",
        "    msg = str(err)\n",
        "    return ('429' in msg) or ('rate exceeded' in msg.lower()) or ('RateLimitError' in msg)\n",
        "\n",
        "\n",
        "def missing_keys_in_db(keys: list[str]) -> set[str]:\n",
        "    if not keys:\n",
        "        return set()\n",
        "    recs, _, _ = run_query(\n",
        "        'UNWIND $keys AS k '\n",
        "        'MATCH (p:Paragraph {paragraph_key:k}) '\n",
        "        'WHERE p.embedding IS NULL AND p.content IS NOT NULL AND trim(p.content) <> \"\" '\n",
        "        'RETURN p.paragraph_key AS paragraph_key',\n",
        "        keys=keys,\n",
        "    )\n",
        "    return {r['paragraph_key'] for r in recs}\n",
        "\n",
        "\n",
        "def upsert_embeddings(batch_rows, vectors):\n",
        "    payload = []\n",
        "    for r, v in zip(batch_rows, vectors):\n",
        "        payload.append({'paragraph_key': r['paragraph_key'], 'embedding': v})\n",
        "\n",
        "    cypher = (\n",
        "        'UNWIND $rows AS r '\n",
        "        'MATCH (p:Paragraph {paragraph_key: r.paragraph_key}) '\n",
        "        'SET p.embedding = coalesce(p.embedding, r.embedding)'\n",
        "    )\n",
        "\n",
        "    last_err = None\n",
        "    for attempt in range(MAX_RETRIES + 1):\n",
        "        try:\n",
        "            _, summary, _ = run_query(cypher, rows=payload)\n",
        "            return summary\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            if is_rate_limited(e) and attempt < MAX_RETRIES:\n",
        "                wait = min(BASE_BACKOFF * (2 ** attempt), 30.0)\n",
        "                print(f'upsert rate-limited, retry {attempt+1}/{MAX_RETRIES}, sleep={wait:.1f}s')\n",
        "                time.sleep(wait)\n",
        "                continue\n",
        "            raise last_err\n",
        "\n",
        "\n",
        "def embed_with_retry(texts: list[str]):\n",
        "    last_err = None\n",
        "    for attempt in range(MAX_RETRIES + 1):\n",
        "        try:\n",
        "            return embedder.embed_documents(texts)\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            if is_rate_limited(e) and attempt < MAX_RETRIES:\n",
        "                wait = min(BASE_BACKOFF * (2 ** attempt), 30.0)\n",
        "                print(f'embed rate-limited, retry {attempt+1}/{MAX_RETRIES}, sleep={wait:.1f}s')\n",
        "                time.sleep(wait)\n",
        "                continue\n",
        "            raise last_err\n",
        "\n",
        "\n",
        "written = 0\n",
        "skipped_existing = 0\n",
        "failed_batches = []\n",
        "\n",
        "for i in range(0, len(rows), BATCH_SIZE):\n",
        "    batch = rows[i:i + BATCH_SIZE]\n",
        "    keys = [str(x.get('paragraph_key', '')) for x in batch]\n",
        "\n",
        "    # 배치마다 DB 재확인 -> 이미 저장된 건 임베딩 호출 전에 pass\n",
        "    missing = missing_keys_in_db(keys)\n",
        "    run_batch = [x for x in batch if x.get('paragraph_key') in missing]\n",
        "\n",
        "    if not run_batch:\n",
        "        skipped_existing += len(batch)\n",
        "        print(f'batch {i//BATCH_SIZE+1}: skip all (already embedded)')\n",
        "        continue\n",
        "\n",
        "    texts = [str(x.get('content', '') or '') for x in run_batch]\n",
        "\n",
        "    try:\n",
        "        vectors = embed_with_retry(texts)\n",
        "        summary = upsert_embeddings(run_batch, vectors)\n",
        "        written += len(run_batch)\n",
        "        skipped_existing += (len(batch) - len(run_batch))\n",
        "        print(f'embedding write: +{len(run_batch)} (total_written={written}), skipped_existing={skipped_existing}, time(ms)={summary.result_available_after}')\n",
        "    except Exception as e:\n",
        "        failed_batches.append({'batch_index': i//BATCH_SIZE+1, 'error': str(e)[:500], 'size': len(run_batch)})\n",
        "        print(f'[warn] batch {i//BATCH_SIZE+1} failed:', e)\n",
        "\n",
        "    time.sleep(SLEEP_SEC)\n",
        "\n",
        "print('embedding done')\n",
        "print('written:', written)\n",
        "print('skipped_existing:', skipped_existing)\n",
        "print('failed_batches:', len(failed_batches))\n",
        "if failed_batches:\n",
        "    print('sample failed:', failed_batches[:3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59aebfbb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3) vector index 생성\n",
        "\n",
        "INDEX_NAME = 'paragraph_embedding_index'\n",
        "EMBEDDING_DIM = 1024\n",
        "SIMILARITY = 'cosine'\n",
        "\n",
        "\n",
        "def ensure_vector_index(index_name: str, dimensions: int, similarity: str):\n",
        "    # Neo4j 5.x 문법 우선\n",
        "    try:\n",
        "        cypher = (\n",
        "            f'CREATE VECTOR INDEX {index_name} IF NOT EXISTS '\n",
        "            'FOR (p:Paragraph) ON (p.embedding) '\n",
        "            'OPTIONS {indexConfig: {'\n",
        "            '`vector.dimensions`: $dims, '\n",
        "            '`vector.similarity_function`: $sim'\n",
        "            '}}'\n",
        "        )\n",
        "        _, summary, _ = run_query(cypher, dims=dimensions, sim=similarity)\n",
        "        print('vector index ensured (CREATE VECTOR INDEX), time(ms)=', summary.result_available_after)\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print('create vector index failed, fallback to procedure:', e)\n",
        "\n",
        "    # 구버전 fallback\n",
        "    try:\n",
        "        _, summary, _ = run_query(\n",
        "            'CALL db.index.vector.createNodeIndex($name, $label, $prop, $dims, $sim)',\n",
        "            name=index_name,\n",
        "            label='Paragraph',\n",
        "            prop='embedding',\n",
        "            dims=dimensions,\n",
        "            sim=similarity,\n",
        "        )\n",
        "        print('vector index ensured (procedure), time(ms)=', summary.result_available_after)\n",
        "    except Exception as e:\n",
        "        print('fallback also failed:', e)\n",
        "        raise\n",
        "\n",
        "\n",
        "ensure_vector_index(INDEX_NAME, EMBEDDING_DIM, SIMILARITY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74f32c8f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4) Vector + Graph retrieval\n",
        "\n",
        "QUERY = '이 조건에 맞는 건축선을 알려줘'\n",
        "TOP_K = 5\n",
        "MAX_REF_HOPS = 1   # 1~2 권장\n",
        "\n",
        "\n",
        "def vector_ref_retrieve(query_text: str, top_k: int = 5, hops: int = 1):\n",
        "    qvec = embedder.embed_query(query_text)\n",
        "\n",
        "    # seed: paragraph vector search\n",
        "    seed_q = (\n",
        "        'CALL db.index.vector.queryNodes($index_name, $k, $query_vec) '\n",
        "        'YIELD node, score '\n",
        "        'RETURN node.paragraph_key AS paragraph_key, node.law_name AS law_name, '\n",
        "        'node.article_num AS article_num, node.paragraph_num AS paragraph_num, '\n",
        "        'node.content AS content, score'\n",
        "    )\n",
        "    seed_records, _, _ = run_query(seed_q, index_name=INDEX_NAME, k=top_k, query_vec=qvec)\n",
        "    seeds = [r.data() for r in seed_records]\n",
        "    if not seeds:\n",
        "        return {'seeds': [], 'ref_edges': [], 'contexts': []}\n",
        "\n",
        "    seed_para_keys = [s['paragraph_key'] for s in seeds]\n",
        "\n",
        "    # seed paragraph -> parent article\n",
        "    parent_q = (\n",
        "        'UNWIND $keys AS k '\n",
        "        'MATCH (a:Article)-[:HAS_PARAGRAPH]->(p:Paragraph {paragraph_key:k}) '\n",
        "        'RETURN DISTINCT a.article_key AS article_key'\n",
        "    )\n",
        "    parent_records, _, _ = run_query(parent_q, keys=seed_para_keys)\n",
        "    frontier = set([r['article_key'] for r in parent_records])\n",
        "    seen_articles = set(frontier)\n",
        "\n",
        "    ref_edges = []\n",
        "    target_article_keys = set()\n",
        "    target_paragraph_keys = set(seed_para_keys)\n",
        "\n",
        "    for _ in range(hops):\n",
        "        if not frontier:\n",
        "            break\n",
        "        ref_q = (\n",
        "            'UNWIND $keys AS k '\n",
        "            'MATCH (a:Article {article_key:k})-[r:REF]->(b) '\n",
        "            'RETURN a.article_key AS from_article, labels(b) AS to_labels, '\n",
        "            'coalesce(b.article_key, b.paragraph_key, b.law_key) AS to_key, '\n",
        "            'r.scope AS scope, r.raw AS raw, r.target_level AS target_level'\n",
        "        )\n",
        "        recs, _, _ = run_query(ref_q, keys=list(frontier))\n",
        "\n",
        "        next_frontier = set()\n",
        "        for rec in recs:\n",
        "            x = rec.data()\n",
        "            ref_edges.append(x)\n",
        "            labels = x.get('to_labels', [])\n",
        "            key = x.get('to_key')\n",
        "            if not key:\n",
        "                continue\n",
        "            if 'Article' in labels:\n",
        "                target_article_keys.add(key)\n",
        "                if key not in seen_articles:\n",
        "                    seen_articles.add(key)\n",
        "                    next_frontier.add(key)\n",
        "            elif 'Paragraph' in labels:\n",
        "                target_paragraph_keys.add(key)\n",
        "        frontier = next_frontier\n",
        "\n",
        "    # article targets -> paragraph context 확장\n",
        "    if target_article_keys:\n",
        "        recs, _, _ = run_query(\n",
        "            'UNWIND $keys AS k '\n",
        "            'MATCH (a:Article {article_key:k})-[:HAS_PARAGRAPH]->(p:Paragraph) '\n",
        "            'RETURN p.paragraph_key AS paragraph_key',\n",
        "            keys=list(target_article_keys),\n",
        "        )\n",
        "        for r in recs:\n",
        "            target_paragraph_keys.add(r['paragraph_key'])\n",
        "\n",
        "    # context fetch\n",
        "    ctx_q = (\n",
        "        'UNWIND $keys AS k '\n",
        "        'MATCH (p:Paragraph {paragraph_key:k}) '\n",
        "        'RETURN p.paragraph_key AS paragraph_key, p.law_name AS law_name, p.article_num AS article_num, '\n",
        "        'p.paragraph_num AS paragraph_num, p.content AS content'\n",
        "    )\n",
        "    ctx_records, _, _ = run_query(ctx_q, keys=list(target_paragraph_keys))\n",
        "    contexts = [r.data() for r in ctx_records if str(r.get('content','')).strip()]\n",
        "\n",
        "    return {'seeds': seeds, 'ref_edges': ref_edges, 'contexts': contexts}\n",
        "\n",
        "\n",
        "retrieved = vector_ref_retrieve(QUERY, top_k=TOP_K, hops=MAX_REF_HOPS)\n",
        "print('seeds:', len(retrieved['seeds']))\n",
        "print('ref_edges:', len(retrieved['ref_edges']))\n",
        "print('contexts:', len(retrieved['contexts']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "325fbd39",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5) Clova 답변 생성\n",
        "\n",
        "def build_context_text(contexts, max_items=20):\n",
        "    lines = []\n",
        "    for c in contexts[:max_items]:\n",
        "        lines.append(\n",
        "            f\"[{c.get('law_name','')} 제{c.get('article_num','')}조 {c.get('paragraph_num','')}항 | {c.get('paragraph_key','')}]\\n{str(c.get('content',''))[:700]}\"\n",
        "        )\n",
        "    return '\\n\\n'.join(lines)\n",
        "\n",
        "\n",
        "context_text = build_context_text(retrieved['contexts'], max_items=20)\n",
        "\n",
        "prompt = f'''당신은 건축법률 분석 어시스턴트입니다.\n",
        "아래 질문에 대해, 제공된 법령 문맥만 근거로 한국어로 답하세요.\n",
        "\n",
        "요구사항:\n",
        "1) 근거 조문(법령명, 조, 항)을 2개 이상 명시\n",
        "2) 불확실하면 불확실하다고 말하고 추가 필요 조건을 질문\n",
        "3) 계산/판단이 있으면 단계적으로 작성\n",
        "\n",
        "질문:\n",
        "{QUERY}\n",
        "\n",
        "문맥:\n",
        "{context_text}\n",
        "'''\n",
        "\n",
        "resp = llm.invoke(prompt)\n",
        "answer_text = getattr(resp, 'content', str(resp))\n",
        "print(answer_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "147ddee0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6) 디버깅 출력\n",
        "print('[SEEDS]')\n",
        "for s in retrieved['seeds']:\n",
        "    print('-', s['paragraph_key'], '| score=', round(float(s.get('score', 0.0)), 4))\n",
        "\n",
        "print('\\n[REF EDGES]')\n",
        "for e in retrieved['ref_edges'][:30]:\n",
        "    print('-', e.get('from_key'), '--REF(', e.get('scope',''), ')->', e.get('to_key'), '|', e.get('raw',''))\n",
        "\n",
        "print('\\n[CONTEXT PREVIEW]')\n",
        "for c in retrieved['contexts'][:5]:\n",
        "    print(f\"[{c.get('law_name','')} 제{c.get('article_num','')}조 {c.get('paragraph_num','')}항]\")\n",
        "    print(str(c.get('content',''))[:220])\n",
        "    print('---')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db7b6fbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional) 스키마/충돌 진단\n",
        "# 같은 라벨/키를 다른 데이터가 쓰는지 확인\n",
        "\n",
        "\n",
        "labels, _, _ = run_query('CALL db.labels() YIELD label RETURN label ORDER BY label')\n",
        "print('labels:', [r['label'] for r in labels])\n",
        "\n",
        "for label, key in [('Document','law_id'), ('Article','article_key'), ('Paragraph','paragraph_key')]:\n",
        "    rows, _, _ = run_query(f'MATCH (n:{label}) RETURN count(n) AS c')\n",
        "    c = rows[0]['c'] if rows else 0\n",
        "    dup, _, _ = run_query(f'MATCH (n:{label}) WITH n.{key} AS k, count(*) AS c WHERE k IS NOT NULL AND c > 1 RETURN count(*) AS d')\n",
        "    d = dup[0]['d'] if dup else 0\n",
        "    print(label, 'count=', c, 'dup_keys=', d)\n",
        "\n",
        "scope, _, _ = run_query(\n",
        "    \"MATCH (p:Paragraph) RETURN count(*) AS total, \"\n",
        "    \"count(CASE WHEN p.law_id IN ['001823','002118'] THEN 1 END) AS in_scope, \"\n",
        "    \"count(CASE WHEN p.law_id IS NULL OR p.law_id NOT IN ['001823','002118'] THEN 1 END) AS out_scope\"\n",
        ")\n",
        "print('paragraph scope:', scope[0] if scope else {})\n",
        "\n",
        "outs, _, _ = run_query(\n",
        "    \"MATCH (p:Paragraph) WHERE p.law_id IS NULL OR p.law_id NOT IN ['001823','002118'] \"\n",
        "    \"RETURN p.paragraph_key AS paragraph_key, p.law_id AS law_id, p.law_name AS law_name LIMIT 10\"\n",
        ")\n",
        "print('out-scope samples:', [x.data() if hasattr(x, 'data') else x for x in outs])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 종료\n",
        "# driver.close()  # 필요 시 수동 종료\n",
        "print('done')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "graphrag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
