{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "07520d55",
      "metadata": {},
      "source": [
        "# 03_extract_refs (LLM only)\n",
        "\n",
        "이 노트북은 chunk 본문을 LLM으로 직접 읽어 `internal_refs / external_refs`를 추출합니다.\n",
        "- 입력: `data/processed/chunks/*.json`, `data/processed/chunks_ordin/*.json`\n",
        "- 출력:\n",
        "  - `data/processed/ref_extract/llm_ref_map.json`\n",
        "  - `data/processed/ref_extract/llm_ref_failed.json`\n",
        "  - `data/processed/chunks_with_refs/*.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca85b73",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_naver import ChatClovaX\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "DATA_ROOT = Path('data/processed')\n",
        "CHUNKS_DIR = DATA_ROOT / 'chunks'\n",
        "CHUNKS_ORDIN_DIR = DATA_ROOT / 'chunks_ordin'\n",
        "REF_OUT_DIR = DATA_ROOT / 'ref_extract'\n",
        "WITH_REFS_DIR = DATA_ROOT / 'chunks_with_refs'\n",
        "\n",
        "REF_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "WITH_REFS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('DATA_ROOT:', DATA_ROOT)\n",
        "print('CHUNKS_DIR exists:', CHUNKS_DIR.exists())\n",
        "print('CHUNKS_ORDIN_DIR exists:', CHUNKS_ORDIN_DIR.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "258755cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_chunk_key(law_id: str, article_num: str, article_sub: str | int | None) -> str:\n",
        "    sub = '' if article_sub is None else str(article_sub).strip()\n",
        "    if not sub:\n",
        "        sub = '0'\n",
        "    return f\"{str(law_id).strip()}:{str(article_num).strip()}:{sub}\"\n",
        "\n",
        "\n",
        "# 비워두면 전체 파일(*_chunks.json) 로드\n",
        "TARGET_CHUNK_FILES = [\n",
        "    # '001823_건축법_chunks.json',\n",
        "    # '2205103_서울특별시_건축물관리_조례_chunks.json',\n",
        "    '22*_서울특별시_*_조례_chunks.json',\n",
        "]\n",
        "\n",
        "# False면 chunks_ordin 로딩 제외\n",
        "INCLUDE_ORDIN = True\n",
        "\n",
        "\n",
        "def resolve_chunk_files(base_dir: Path, selected: list[str] | None) -> list[Path]:\n",
        "    if not base_dir.exists():\n",
        "        return []\n",
        "\n",
        "    if not selected:\n",
        "        return sorted(base_dir.glob('*_chunks.json'))\n",
        "\n",
        "    out: list[Path] = []\n",
        "    seen: set[str] = set()\n",
        "\n",
        "    for token in selected:\n",
        "        token = str(token).strip()\n",
        "        if not token:\n",
        "            continue\n",
        "\n",
        "        # 1) 절대경로/상대경로 직접 지정\n",
        "        p = Path(token)\n",
        "        candidates = []\n",
        "        if p.is_absolute() and p.exists():\n",
        "            candidates = [p]\n",
        "        else:\n",
        "            q = base_dir / token\n",
        "            if q.exists():\n",
        "                candidates = [q]\n",
        "            else:\n",
        "                # 2) 글롭 패턴\n",
        "                candidates = sorted(base_dir.glob(token))\n",
        "\n",
        "        for cp in candidates:\n",
        "            key = str(cp.resolve())\n",
        "            if key in seen:\n",
        "                continue\n",
        "            seen.add(key)\n",
        "            out.append(cp)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def load_all_chunks(selected_files: list[str] | None = None, include_ordin: bool = True):\n",
        "    rows: list[dict[str, Any]] = []\n",
        "    loaded_files: list[str] = []\n",
        "\n",
        "    law_files = resolve_chunk_files(CHUNKS_DIR, selected_files)\n",
        "    for f in law_files:\n",
        "        rows.extend(json.loads(f.read_text(encoding='utf-8')))\n",
        "        loaded_files.append(str(f))\n",
        "\n",
        "    if include_ordin:\n",
        "        ordin_files = resolve_chunk_files(CHUNKS_ORDIN_DIR, selected_files)\n",
        "        for f in ordin_files:\n",
        "            rows.extend(json.loads(f.read_text(encoding='utf-8')))\n",
        "            loaded_files.append(str(f))\n",
        "\n",
        "    for r in rows:\n",
        "        r.setdefault('article_sub', '0')\n",
        "        r.setdefault('internal_refs', [])\n",
        "        r.setdefault('external_refs', [])\n",
        "        r.setdefault('parent_law_refs', [])\n",
        "\n",
        "    return rows, loaded_files\n",
        "\n",
        "\n",
        "all_chunks, loaded_chunk_files = load_all_chunks(\n",
        "    selected_files=TARGET_CHUNK_FILES,\n",
        "    include_ordin=INCLUDE_ORDIN,\n",
        ")\n",
        "\n",
        "print('loaded files:', len(loaded_chunk_files))\n",
        "for x in loaded_chunk_files[:10]:\n",
        "    print('-', x)\n",
        "print('all_chunks:', len(all_chunks))\n",
        "print('sample key:', make_chunk_key(all_chunks[0].get('law_id',''), all_chunks[0].get('article_num',''), all_chunks[0].get('article_sub','')) if all_chunks else 'N/A')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d056182",
      "metadata": {},
      "outputs": [],
      "source": [
        "REF_SYSTEM_PROMPT = \"\"\"\n",
        "당신은 한국 법령/조례 문서의 참조 조항 추출기입니다.\n",
        "아래 chunk 본문에서 명시된 참조만 추출하세요.\n",
        "\n",
        "규칙:\n",
        "1) 추측 금지. 본문에 없으면 넣지 마세요.\n",
        "2) 출력은 JSON 객체 하나만.\n",
        "3) 반드시 다음 키만 출력:\n",
        "   - internal_refs: list\n",
        "   - external_refs: list\n",
        "4) 각 ref 원소는 아래 키만 사용:\n",
        "   - law_name: string  (내부참조면 현재 법령명)\n",
        "   - article: string   (조 번호 숫자/의숫자 형태, 예: \"46\", \"1의2\")\n",
        "   - paragraph: string (항 번호만, 없으면 \"\")\n",
        "   - item: string      (호 번호만, 없으면 \"\")\n",
        "   - raw: string       (원문 참조 문자열)\n",
        "5) 형식 예시:\n",
        "{\n",
        "  \"internal_refs\": [{\"law_name\":\"건축법\",\"article\":\"46\",\"paragraph\":\"1\",\"item\":\"\",\"raw\":\"제46조제1항\"}],\n",
        "  \"external_refs\": [{\"law_name\":\"건축법 시행령\",\"article\":\"31\",\"paragraph\":\"\",\"item\":\"\",\"raw\":\"「건축법 시행령」 제31조\"}]\n",
        "}\n",
        "6) 참조가 전혀 없으면:\n",
        "{\"internal_refs\": [], \"external_refs\": []}\n",
        "\"\"\".strip()\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        ('system', REF_SYSTEM_PROMPT),\n",
        "        ('human', '현재 법령명: {law_name}\\n\\nchunk 본문:\\n{chunk_text}'),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def build_ref_llm(model: str = 'HCX-005', temperature: float = 0.0, max_tokens: int = 1200):\n",
        "    return ChatClovaX(model=model, temperature=temperature, max_tokens=max_tokens)\n",
        "\n",
        "\n",
        "llm = build_ref_llm(model='HCX-005', temperature=0.0, max_tokens=1200)\n",
        "chain = prompt | llm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c836d138",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _extract_json_text(raw_text: str) -> str:\n",
        "    text = (raw_text or '').strip()\n",
        "    if '```json' in text:\n",
        "        text = text.split('```json', 1)[1]\n",
        "        text = text.split('```', 1)[0]\n",
        "    elif '```' in text:\n",
        "        text = text.split('```', 1)[1]\n",
        "        text = text.split('```', 1)[0]\n",
        "\n",
        "    start = text.find('{')\n",
        "    end = text.rfind('}')\n",
        "    if start >= 0 and end > start:\n",
        "        return text[start:end + 1]\n",
        "    return text\n",
        "\n",
        "\n",
        "def _norm_ref_list(v: Any) -> list[dict[str, str]]:\n",
        "    if not isinstance(v, list):\n",
        "        return []\n",
        "    out: list[dict[str, str]] = []\n",
        "    for x in v:\n",
        "        if not isinstance(x, dict):\n",
        "            continue\n",
        "        out.append(\n",
        "            {\n",
        "                'law_name': str(x.get('law_name', '') or '').strip(),\n",
        "                'article': str(x.get('article', '') or '').strip(),\n",
        "                'paragraph': str(x.get('paragraph', '') or '').strip(),\n",
        "                'item': str(x.get('item', '') or '').strip(),\n",
        "                'raw': str(x.get('raw', '') or '').strip(),\n",
        "            }\n",
        "        )\n",
        "    return out\n",
        "\n",
        "\n",
        "def extract_refs_from_chunk_llm(law_name: str, chunk_text: str, max_retries: int = 4, base_sleep: float = 1.0) -> dict[str, Any]:\n",
        "    last_err = ''\n",
        "    for attempt in range(max_retries + 1):\n",
        "        try:\n",
        "            resp = chain.invoke({'law_name': law_name, 'chunk_text': chunk_text})\n",
        "            raw_text = getattr(resp, 'content', str(resp))\n",
        "            obj = json.loads(_extract_json_text(raw_text))\n",
        "            return {\n",
        "                'internal_refs': _norm_ref_list(obj.get('internal_refs', [])),\n",
        "                'external_refs': _norm_ref_list(obj.get('external_refs', [])),\n",
        "                'llm_raw_text': raw_text,\n",
        "                'error': '',\n",
        "            }\n",
        "        except Exception as e:\n",
        "            last_err = str(e)\n",
        "            if ('429' in last_err or 'rate exceeded' in last_err.lower()) and attempt < max_retries:\n",
        "                time.sleep(min(base_sleep * (2 ** attempt), 30.0))\n",
        "                continue\n",
        "            if attempt < max_retries:\n",
        "                time.sleep(0.7)\n",
        "                continue\n",
        "            break\n",
        "\n",
        "    return {\n",
        "        'internal_refs': [],\n",
        "        'external_refs': [],\n",
        "        'llm_raw_text': '',\n",
        "        'error': last_err,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f76f6d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "REF_MAP_PATH = REF_OUT_DIR / 'llm_ref_map.json'\n",
        "FAILED_PATH = REF_OUT_DIR / 'llm_ref_failed.json'\n",
        "\n",
        "\n",
        "def load_json_or_default(path: Path, default):\n",
        "    if path.exists():\n",
        "        return json.loads(path.read_text(encoding='utf-8'))\n",
        "    return default\n",
        "\n",
        "\n",
        "def save_json(path: Path, obj: Any):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "\n",
        "\n",
        "def run_ref_extraction(chunks: list[dict[str, Any]], limit: int | None = None, save_every: int = 10):\n",
        "    ref_map = load_json_or_default(REF_MAP_PATH, {})\n",
        "    failed = load_json_or_default(FAILED_PATH, {})\n",
        "\n",
        "    targets = chunks if limit is None else chunks[:limit]\n",
        "    done = 0\n",
        "    new = 0\n",
        "\n",
        "    print('resume_mode:', REF_MAP_PATH.exists())\n",
        "    print('already_done:', len(ref_map))\n",
        "    print('targets:', len(targets))\n",
        "\n",
        "    for i, c in enumerate(targets, 1):\n",
        "        key = make_chunk_key(c.get('law_id', ''), c.get('article_num', ''), c.get('article_sub', '0'))\n",
        "        if key in ref_map:\n",
        "            done += 1\n",
        "            continue\n",
        "\n",
        "        out = extract_refs_from_chunk_llm(c.get('law_name', ''), c.get('content', ''))\n",
        "        ref_map[key] = {\n",
        "            'internal_refs': out['internal_refs'],\n",
        "            'external_refs': out['external_refs'],\n",
        "        }\n",
        "\n",
        "        if out.get('error'):\n",
        "            failed[key] = {\n",
        "                'error': out['error'][:500],\n",
        "                'law_name': c.get('law_name', ''),\n",
        "                'article_num': c.get('article_num', ''),\n",
        "            }\n",
        "        else:\n",
        "            failed.pop(key, None)\n",
        "\n",
        "        new += 1\n",
        "\n",
        "        if new % save_every == 0:\n",
        "            save_json(REF_MAP_PATH, ref_map)\n",
        "            save_json(FAILED_PATH, failed)\n",
        "            print(f'checkpoint: +{new} new, processed={i}/{len(targets)}')\n",
        "\n",
        "    save_json(REF_MAP_PATH, ref_map)\n",
        "    save_json(FAILED_PATH, failed)\n",
        "\n",
        "    print('done_skip:', done)\n",
        "    print('new_saved:', new)\n",
        "    print('total_ref_map:', len(ref_map))\n",
        "    print('failed:', len(failed))\n",
        "\n",
        "    return ref_map, failed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01cb8e27",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 실행\n",
        "# 테스트: limit=20\n",
        "# 전체: limit=None\n",
        "ref_map, failed_map = run_ref_extraction(all_chunks, limit=20, save_every=5)\n",
        "\n",
        "print('REF_MAP_PATH:', REF_MAP_PATH)\n",
        "print('FAILED_PATH:', FAILED_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91621838",
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_refs_to_chunks(chunks: list[dict[str, Any]], ref_map: dict[str, Any]) -> list[dict[str, Any]]:\n",
        "    out = []\n",
        "    for c in chunks:\n",
        "        row = dict(c)\n",
        "        key = make_chunk_key(row.get('law_id', ''), row.get('article_num', ''), row.get('article_sub', '0'))\n",
        "        refs = ref_map.get(key, {}) or {}\n",
        "        row['internal_refs'] = refs.get('internal_refs', []) if isinstance(refs.get('internal_refs', []), list) else []\n",
        "        row['external_refs'] = refs.get('external_refs', []) if isinstance(refs.get('external_refs', []), list) else []\n",
        "        row['parent_law_refs'] = []\n",
        "        out.append(row)\n",
        "    return out\n",
        "\n",
        "\n",
        "def safe_name(name: str) -> str:\n",
        "    return ''.join(ch if ch.isalnum() or ch in ['_', '-', ' '] else '_' for ch in str(name)).replace(' ', '_')\n",
        "\n",
        "\n",
        "def save_chunks_with_refs(rows: list[dict[str, Any]], out_dir: Path):\n",
        "    by_law: dict[str, list[dict[str, Any]]] = {}\n",
        "    for r in rows:\n",
        "        lid = str(r.get('law_id', '')).strip()\n",
        "        by_law.setdefault(lid, []).append(r)\n",
        "\n",
        "    for lid, items in by_law.items():\n",
        "        lname = str(items[0].get('law_name', '')).strip()\n",
        "        p = out_dir / f'{lid}_{safe_name(lname)}_chunks_with_refs.json'\n",
        "        p.write_text(json.dumps(items, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "\n",
        "    p_all = out_dir / 'all_chunks_with_refs.json'\n",
        "    p_all.write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "\n",
        "    return {'laws': len(by_law), 'all_path': str(p_all)}\n",
        "\n",
        "\n",
        "chunks_with_refs = apply_refs_to_chunks(all_chunks, ref_map)\n",
        "save_info = save_chunks_with_refs(chunks_with_refs, WITH_REFS_DIR)\n",
        "print('saved:', save_info)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}