{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "202253c8",
      "metadata": {},
      "source": [
        "## Qdrant Cloud 연결 설정\n",
        "\n",
        "로컬 대신 클라우드를 쓰려면 아래 환경변수를 먼저 설정하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9a0c5c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4edefd8",
      "metadata": {},
      "source": [
        "# 06. 에이전트 실행 (Self-contained)\n",
        "\n",
        "그래프/툴 정의를 노트북 셀에서 직접 작성해 실행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7d49fb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "\n",
        "from langchain_naver import ChatClovaX, ClovaXEmbeddings\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import FieldCondition, Filter, MatchValue\n",
        "from qdrant_client.http.exceptions import UnexpectedResponse\n",
        "\n",
        "try:\n",
        "    from langgraph.graph import END, StateGraph\n",
        "except Exception:\n",
        "    END = None\n",
        "    StateGraph = None\n",
        "\n",
        "\n",
        "def load_appendix_terms(path='data/processed/appendix1_terms.json'):\n",
        "    data = json.loads(Path(path).read_text(encoding='utf-8'))\n",
        "    return data.get('terms', [])\n",
        "\n",
        "\n",
        "def lookup_appendix1_term(term_or_query: str, terms):\n",
        "    q = term_or_query.lower().strip()\n",
        "    if not q:\n",
        "        return []\n",
        "\n",
        "    exact = []\n",
        "    alias = []\n",
        "    fuzzy = []\n",
        "    q_tokens = {t for t in re.split(r'[^0-9A-Za-z가-힣]+', q) if t}\n",
        "\n",
        "    for t in terms:\n",
        "        c = str(t.get('category', ''))\n",
        "        s = str(t.get('subcategory', ''))\n",
        "        a = [str(x) for x in t.get('aliases', [])]\n",
        "        d = str(t.get('description', ''))\n",
        "\n",
        "        if q in c.lower() or q in s.lower():\n",
        "            exact.append(t)\n",
        "            continue\n",
        "        if any(q in x.lower() or x.lower() in q for x in a):\n",
        "            alias.append(t)\n",
        "            continue\n",
        "        doc_tokens = {x for x in re.split(r'[^0-9A-Za-z가-힣]+', ' '.join([c, s, ' '.join(a), d]).lower()) if x}\n",
        "        if q_tokens and doc_tokens:\n",
        "            score = len(q_tokens & doc_tokens) / len(q_tokens | doc_tokens)\n",
        "            if score > 0:\n",
        "                fuzzy.append((score, t))\n",
        "\n",
        "    fuzzy.sort(key=lambda x: x[0], reverse=True)\n",
        "    return exact + alias + [x[1] for x in fuzzy][:5]\n",
        "\n",
        "\n",
        "def build_retriever(\n",
        "    collection='building_law',\n",
        "    qdrant_path='./qdrant_data',\n",
        "    qdrant_url=None,\n",
        "    qdrant_api_key=None,\n",
        "    prefer_grpc=False,\n",
        "):\n",
        "    url = (qdrant_url or os.getenv('QDRANT_URL') or '').strip()\n",
        "    api_key = (qdrant_api_key or os.getenv('QDRANT_API_KEY') or '').strip()\n",
        "\n",
        "    if url:\n",
        "        if '/dashboard' in url:\n",
        "            raise ValueError('QDRANT_URL은 dashboard URL이 아니라 API endpoint여야 합니다. 예: https://<cluster>.cloud.qdrant.io:6333')\n",
        "        if not api_key:\n",
        "            raise ValueError('QDRANT_URL 사용 시 QDRANT_API_KEY도 설정해야 합니다.')\n",
        "        client = QdrantClient(url=url, api_key=api_key, prefer_grpc=prefer_grpc)\n",
        "        print('Qdrant mode: CLOUD')\n",
        "        print('Qdrant url :', url)\n",
        "    else:\n",
        "        client = QdrantClient(path=qdrant_path)\n",
        "        print('Qdrant mode: LOCAL')\n",
        "        print('Qdrant path:', qdrant_path)\n",
        "\n",
        "    # 연결 확인 + 상세 에러 출력\n",
        "    try:\n",
        "        exists = client.collection_exists(collection)\n",
        "        print(f'collection \"{collection}\" exists:', exists)\n",
        "    except UnexpectedResponse as e:\n",
        "        print('Qdrant request failed')\n",
        "        print('status/code:', e)\n",
        "        print('hint:')\n",
        "        print('- QDRANT_URL 형식 확인 (https://<cluster>.cloud.qdrant.io:6333)')\n",
        "        print('- QDRANT_API_KEY 권한 확인 (읽기/쓰기)')\n",
        "        print('- 컬렉션명 확인:', collection)\n",
        "        raise\n",
        "\n",
        "    embeddings = ClovaXEmbeddings(model='bge-m3')\n",
        "    store = QdrantVectorStore(client=client, collection_name=collection, embedding=embeddings)\n",
        "    return client, store\n",
        "\n",
        "\n",
        "def search_law_chunks(store, query, k=4):\n",
        "    docs = store.similarity_search(query, k=k)\n",
        "    return [{'content': d.page_content, 'metadata': d.metadata} for d in docs]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def parse_article_token(article: str) -> tuple[str, str]:\n",
        "    a = str(article or '').strip()\n",
        "    m = re.fullmatch(r'(\\d+)(?:의(\\d+))?', a)\n",
        "    if m:\n",
        "        return m.group(1) or '', m.group(2) or ''\n",
        "    return a, ''\n",
        "\n",
        "\n",
        "def make_chunk_key(law_id: str, article_num: str, article_sub: str | int | None = '') -> str:\n",
        "    lid = str(law_id).strip().zfill(6)\n",
        "    sub = str(article_sub or '').strip()\n",
        "    return f\"{lid}:{str(article_num).strip()}:{sub if sub else '0'}\"\n",
        "\n",
        "\n",
        "def get_article(client, collection, law_id, article_num):\n",
        "    # article_num could be '4' or '4의2'\n",
        "    law_id = str(law_id).strip().zfill(6)\n",
        "    main, sub = parse_article_token(str(article_num))\n",
        "\n",
        "    candidate_keys = []\n",
        "    if main:\n",
        "        candidate_keys.append(make_chunk_key(law_id, main, sub))\n",
        "        # backward compatibility for old indexed data\n",
        "        if sub:\n",
        "            candidate_keys.append(f\"{law_id}:{main}의{sub}\")\n",
        "        candidate_keys.append(f\"{law_id}:{main}\")\n",
        "    else:\n",
        "        candidate_keys.append(f\"{law_id}:{str(article_num).strip()}\")\n",
        "\n",
        "    # 1) retrieve by deterministic point id candidates\n",
        "    for source_key in candidate_keys:\n",
        "        point_id = str(uuid.uuid5(uuid.NAMESPACE_URL, source_key))\n",
        "        try:\n",
        "            points = client.retrieve(\n",
        "                collection_name=collection,\n",
        "                ids=[point_id],\n",
        "                with_payload=True,\n",
        "                with_vectors=False,\n",
        "            )\n",
        "            if points:\n",
        "                return [{'content': (p.payload or {}).get('content_original', ''), 'metadata': (p.payload or {})} for p in points]\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 2) fallback: scroll and memory filter (works even without payload index)\n",
        "    points, _ = client.scroll(\n",
        "        collection_name=collection,\n",
        "        limit=5000,\n",
        "        with_payload=True,\n",
        "        with_vectors=False,\n",
        "    )\n",
        "\n",
        "    out = []\n",
        "    for p in points:\n",
        "        payload = p.payload or {}\n",
        "        pl_law = str(payload.get('law_id', '')).strip().zfill(6)\n",
        "        pl_num = str(payload.get('article_num', '')).strip()\n",
        "        pl_sub = str(payload.get('article_sub', '')).strip()\n",
        "\n",
        "        if pl_law != law_id:\n",
        "            continue\n",
        "        if main and pl_num == main and (pl_sub or '') == (sub or ''):\n",
        "            out.append({'content': payload.get('content_original', ''), 'metadata': payload})\n",
        "        elif (not main) and pl_num == str(article_num).strip():\n",
        "            out.append({'content': payload.get('content_original', ''), 'metadata': payload})\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be6f728d",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa74c7b4",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aa6acbb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# (legacy graph helpers removed)\n",
        "# 아래쪽 cell의 build_reference_graph_data / render_reference_graph 를 사용하세요.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebd679f1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8a681a47",
      "metadata": {},
      "source": [
        "## Step A. 상태 스키마\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a544be1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict\n",
        "\n",
        "\n",
        "class GraphState(TypedDict, total=False):\n",
        "    user_query: str\n",
        "    targets: list[str]\n",
        "    target_hits: dict[str, list[dict]]\n",
        "    target_decisions: dict[str, list[dict]]\n",
        "    target_retrieve_debug: dict[str, dict]\n",
        "    hits: list[dict]\n",
        "    pending_refs: list[dict]\n",
        "    seen_ref_keys: list[str]\n",
        "    contexts: list[dict]\n",
        "    appendix: list[dict]\n",
        "    hop_count: int\n",
        "    max_hops: int\n",
        "    answer: str\n",
        "    ref_batch_decisions: list[dict]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8447075c",
      "metadata": {},
      "source": [
        "## Step B. 런타임 초기화\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71f51358",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "DEFAULT_LLM_CONFIGS = {\n",
        "    # 질문에서 구할 항목(건축선/용적률/...) 추출\n",
        "    'target_extractor': {\n",
        "        'model': 'HCX-005',\n",
        "        'temperature': 0.1,\n",
        "        'max_tokens': 512,\n",
        "    },\n",
        "    # 청크별 ref 확장 여부/우선순위/이유 판단\n",
        "    'ref_expander': {\n",
        "        'model': 'HCX-005',\n",
        "        'temperature': 0.1,\n",
        "        'max_tokens': 512,\n",
        "    },\n",
        "    # hop마다 몇 개 ref를 우선 확장할지 결정\n",
        "    'ref_batch_planner': {\n",
        "        'model': 'HCX-005',\n",
        "        'temperature': 0.0,\n",
        "        'max_tokens': 500,\n",
        "    },\n",
        "    # 최종 답변 생성\n",
        "    'answer_generator': {\n",
        "        'model': 'HCX-005',\n",
        "        'temperature': 0.3,\n",
        "        'max_tokens': 2048,\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "def _build_llm_from_cfg(cfg: dict) -> ChatClovaX:\n",
        "    return ChatClovaX(\n",
        "        model=cfg.get('model', 'HCX-005'),\n",
        "        temperature=float(cfg.get('temperature', 0.0)),\n",
        "        max_tokens=int(cfg.get('max_tokens', 1024)),\n",
        "    )\n",
        "\n",
        "\n",
        "def get_runtime_llm(runtime: dict, role: str) -> ChatClovaX:\n",
        "    cache = runtime.setdefault('_llm_cache', {})\n",
        "    if role in cache:\n",
        "        return cache[role]\n",
        "\n",
        "    cfg_map = runtime.get('llm_configs', {}) or {}\n",
        "    cfg = dict(DEFAULT_LLM_CONFIGS.get(role, DEFAULT_LLM_CONFIGS['answer_generator']))\n",
        "    cfg.update(cfg_map.get(role, {}) or {})\n",
        "\n",
        "    llm = _build_llm_from_cfg(cfg)\n",
        "    cache[role] = llm\n",
        "    return llm\n",
        "\n",
        "\n",
        "def init_runtime(llm_configs: dict | None = None):\n",
        "    client, store = build_retriever()  # Cloud/Local 자동\n",
        "    terms = load_appendix_terms()\n",
        "\n",
        "    merged = {k: dict(v) for k, v in DEFAULT_LLM_CONFIGS.items()}\n",
        "    if llm_configs:\n",
        "        for k, v in llm_configs.items():\n",
        "            if k not in merged:\n",
        "                merged[k] = {}\n",
        "            merged[k].update(v or {})\n",
        "\n",
        "    return {\n",
        "        'client': client,\n",
        "        'store': store,\n",
        "        'terms': terms,\n",
        "        'llm_configs': merged,\n",
        "        '_llm_cache': {},\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfd6a98f",
      "metadata": {},
      "source": [
        "## Step C. 질문에서 target 추출 (LLM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd89e8cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_targets_with_llm(llm, user_query: str) -> list[str]:\n",
        "    prompt = f\"\"\"\n",
        "당신은 건축법 질의에서 '무엇을 구하려는지'를 추출하는 분류기입니다.\n",
        "\n",
        "규칙:\n",
        "1) 출력은 반드시 JSON 하나만 출력\n",
        "2) 스키마: {{\"targets\": [\"...\", \"...\"]}}\n",
        "3) targets에는 계산/판단 대상 키워드를 넣는다 (예: 건축선, 용적률, 건폐율, 주차대수, 높이제한, 접도조건)\n",
        "4) 중복 제거\n",
        "5) 없으면 {{\"targets\": [\"일반\"]}}\n",
        "\n",
        "질문:\n",
        "{user_query}\n",
        "\"\"\".strip()\n",
        "\n",
        "    raw = llm.invoke(prompt)\n",
        "    text = getattr(raw, 'content', str(raw)).strip()\n",
        "\n",
        "    parsed = None\n",
        "    try:\n",
        "        parsed = json.loads(text)\n",
        "    except Exception:\n",
        "        m = re.search(r'\\{[\\s\\S]*\\}', text)\n",
        "        if m:\n",
        "            try:\n",
        "                parsed = json.loads(m.group(0))\n",
        "            except Exception:\n",
        "                parsed = None\n",
        "\n",
        "    targets = []\n",
        "    if isinstance(parsed, dict) and isinstance(parsed.get('targets'), list):\n",
        "        for t in parsed['targets']:\n",
        "            if isinstance(t, str) and t.strip():\n",
        "                targets.append(t.strip())\n",
        "\n",
        "    if not targets:\n",
        "        cand = ['건축선', '용적률', '건폐율', '주차', '높이제한', '접도조건', '일조권', '도로사선']\n",
        "        for c in cand:\n",
        "            if c in user_query:\n",
        "                targets.append(c)\n",
        "\n",
        "    out = []\n",
        "    seen = set()\n",
        "    for t in targets:\n",
        "        if t not in seen:\n",
        "            out.append(t)\n",
        "            seen.add(t)\n",
        "\n",
        "    return out or ['일반']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "367c2661",
      "metadata": {},
      "outputs": [],
      "source": [
        "# runtime 초기화 (예시 실행 아님)\n",
        "runtime = init_runtime(\n",
        "    llm_configs={\n",
        "        'target_extractor': {'temperature': 0.1, 'max_tokens': 512},\n",
        "        'ref_expander': {'temperature': 0.1, 'max_tokens': 512},\n",
        "        'ref_batch_planner': {'temperature': 0.1, 'max_tokens': 100},\n",
        "        'answer_generator': {'temperature': 0.3, 'max_tokens': 2048},\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f11779c",
      "metadata": {},
      "source": [
        "## Step D. 공통 유틸 (dedup / key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ecca786",
      "metadata": {},
      "outputs": [],
      "source": [
        "def context_key(item: dict) -> str:\n",
        "    m = item.get('metadata', {}) if isinstance(item, dict) else {}\n",
        "    return f\"{m.get('law_id')}:{m.get('article_num')}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb8cd9c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def dedup_contexts(items: list[dict]) -> list[dict]:\n",
        "    dedup = {}\n",
        "    for x in items:\n",
        "        dedup[context_key(x)] = x\n",
        "    return list(dedup.values())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85cbc032",
      "metadata": {},
      "source": [
        "## Step D-1. 깊은 참조 추적 유틸 (LLM + Rule)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "173315b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "IN_SCOPE_LAW_NAME_TO_ID = {\n",
        "    '건축법': '001823',\n",
        "    '건축법 시행령': '002118',\n",
        "}\n",
        "\n",
        "\n",
        "def normalize_abbr_map(abbr_obj) -> dict:\n",
        "    if not isinstance(abbr_obj, dict):\n",
        "        return {}\n",
        "    if '약어' in abbr_obj and isinstance(abbr_obj.get('약어'), dict):\n",
        "        return abbr_obj['약어']\n",
        "    return abbr_obj\n",
        "\n",
        "\n",
        "def extract_ref_candidates(meta: dict) -> list[dict]:\n",
        "    law_id = str(meta.get('law_id', '')).strip().zfill(6)\n",
        "    out = []\n",
        "    seen = set()\n",
        "\n",
        "    def parse_para_item_fallback(r: dict) -> tuple[str, str]:\n",
        "        p = str(r.get('paragraph', '') or '').strip()\n",
        "        it = str(r.get('item', '') or '').strip()\n",
        "        if p or it:\n",
        "            return p, it\n",
        "        raw = str(r.get('raw', '') or '')\n",
        "        m1 = re.search(r'제\\s*(\\d+)\\s*항', raw)\n",
        "        m2 = re.search(r'제\\s*(\\d+)\\s*호', raw)\n",
        "        return (m1.group(1) if m1 else ''), (m2.group(1) if m2 else '')\n",
        "\n",
        "    def resolve_external_law_id(r: dict) -> str:\n",
        "        lname = str(r.get('law_name', '') or '').strip()\n",
        "        raw = str(r.get('raw', '') or '')\n",
        "        if lname in IN_SCOPE_LAW_NAME_TO_ID:\n",
        "            return IN_SCOPE_LAW_NAME_TO_ID[lname]\n",
        "        if '대통령령' in raw or '시행령' in raw:\n",
        "            return '002118'\n",
        "        if re.search(r'\\b법\\b|법\\s*제\\d+', raw):\n",
        "            return '001823'\n",
        "        if '이 법' in raw:\n",
        "            return law_id\n",
        "        return ''\n",
        "\n",
        "    for r in meta.get('internal_refs', []) or []:\n",
        "        if not isinstance(r, dict):\n",
        "            continue\n",
        "        article = str(r.get('article', '')).strip()\n",
        "        if not (law_id and article):\n",
        "            continue\n",
        "        paragraph, item = parse_para_item_fallback(r)\n",
        "        key = (law_id, article, paragraph, item, 'internal')\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        out.append({\n",
        "            'law_id': law_id,\n",
        "            'article': article,\n",
        "            'paragraph': paragraph,\n",
        "            'item': item,\n",
        "            'source_ref': r,\n",
        "            'source': 'internal',\n",
        "        })\n",
        "\n",
        "    for r in meta.get('external_refs', []) or []:\n",
        "        if not isinstance(r, dict):\n",
        "            continue\n",
        "        mapped_law_id = resolve_external_law_id(r)\n",
        "        article = str(r.get('article', '')).strip()\n",
        "        if not mapped_law_id:\n",
        "            continue\n",
        "        paragraph, item = parse_para_item_fallback(r)\n",
        "        key = (mapped_law_id, article, paragraph, item, 'external')\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        out.append({\n",
        "            'law_id': mapped_law_id,\n",
        "            'article': article,\n",
        "            'paragraph': paragraph,\n",
        "            'item': item,\n",
        "            'source_ref': r,\n",
        "            'source': 'external-in-scope' if mapped_law_id in ['001823', '002118'] else 'external',\n",
        "        })\n",
        "\n",
        "    out.sort(key=lambda x: (0 if str(x.get('article','')).strip() else 1, x.get('source') != 'internal'))\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53409a23",
      "metadata": {},
      "outputs": [],
      "source": [
        "def rule_expand_analysis(content: str, meta: dict) -> dict:\n",
        "    refs = extract_ref_candidates(meta)\n",
        "    abbr = normalize_abbr_map(meta.get('abbreviations', {}))\n",
        "    text = content or ''\n",
        "\n",
        "    cue_pattern = re.compile(r'(이하|에\\s*따른|에\\s*의한|에서\\s*정하는|대통령령으로\\s*정하는|국토교통부령으로\\s*정하는)')\n",
        "    cue_matches = cue_pattern.findall(text)\n",
        "\n",
        "    reasons = []\n",
        "    expand = False\n",
        "\n",
        "    if not refs:\n",
        "        reasons.append('ref 후보 없음')\n",
        "    else:\n",
        "        reasons.append(f\"ref 후보 {len(refs)}개\")\n",
        "\n",
        "    if abbr:\n",
        "        expand = True\n",
        "        reasons.append(f\"축약어 {len(abbr)}개 존재\")\n",
        "\n",
        "    if cue_matches:\n",
        "        expand = True\n",
        "        reasons.append(f\"법령 해석 cue 발견: {sorted(set(cue_matches))}\")\n",
        "\n",
        "    internal_cnt = len(meta.get('internal_refs', []) or [])\n",
        "    if internal_cnt >= 2:\n",
        "        expand = True\n",
        "        reasons.append(f\"internal_refs {internal_cnt}개 (>=2)\")\n",
        "\n",
        "    if refs and not expand:\n",
        "        reasons.append('rule 기준으로는 확장 필요성 낮음')\n",
        "\n",
        "    return {\n",
        "        'expand': bool(expand),\n",
        "        'reasons': reasons,\n",
        "        'cue_matches': sorted(set(cue_matches)),\n",
        "        'abbr_count': len(abbr),\n",
        "        'ref_candidates_count': len(refs),\n",
        "        'internal_ref_count': internal_cnt,\n",
        "    }\n",
        "\n",
        "\n",
        "def rule_should_expand_chunk(content: str, meta: dict) -> bool:\n",
        "    return rule_expand_analysis(content, meta)['expand']\n",
        "\n",
        "\n",
        "def llm_should_expand_chunk(llm, user_query: str, target: str, content: str, meta: dict, return_debug: bool = False):\n",
        "    refs_preview = {\n",
        "        'internal_refs': (meta.get('internal_refs', []) or [])[:5],\n",
        "        'external_refs': (meta.get('external_refs', []) or [])[:5],\n",
        "    }\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "당신은 건축법률 RAG 참조 확장 판단기다.\n",
        "현재 청크를 읽고, 이 청크의 참조조항을 추가 추적해야 하는지 판단하라.\n",
        "\n",
        "규칙:\n",
        "- 출력은 JSON만: {{\"expand\": true/false, \"priority\": 0|1|2, \"reason\": \"...\"}}\n",
        "- expand=true 기준: 질문/타깃 답변에 참조조항 해석이 핵심인 경우\n",
        "- priority: 2(매우중요), 1(중요), 0(낮음)\n",
        "- reason에는 \"왜 참조를 확장/비확장해야 하는지\"를 한 문장 이상 구체적으로 **한국어**로 쓴다.\n",
        "\n",
        "질문: {user_query}\n",
        "타깃: {target}\n",
        "청크: {(content or '')}\n",
        "refs_preview: {refs_preview}\n",
        "\"\"\".strip()\n",
        "\n",
        "    raw = llm.invoke(prompt)\n",
        "    text = getattr(raw, 'content', str(raw)).strip()\n",
        "\n",
        "    parse_error = ''\n",
        "    obj = {}\n",
        "    try:\n",
        "        obj = json.loads(text)\n",
        "    except Exception:\n",
        "        m = re.search(r'\\{[\\s\\S]*\\}', text)\n",
        "        if m:\n",
        "            try:\n",
        "                obj = json.loads(m.group(0))\n",
        "            except Exception as e:\n",
        "                parse_error = str(e)\n",
        "                obj = {}\n",
        "        else:\n",
        "            parse_error = 'json object not found'\n",
        "\n",
        "    # key alias fallback (모델이 한국어 키를 쓸 때 대비)\n",
        "    expand_v = obj.get('expand', obj.get('확장', obj.get('need_expand', False)))\n",
        "    pri_v = obj.get('priority', obj.get('우선순위', obj.get('importance', 0)))\n",
        "    reason_v = obj.get('reason', obj.get('이유', obj.get('근거', '')))\n",
        "\n",
        "    expand = bool(expand_v)\n",
        "    priority = int(pri_v) if str(pri_v).isdigit() else 0\n",
        "    reason = str(reason_v or '').strip()\n",
        "\n",
        "    # reason이 비면 raw_text를 짧게라도 남겨 디버깅 가능하게 함\n",
        "    if not reason:\n",
        "        reason = (text[:300] + '...') if len(text) > 300 else text\n",
        "\n",
        "    result = (expand, max(0, min(priority, 2)), reason)\n",
        "    if not return_debug:\n",
        "        return result\n",
        "\n",
        "    debug = {\n",
        "        'prompt_preview': prompt[:1200],\n",
        "        'raw_text': text,\n",
        "        'parsed_obj': obj,\n",
        "        'parse_error': parse_error,\n",
        "    }\n",
        "    return result, debug\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def llm_select_ref_batch_size(\n",
        "    llm,\n",
        "    user_query: str,\n",
        "    targets: list[str],\n",
        "    hop_count: int,\n",
        "    max_hops: int,\n",
        "    pending_refs: list[dict],\n",
        "    max_batch: int = 4,\n",
        "    return_debug: bool = False,\n",
        "):\n",
        "    # 아직 확장하지 않은 후보 미리보기\n",
        "    preview = []\n",
        "    for r in (pending_refs or [])[:12]:\n",
        "        preview.append({\n",
        "            'law_id': r.get('law_id'),\n",
        "            'article': r.get('article'),\n",
        "            'priority': r.get('priority', 0),\n",
        "            'source': r.get('source', ''),\n",
        "            'decision_reason': str(r.get('decision_reason', ''))[:120],\n",
        "        })\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "당신은 법률 RAG의 ref 확장 플래너다.\n",
        "이번 hop에서 ref를 몇 개까지 우선 확장할지 정한다.\n",
        "\n",
        "규칙:\n",
        "- 출력은 JSON만: {{\"expand_count\": 정수, \"reason\": \"...\"}}\n",
        "- expand_count 범위: 1 ~ {max_batch}\n",
        "- 중요도(priority), 질문과의 관련성, 남은 hop({max_hops - hop_count})을 고려\n",
        "- 보수적으로 판단하되, 핵심 ref를 놓치지 않도록 한다.\n",
        "\n",
        "질문: {user_query}\n",
        "targets: {targets}\n",
        "현재 hop: {hop_count}\n",
        "max_hops: {max_hops}\n",
        "pending_ref_count: {len(pending_refs or [])}\n",
        "pending_preview: {preview}\n",
        "\"\"\".strip()\n",
        "\n",
        "    raw = llm.invoke(prompt)\n",
        "    text = getattr(raw, 'content', str(raw)).strip()\n",
        "\n",
        "    obj = {}\n",
        "    parse_error = ''\n",
        "    try:\n",
        "        obj = json.loads(text)\n",
        "    except Exception:\n",
        "        m = re.search(r'\\{[\\s\\S]*\\}', text)\n",
        "        if m:\n",
        "            try:\n",
        "                obj = json.loads(m.group(0))\n",
        "            except Exception as e:\n",
        "                parse_error = str(e)\n",
        "                obj = {}\n",
        "        else:\n",
        "            parse_error = 'json object not found'\n",
        "\n",
        "    n = obj.get('expand_count', obj.get('count', obj.get('n', 1)))\n",
        "    reason = str(obj.get('reason', obj.get('이유', '')) or '').strip()\n",
        "\n",
        "    if not str(n).isdigit():\n",
        "        # fallback heuristic\n",
        "        p = len(pending_refs or [])\n",
        "        if p >= 10:\n",
        "            n = 4\n",
        "        elif p >= 6:\n",
        "            n = 3\n",
        "        elif p >= 3:\n",
        "            n = 2\n",
        "        else:\n",
        "            n = 1\n",
        "        reason = reason or f'fallback_heuristic(pending={p})'\n",
        "\n",
        "    n = max(1, min(int(n), int(max_batch)))\n",
        "\n",
        "    if not return_debug:\n",
        "        return n, reason\n",
        "\n",
        "    debug = {\n",
        "        'raw_text': text,\n",
        "        'parsed_obj': obj,\n",
        "        'parse_error': parse_error,\n",
        "        'prompt_preview': prompt[:1500],\n",
        "    }\n",
        "    return (n, reason), debug\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_ref_docs_cached(runtime, law_id: str, article: str):\n",
        "    cache = runtime.setdefault('_ref_doc_cache', {})\n",
        "    key = f\"{str(law_id).strip()}:{str(article).strip()}\"\n",
        "    if key in cache:\n",
        "        return cache[key]\n",
        "    docs = get_article(runtime['client'], 'building_law', str(law_id).strip(), str(article).strip())\n",
        "    cache[key] = docs or []\n",
        "    return cache[key]\n",
        "\n",
        "\n",
        "\n",
        "def build_ref_docs_preview(\n",
        "    docs: list[dict],\n",
        "    paragraph: str = '',\n",
        "    item: str = '',\n",
        "    ref_key: str = '',\n",
        "    max_docs: int = 2,\n",
        "    max_chars: int = 260,\n",
        "):\n",
        "    out = []\n",
        "    p_raw = str(paragraph or '').strip()\n",
        "    it = str(item or '').strip()\n",
        "\n",
        "    circled = {\n",
        "        '1':'①','2':'②','3':'③','4':'④','5':'⑤','6':'⑥','7':'⑦','8':'⑧','9':'⑨','10':'⑩',\n",
        "        '11':'⑪','12':'⑫','13':'⑬','14':'⑭','15':'⑮','16':'⑯','17':'⑰','18':'⑱','19':'⑲','20':'⑳'\n",
        "    }\n",
        "\n",
        "    def norm_para_token(v: str) -> str:\n",
        "        t = str(v or '').strip()\n",
        "        if not t:\n",
        "            return ''\n",
        "        m = re.search(r'제\\s*(\\d+)\\s*항', t)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "        m = re.search(r'(\\d+)\\s*항', t)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "        for k, c in circled.items():\n",
        "            if t == c:\n",
        "                return k\n",
        "        if t.isdigit():\n",
        "            return t\n",
        "        return t\n",
        "\n",
        "    p_norm = norm_para_token(p_raw)\n",
        "\n",
        "    def extract_by_circled_from_content(full_text: str, pnum: str) -> str:\n",
        "        symbol = circled.get(str(pnum), '')\n",
        "        if not symbol:\n",
        "            return ''\n",
        "        t = str(full_text or '')\n",
        "        i = t.find(symbol)\n",
        "        if i < 0:\n",
        "            return ''\n",
        "        # 다음 항 기호 위치 탐색\n",
        "        next_positions = [t.find(sym, i+1) for sym in circled.values() if t.find(sym, i+1) >= 0]\n",
        "        j = min(next_positions) if next_positions else len(t)\n",
        "        return t[i:j].strip()\n",
        "\n",
        "    def extract_article_header(full_text: str) -> str:\n",
        "        t = str(full_text or '').strip()\n",
        "        if not t:\n",
        "            return ''\n",
        "        lines = [ln.strip() for ln in t.splitlines() if str(ln).strip()]\n",
        "        if not lines:\n",
        "            return ''\n",
        "        first = lines[0]\n",
        "        # 예: 제46조(건축선의 지정)\n",
        "        if re.match(r'^제\\s*\\d+(?:의\\d+)?조', first):\n",
        "            return first\n",
        "        m = re.search(r'(제\\s*\\d+(?:의\\d+)?조[^\\n]*)', t)\n",
        "        return m.group(1).strip() if m else ''\n",
        "\n",
        "    for d in (docs or [])[:max_docs]:\n",
        "        meta = (d.get('metadata', {}) or {}) if isinstance(d, dict) else {}\n",
        "\n",
        "        content = ''\n",
        "        if isinstance(d, dict):\n",
        "            content = (\n",
        "                d.get('content')\n",
        "                or d.get('page_content')\n",
        "                or meta.get('content_original')\n",
        "                or meta.get('page_content')\n",
        "                or ''\n",
        "            )\n",
        "\n",
        "        law_id = str(meta.get('law_id', '') or '').zfill(6) if meta.get('law_id') else ''\n",
        "        article_num = str(meta.get('article_num', '') or '')\n",
        "        article_sub = str(meta.get('article_sub', '0') or '0')\n",
        "\n",
        "        if (not law_id or not article_num) and ref_key and ':' in ref_key:\n",
        "            parts = ref_key.split(':')\n",
        "            law_id = law_id or parts[0].zfill(6)\n",
        "            article_num = article_num or (parts[1] if len(parts) > 1 else '')\n",
        "            article_sub = article_sub or (parts[2] if len(parts) > 2 else '0')\n",
        "\n",
        "        targeted = ''\n",
        "        paragraphs = meta.get('paragraphs', []) or []\n",
        "        if p_norm:\n",
        "            # 1) structured paragraphs 우선\n",
        "            if isinstance(paragraphs, list):\n",
        "                for para in paragraphs:\n",
        "                    if not isinstance(para, dict):\n",
        "                        continue\n",
        "                    para_num_raw = str(para.get('num', '')).strip()\n",
        "                    para_norm = norm_para_token(para_num_raw)\n",
        "                    if para_norm == p_norm:\n",
        "                        targeted = str(para.get('content', '') or '')\n",
        "                        break\n",
        "            # 2) fallback: content_original에서 ①② 패턴 직접 추출\n",
        "            if not targeted:\n",
        "                targeted = extract_by_circled_from_content(content, p_norm)\n",
        "\n",
        "        header = extract_article_header(content)\n",
        "        full_preview = str(content).replace('\\n', ' ')[:max_chars]\n",
        "        targeted_with_header = f\"{header}\\n{targeted}\".strip() if (p_norm and targeted and header) else targeted\n",
        "        targeted_preview = str(targeted_with_header).replace('\\n', ' ')[:max_chars] if targeted_with_header else ''\n",
        "\n",
        "        if p_norm:\n",
        "            final_preview = targeted_preview\n",
        "            paragraph_match = bool(targeted_preview)\n",
        "        else:\n",
        "            final_preview = targeted_preview or full_preview\n",
        "            paragraph_match = True\n",
        "\n",
        "        out.append({\n",
        "            'chunk_key': f\"{law_id}:{article_num}:{article_sub}\",\n",
        "            'law_name': meta.get('law_name', ''),\n",
        "            'article_num': article_num,\n",
        "            'article_title': meta.get('article_title', ''),\n",
        "            'paragraph': p_raw,\n",
        "            'item': it,\n",
        "            'targeted_preview': targeted_preview,\n",
        "            'content_preview': final_preview,\n",
        "            'raw_content_preview': full_preview,\n",
        "            'matched_paragraph_norm': p_norm,\n",
        "            'paragraph_match': paragraph_match,\n",
        "        })\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "def llm_should_follow_ref_by_content(\n",
        "    llm,\n",
        "    user_query: str,\n",
        "    target: str,\n",
        "    parent_chunk_preview: str,\n",
        "    ref_key: str,\n",
        "    ref_docs_preview: list[dict],\n",
        "    return_debug: bool = False,\n",
        "):\n",
        "    # 중요: 이 판단은 ref 본문을 읽고 결정하지 않는다.\n",
        "    # ref_docs_preview는 디버깅/표시용으로만 전달되며, 프롬프트에는 포함하지 않는다.\n",
        "    ref_preview_count = len(ref_docs_preview or [])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "당신은 법률 참조 추적 판단기다.\n",
        "반드시 현재 chunk 맥락만 보고, 해당 ref를 추적해야 하는지 판단하라.\n",
        "\n",
        "중요 규칙:\n",
        "- ref 조문 본문 내용은 아직 읽지 않은 상태로 판단한다.\n",
        "- 아래 입력 중 ref_docs_preview 내용은 사용하지 않는다.\n",
        "- 출력 reason에는 ref 본문에 대한 구체 인용을 쓰지 않는다.\n",
        "\n",
        "출력 JSON만:\n",
        "{{\"follow\": true/false, \"priority\": 0|1|2, \"reason\": \"...\"}}\n",
        "\n",
        "판단 기준:\n",
        "- 현재 chunk를 이해/적용하려면 참조 해석이 필수면 follow=true\n",
        "- 필수성이 낮거나 현재 chunk만으로 충분하면 follow=false\n",
        "\n",
        "query: {user_query}\n",
        "target: {target}\n",
        "parent_chunk_preview: {(parent_chunk_preview or '')[:700]}\n",
        "ref_key: {ref_key}\n",
        "ref_preview_count: {ref_preview_count}\n",
        "\"\"\".strip()\n",
        "\n",
        "    raw = llm.invoke(prompt)\n",
        "    text = getattr(raw, 'content', str(raw)).strip()\n",
        "\n",
        "    obj = {}\n",
        "    parse_error = ''\n",
        "    try:\n",
        "        obj = json.loads(text)\n",
        "    except Exception:\n",
        "        m = re.search(r'\\{[\\s\\S]*\\}', text)\n",
        "        if m:\n",
        "            try:\n",
        "                obj = json.loads(m.group(0))\n",
        "            except Exception as e:\n",
        "                parse_error = str(e)\n",
        "                obj = {}\n",
        "        else:\n",
        "            parse_error = 'json object not found'\n",
        "\n",
        "    follow_v = obj.get('follow', obj.get('추적', obj.get('expand', False)))\n",
        "    pri_v = obj.get('priority', obj.get('우선순위', 0))\n",
        "    reason_v = obj.get('reason', obj.get('이유', ''))\n",
        "\n",
        "    follow = bool(follow_v)\n",
        "    priority = int(pri_v) if str(pri_v).isdigit() else 0\n",
        "    reason = str(reason_v or '').strip()\n",
        "    if not reason:\n",
        "        reason = (text[:250] + '...') if len(text) > 250 else text\n",
        "\n",
        "    # reason에 ref 본문 직접 근거가 섞인 경우를 줄이기 위한 prefix\n",
        "    reason = f\"precheck_without_ref_content: {reason}\"\n",
        "\n",
        "    result = (follow, max(0, min(priority, 2)), reason)\n",
        "    if not return_debug:\n",
        "        return result\n",
        "\n",
        "    debug = {\n",
        "        'raw_text': text,\n",
        "        'parsed_obj': obj,\n",
        "        'parse_error': parse_error,\n",
        "        'prompt_preview': prompt[:1400],\n",
        "    }\n",
        "    return result, debug\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_ref_candidates_from_chunk(runtime, user_query: str, target: str, doc: dict, max_refs_per_chunk: int = 6) -> list[dict]:\n",
        "    content = str(doc.get('content', ''))\n",
        "    meta = doc.get('metadata', {}) or {}\n",
        "    src_key = context_key(doc)\n",
        "\n",
        "    candidates = extract_ref_candidates(meta)\n",
        "    if not candidates:\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        need_ref, need_reason = llm_need_refs_from_current_context(\n",
        "            get_runtime_llm(runtime, 'ref_expander'),\n",
        "            user_query=user_query,\n",
        "            target=target,\n",
        "            context_preview=(content or '')[:900],\n",
        "        )\n",
        "    except Exception as e:\n",
        "        need_ref, need_reason = True, f'llm_error:{e}'\n",
        "\n",
        "    out = []\n",
        "    if not need_ref:\n",
        "        for c in candidates[:max_refs_per_chunk]:\n",
        "            out.append({\n",
        "                'law_id': str(c.get('law_id', '')).strip(),\n",
        "                'article': str(c.get('article', '')).strip(),\n",
        "                'paragraph': str(c.get('paragraph', '')).strip(),\n",
        "                'item': str(c.get('item', '')).strip(),\n",
        "                'source': c.get('source', ''),\n",
        "                'source_chunk_key': src_key,\n",
        "                'follow': False,\n",
        "                'priority': 0,\n",
        "                'decision_reason': f'chunk_context_sufficient: {need_reason}',\n",
        "                'ref_docs_preview': [],\n",
        "                'raw_ref': (c.get('source_ref') or {}).get('raw', ''),\n",
        "            })\n",
        "        return out\n",
        "\n",
        "    for c in candidates[:max_refs_per_chunk]:\n",
        "        law_id = str(c.get('law_id', '')).strip()\n",
        "        article = str(c.get('article', '')).strip()\n",
        "        paragraph = str(c.get('paragraph', '')).strip()\n",
        "        item = str(c.get('item', '')).strip()\n",
        "        ref_key = f\"{law_id}:{article or '__law__'}\"\n",
        "\n",
        "        docs = get_ref_docs_for_candidate(runtime, law_id, article, user_query=user_query, target=target)\n",
        "        ref_preview = build_ref_docs_preview(\n",
        "            docs,\n",
        "            paragraph=paragraph,\n",
        "            item=item,\n",
        "            ref_key=ref_key,\n",
        "            max_docs=2,\n",
        "            max_chars=260,\n",
        "        )\n",
        "\n",
        "        if not ref_preview:\n",
        "            out.append({\n",
        "                'law_id': law_id,\n",
        "                'article': article,\n",
        "                'paragraph': paragraph,\n",
        "                'item': item,\n",
        "                'source': c.get('source', ''),\n",
        "                'source_chunk_key': src_key,\n",
        "                'follow': False,\n",
        "                'priority': 0,\n",
        "                'decision_reason': 'ref_docs_not_found',\n",
        "                'ref_docs_preview': [],\n",
        "                'raw_ref': (c.get('source_ref') or {}).get('raw', ''),\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            (follow, pri, reason), _dbg = llm_should_follow_ref_by_content(\n",
        "                get_runtime_llm(runtime, 'ref_expander'),\n",
        "                user_query=user_query,\n",
        "                target=target,\n",
        "                parent_chunk_preview=content,\n",
        "                ref_key=ref_key,\n",
        "                ref_docs_preview=ref_preview,\n",
        "                return_debug=True,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            follow, pri, reason = False, 0, f'llm_error:{e}'\n",
        "\n",
        "        out.append({\n",
        "            'law_id': law_id,\n",
        "            'article': article,\n",
        "            'paragraph': paragraph,\n",
        "            'item': item,\n",
        "            'source': c.get('source', ''),\n",
        "            'source_chunk_key': src_key,\n",
        "            'follow': bool(follow),\n",
        "            'priority': int(pri or 0),\n",
        "            'decision_reason': reason,\n",
        "            'ref_docs_preview': ref_preview,\n",
        "            'raw_ref': (c.get('source_ref') or {}).get('raw', ''),\n",
        "        })\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "def propose_refs_from_chunk(runtime, user_query: str, target: str, doc: dict, max_refs_per_chunk: int = 6) -> list[dict]:\n",
        "    reviews = evaluate_ref_candidates_from_chunk(\n",
        "        runtime,\n",
        "        user_query=user_query,\n",
        "        target=target,\n",
        "        doc=doc,\n",
        "        max_refs_per_chunk=max_refs_per_chunk,\n",
        "    )\n",
        "    out = []\n",
        "    for r in reviews:\n",
        "        if not r.get('follow'):\n",
        "            continue\n",
        "        out.append({\n",
        "            'law_id': r.get('law_id'),\n",
        "            'article': r.get('article'),\n",
        "            'source': r.get('source', ''),\n",
        "            'source_chunk_key': r.get('source_chunk_key', ''),\n",
        "            'priority': int(r.get('priority', 0)),\n",
        "            'decision_reason': r.get('decision_reason', ''),\n",
        "            'llm_reason': r.get('decision_reason', ''),\n",
        "            'ref_docs_preview': r.get('ref_docs_preview', []),\n",
        "            'raw_ref': r.get('raw_ref', ''),\n",
        "        })\n",
        "    return out\n",
        "\n",
        "\n",
        "def llm_need_refs_from_current_context(llm, user_query: str, target: str, context_preview: str, return_debug: bool = False):\n",
        "    prompt = f\"\"\"\n",
        "너는 법률 QA의 ref 필요성 판단기다.\n",
        "반드시 현재 컨텍스트만 보고 판단하라. ref 내용은 아직 보지 않는다.\n",
        "\n",
        "출력 JSON만:\n",
        "{{\"need_ref\": true/false, \"reason\": \"...\"}}\n",
        "\n",
        "판단 기준:\n",
        "- 현재 컨텍스트만으로 질문의 판단/계산/결론이 가능하면 need_ref=false\n",
        "- 참조 법령/조항의 해석이 있어야 현재 컨텍스트를 이해할 수 있으면 need_ref=true\n",
        "\n",
        "query: {user_query}\n",
        "target: {target}\n",
        "current_context_preview: {context_preview}\n",
        "\"\"\".strip()\n",
        "\n",
        "    raw = llm.invoke(prompt)\n",
        "    text = getattr(raw, 'content', str(raw)).strip()\n",
        "\n",
        "    obj = {}\n",
        "    parse_error = ''\n",
        "    try:\n",
        "        obj = json.loads(text)\n",
        "    except Exception:\n",
        "        m = re.search(r'\\{[\\s\\S]*\\}', text)\n",
        "        if m:\n",
        "            try:\n",
        "                obj = json.loads(m.group(0))\n",
        "            except Exception as e:\n",
        "                parse_error = str(e)\n",
        "                obj = {}\n",
        "        else:\n",
        "            parse_error = 'json object not found'\n",
        "\n",
        "    need = bool(obj.get('need_ref', obj.get('need_refs', obj.get('expand', False))))\n",
        "    reason = str(obj.get('reason', obj.get('이유', '')) or '').strip() or (text[:220] if text else '')\n",
        "\n",
        "    result = (need, reason)\n",
        "    if not return_debug:\n",
        "        return result\n",
        "    return result, {'raw_text': text, 'parsed_obj': obj, 'parse_error': parse_error, 'prompt_preview': prompt[:1200]}\n",
        "\n",
        "\n",
        "def llm_need_refs_for_state_context(runtime, state: dict):\n",
        "    contexts = list(state.get('contexts', []) or [])\n",
        "    ctx = []\n",
        "    for c in contexts[:6]:\n",
        "        m = c.get('metadata', {}) or {}\n",
        "        ctx.append({\n",
        "            'chunk_key': f\"{m.get('law_id')}:{m.get('article_num')}:{m.get('article_sub','0')}\",\n",
        "            'law_name': m.get('law_name',''),\n",
        "            'article_num': m.get('article_num',''),\n",
        "            'article_title': m.get('article_title',''),\n",
        "            'excerpt': (c.get('content') or '').replace('\\n',' ')[:220],\n",
        "        })\n",
        "    return llm_need_refs_from_current_context(\n",
        "        get_runtime_llm(runtime, 'ref_expander'),\n",
        "        user_query=state.get('user_query',''),\n",
        "        target=', '.join(state.get('targets',[])) or '일반',\n",
        "        context_preview=str(ctx),\n",
        "    )\n",
        "\n",
        "\n",
        "def retrieve_related_chunks_in_law(runtime, user_query: str, target: str, law_id: str, k: int = 2):\n",
        "    lid = str(law_id).strip().zfill(6)\n",
        "    queries = [user_query, target, f\"{user_query} {target}\", f\"{target} 관련 조문\"]\n",
        "    out = []\n",
        "    seen = set()\n",
        "    for q in queries:\n",
        "        docs = search_law_chunks(runtime['store'], q, k=max(8, k * 4))\n",
        "        for d in docs:\n",
        "            m = d.get('metadata', {}) or {}\n",
        "            if str(m.get('law_id','')).strip().zfill(6) != lid:\n",
        "                continue\n",
        "            ckey = context_key(d)\n",
        "            if ckey in seen:\n",
        "                continue\n",
        "            seen.add(ckey)\n",
        "            out.append(d)\n",
        "            if len(out) >= k:\n",
        "                return out\n",
        "    return out\n",
        "\n",
        "\n",
        "def get_ref_docs_for_candidate(runtime, law_id: str, article: str, user_query: str, target: str):\n",
        "    lid = str(law_id).strip()\n",
        "    art = str(article).strip()\n",
        "    if not lid:\n",
        "        return []\n",
        "    if art:\n",
        "        return get_ref_docs_cached(runtime, lid, art)\n",
        "    return retrieve_related_chunks_in_law(runtime, user_query=user_query, target=target, law_id=lid, k=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fb7797c",
      "metadata": {},
      "source": [
        "## Step E. Target 단위 Agent (Single Target Worker)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede8da30",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_target_agent(runtime, user_query: str, target: str, k: int = 6) -> dict:\n",
        "    q1 = f\"{target}\"\n",
        "    q2 = f\"{user_query}\"\n",
        "    q3 = f\"{user_query} 관련 핵심: {target}\"\n",
        "\n",
        "    per_query_k = max(k, 4)\n",
        "    query_plan = [(q1, per_query_k), (q2, per_query_k), (q3, per_query_k)]\n",
        "\n",
        "    hits = []\n",
        "    for q, qk in query_plan:\n",
        "        hits.extend(search_law_chunks(runtime['store'], q, k=qk))\n",
        "    hits = dedup_contexts(hits)\n",
        "\n",
        "    if len(hits) < k:\n",
        "        for bq in [f\"{target} 건축법 조문\", f\"{target} 건축법 시행령 조문\", f\"{user_query} 법령 조문\"]:\n",
        "            if len(hits) >= k:\n",
        "                break\n",
        "            hits.extend(search_law_chunks(runtime['store'], bq, k=max(k * 2, 8)))\n",
        "            hits = dedup_contexts(hits)\n",
        "\n",
        "    hits = hits[:k]\n",
        "\n",
        "    pending_refs = []\n",
        "    seen = set()\n",
        "    chunk_decisions = []\n",
        "\n",
        "    for h in hits:\n",
        "        meta = h.get('metadata', {}) or {}\n",
        "        chunk_key = f\"{meta.get('law_id')}:{meta.get('article_num')}:{meta.get('article_sub', '0')}\"\n",
        "\n",
        "        reviews = evaluate_ref_candidates_from_chunk(\n",
        "            runtime,\n",
        "            user_query=user_query,\n",
        "            target=target,\n",
        "            doc=h,\n",
        "            max_refs_per_chunk=6,\n",
        "        )\n",
        "\n",
        "        proposed_refs = []\n",
        "        reasons = []\n",
        "        for r in reviews:\n",
        "            if not r.get('follow'):\n",
        "                reasons.append(str(r.get('decision_reason', '')))\n",
        "                continue\n",
        "            cand = {\n",
        "                'law_id': r.get('law_id'),\n",
        "                'article': r.get('article', ''),\n",
        "                'source': r.get('source', ''),\n",
        "                'source_chunk_key': chunk_key,\n",
        "                'priority': int(r.get('priority', 0)),\n",
        "                'decision_reason': r.get('decision_reason', ''),\n",
        "                'llm_reason': r.get('decision_reason', ''),\n",
        "                'ref_docs_preview': r.get('ref_docs_preview', []),\n",
        "                'raw_ref': r.get('raw_ref', ''),\n",
        "            }\n",
        "            rk = (str(cand.get('law_id','')), str(cand.get('article','')))\n",
        "            if rk in seen:\n",
        "                continue\n",
        "            seen.add(rk)\n",
        "            pending_refs.append(cand)\n",
        "            proposed_refs.append(f\"{cand.get('law_id')}:{cand.get('article') or '__law__'}\")\n",
        "\n",
        "        should_expand = len(proposed_refs) > 0\n",
        "        llm_reason = '; '.join([x for x in reasons if x][:2]) if reasons else ('needs_ref' if should_expand else 'chunk_context_sufficient')\n",
        "\n",
        "        chunk_decisions.append({\n",
        "            'chunk_key': chunk_key,\n",
        "            'target': target,\n",
        "            'ref_candidates_count': len(reviews),\n",
        "            'rule_expand': None,\n",
        "            'rule_reasons': [],\n",
        "            'llm_expand': should_expand,\n",
        "            'llm_priority': max([int(x.get('priority',0)) for x in reviews] + [0]),\n",
        "            'llm_reason': llm_reason,\n",
        "            'llm_raw_text': '',\n",
        "            'should_expand': should_expand,\n",
        "            'decision_reason': llm_reason,\n",
        "            'proposed_refs': proposed_refs,\n",
        "        })\n",
        "\n",
        "    pending_refs.sort(key=lambda x: int(x.get('priority', 0)), reverse=True)\n",
        "\n",
        "    return {\n",
        "        'target': target,\n",
        "        'hits': hits,\n",
        "        'pending_refs': pending_refs,\n",
        "        'chunk_decisions': chunk_decisions,\n",
        "        'retrieve_debug': {'requested_k': k, 'returned_k': len(hits), 'query_plan': query_plan},\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16f8bea2",
      "metadata": {},
      "source": [
        "## Step F. Multi-Agent Fan-out / Fan-in\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc854ce0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f7b2689",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_multi_target_agents(runtime, user_query: str, targets: list[str], k: int = 6) -> dict:\n",
        "    targets = targets or ['일반']\n",
        "    outputs = []\n",
        "\n",
        "    max_workers = min(4, max(1, len(targets)))\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        futures = [ex.submit(run_target_agent, runtime, user_query, t, k) for t in targets]\n",
        "        for f in as_completed(futures):\n",
        "            outputs.append(f.result())\n",
        "\n",
        "    target_hits = {o['target']: o['hits'] for o in outputs}\n",
        "    target_decisions = {o['target']: o.get('chunk_decisions', []) for o in outputs}\n",
        "    target_retrieve_debug = {o['target']: o.get('retrieve_debug', {}) for o in outputs}\n",
        "\n",
        "    all_contexts = []\n",
        "    all_pending_refs = []\n",
        "    seen_ref = set()\n",
        "\n",
        "    for o in outputs:\n",
        "        all_contexts.extend(o['hits'])\n",
        "        for r in o['pending_refs']:\n",
        "            rk = (r.get('law_id'), r.get('article'))\n",
        "            if rk in seen_ref:\n",
        "                continue\n",
        "            seen_ref.add(rk)\n",
        "            all_pending_refs.append(r)\n",
        "\n",
        "    all_pending_refs.sort(key=lambda x: int(x.get('priority', 0)), reverse=True)\n",
        "\n",
        "    return {\n",
        "        'target_hits': target_hits,\n",
        "        'target_decisions': target_decisions,\n",
        "        'contexts': dedup_contexts(all_contexts),\n",
        "        'pending_refs': all_pending_refs,\n",
        "        'target_retrieve_debug': target_retrieve_debug,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5975dd10",
      "metadata": {},
      "source": [
        "## Step F-1. 디버깅: Retrieve/확장/Hop 추적\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0fcb91b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_target_hits(target_hits: dict[str, list[dict]]) -> list[dict]:\n",
        "    rows = []\n",
        "    for target, docs in (target_hits or {}).items():\n",
        "        rows.append({'target': target, 'retrieved_chunks': len(docs)})\n",
        "    rows.sort(key=lambda x: x['target'])\n",
        "    return rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef8d74c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def inspect_target_hits(target_hits: dict[str, list[dict]], target: str, limit: int = 10) -> list[dict]:\n",
        "    rows = []\n",
        "    docs = (target_hits or {}).get(target, [])\n",
        "    for d in docs[:limit]:\n",
        "        m = d.get('metadata', {})\n",
        "        rows.append({\n",
        "            'chunk_key': f\"{m.get('law_id')}:{m.get('article_num')}:{m.get('article_sub', '0')}\",\n",
        "            'law_name': m.get('law_name'),\n",
        "            'article_num': m.get('article_num'),\n",
        "            'article_sub': m.get('article_sub', ''),\n",
        "            'article_title': m.get('article_title'),\n",
        "            'internal_ref_cnt': len(m.get('internal_refs', []) or []),\n",
        "            'external_ref_cnt': len(m.get('external_refs', []) or []),\n",
        "            'content_head': (d.get('content', '') or '')[:180],\n",
        "        })\n",
        "    return rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a654e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def inspect_expand_decisions(runtime, user_query: str, target: str, docs: list[dict], limit: int = 10) -> list[dict]:\n",
        "    rows = []\n",
        "    for d in docs[:limit]:\n",
        "        m = d.get('metadata', {})\n",
        "\n",
        "        reviews = evaluate_ref_candidates_from_chunk(\n",
        "            runtime,\n",
        "            user_query=user_query,\n",
        "            target=target,\n",
        "            doc=d,\n",
        "            max_refs_per_chunk=10,\n",
        "        )\n",
        "\n",
        "        proposed = [\n",
        "            {\n",
        "                'law_id': x.get('law_id'),\n",
        "                'article': x.get('article'),\n",
        "                'paragraph': x.get('paragraph', ''),\n",
        "                'item': x.get('item', ''),\n",
        "                'source': x.get('source'),\n",
        "                'priority': x.get('priority', 0),\n",
        "                'decision_reason': x.get('decision_reason', ''),\n",
        "                'raw_ref': x.get('raw_ref', ''),\n",
        "                'ref_docs_preview': x.get('ref_docs_preview', []),\n",
        "            }\n",
        "            for x in reviews if x.get('follow')\n",
        "        ]\n",
        "\n",
        "        rows.append({\n",
        "            'chunk_key': f\"{m.get('law_id')}:{m.get('article_num')}:{m.get('article_sub', '0')}\",\n",
        "            'target': target,\n",
        "            'ref_candidates_count': len(reviews),\n",
        "            'ref_candidates_preview': [\n",
        "                {\n",
        "                    'law_id': x.get('law_id'),\n",
        "                    'article': x.get('article'),\n",
        "                    'paragraph': x.get('paragraph', ''),\n",
        "                    'item': x.get('item', ''),\n",
        "                    'source': x.get('source'),\n",
        "                    'raw': x.get('raw_ref', ''),\n",
        "                    'follow': x.get('follow', False),\n",
        "                    'priority': x.get('priority', 0),\n",
        "                    'reason': x.get('decision_reason', ''),\n",
        "                    'ref_docs_preview': x.get('ref_docs_preview', []),\n",
        "                }\n",
        "                for x in reviews\n",
        "            ],\n",
        "            'llm_expand': any(x.get('follow') for x in reviews),\n",
        "            'llm_priority': max([int(x.get('priority', 0)) for x in reviews] + [0]),\n",
        "            'llm_reason': 'ref별 본문+항/호 기반 판단 적용',\n",
        "            'llm_raw_text': '',\n",
        "            'llm_parsed_obj': {},\n",
        "            'llm_parse_error': '',\n",
        "            'llm_prompt_preview': '',\n",
        "            'proposed_ref_cnt': len(proposed),\n",
        "            'proposed_refs': proposed,\n",
        "        })\n",
        "    return rows\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b031f4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_expand_detail(rows: list[dict], idx: int = 0):\n",
        "    if not rows:\n",
        "        print('rows is empty')\n",
        "        return\n",
        "    r = rows[idx]\n",
        "    print('chunk_key:', r.get('chunk_key'))\n",
        "    print('target:', r.get('target'))\n",
        "    print('rule_expand:', r.get('rule_expand'))\n",
        "    print('rule_reasons:', r.get('rule_reasons'))\n",
        "    print('rule_cue_matches:', r.get('rule_cue_matches'))\n",
        "    print('ref_candidates_count:', r.get('ref_candidates_count'))\n",
        "    print('ref_candidates_preview:', r.get('ref_candidates_preview'))\n",
        "    print('llm_expand:', r.get('llm_expand'))\n",
        "    print('llm_priority:', r.get('llm_priority'))\n",
        "    print('llm_reason:', r.get('llm_reason'))\n",
        "    print('llm_parse_error:', r.get('llm_parse_error'))\n",
        "    print('llm_raw_text:', r.get('llm_raw_text'))\n",
        "    print('proposed_ref_cnt:', r.get('proposed_ref_cnt'))\n",
        "    print('proposed_refs:', r.get('proposed_refs'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe381e3b",
      "metadata": {},
      "source": [
        "## Step F-2. 초세부 디버깅 (청크 1개 단위)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f630f16c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pick_doc_for_debug(target_hits: dict[str, list[dict]], target: str, idx: int = 0) -> dict:\n",
        "    docs = (target_hits or {}).get(target, [])\n",
        "    if not docs:\n",
        "        return {}\n",
        "    if idx < 0:\n",
        "        idx = 0\n",
        "    if idx >= len(docs):\n",
        "        idx = len(docs) - 1\n",
        "    return docs[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afd21a67",
      "metadata": {},
      "source": [
        "### 디버깅 실행 예시 2: chunk별 ref 확장 판단 확인\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "033e8009",
      "metadata": {},
      "source": [
        "## Step G. Graph Node 함수\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce6ed0ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "def node_extract_targets(state: GraphState, runtime) -> GraphState:\n",
        "    q = state.get('user_query', '')\n",
        "    state['targets'] = extract_targets_with_llm(get_runtime_llm(runtime, 'target_extractor'), q)\n",
        "    state['hop_count'] = 0\n",
        "    state['max_hops'] = state.get('max_hops', 3)\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "701256b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def node_retrieve_multi_agent(state: GraphState, runtime) -> GraphState:\n",
        "    out = run_multi_target_agents(runtime, state.get('user_query', ''), state.get('targets', []), k=6)\n",
        "    state['target_hits'] = out['target_hits']\n",
        "    state['target_decisions'] = out.get('target_decisions', {})\n",
        "    state['target_retrieve_debug'] = out.get('target_retrieve_debug', {})\n",
        "    state['hits'] = out['contexts']\n",
        "    state['contexts'] = list(out['contexts'])\n",
        "    state['pending_refs'] = list(out['pending_refs'])\n",
        "    state['seen_ref_keys'] = []\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a9363c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def node_reference_tracker(state: GraphState, runtime) -> GraphState:\n",
        "    pending = list(state.get('pending_refs', []))\n",
        "    if not pending:\n",
        "        return state\n",
        "\n",
        "    try:\n",
        "        need_ref, need_reason = llm_need_refs_for_state_context(runtime, state)\n",
        "    except Exception as e:\n",
        "        need_ref, need_reason = True, f'llm_error:{e}'\n",
        "\n",
        "    if not need_ref:\n",
        "        state['pending_refs'] = []\n",
        "        state['seen_ref_keys'] = list(state.get('seen_ref_keys', []))\n",
        "        logs = list(state.get('ref_batch_decisions', []))\n",
        "        logs.append({\n",
        "            'hop': int(state.get('hop_count', 0)),\n",
        "            'expand_count': 0,\n",
        "            'batch_reason': f'skip_ref: {need_reason}',\n",
        "            'picked_refs': [],\n",
        "            'checked_refs': [],\n",
        "            'fetched_chunks': [],\n",
        "            'llm_raw_text': '',\n",
        "            'llm_parsed_obj': {},\n",
        "            'llm_parse_error': '',\n",
        "        })\n",
        "        state['ref_batch_decisions'] = logs\n",
        "        state['hop_count'] = int(state.get('hop_count', 0)) + 1\n",
        "        return state\n",
        "\n",
        "    seen_ref_keys = list(state.get('seen_ref_keys', []))\n",
        "    seen_set = set(seen_ref_keys)\n",
        "\n",
        "    candidates = []\n",
        "    for r in pending:\n",
        "        ref_key = f\"{r.get('law_id')}:{r.get('article') or '__law__'}\"\n",
        "        if ref_key in seen_set:\n",
        "            continue\n",
        "        candidates.append(r)\n",
        "\n",
        "    if not candidates:\n",
        "        state['pending_refs'] = pending\n",
        "        state['seen_ref_keys'] = list(seen_set)\n",
        "        state['hop_count'] = int(state.get('hop_count', 0)) + 1\n",
        "        return state\n",
        "\n",
        "    hop_count = int(state.get('hop_count', 0))\n",
        "    max_hops = int(state.get('max_hops', 3))\n",
        "\n",
        "    checked = []\n",
        "    for r in candidates[:12]:\n",
        "        law_id = str(r.get('law_id', '')).strip()\n",
        "        article = str(r.get('article', '')).strip()\n",
        "        ref_key = f\"{law_id}:{article or '__law__'}\"\n",
        "        docs = get_ref_docs_for_candidate(runtime, law_id=law_id, article=article, user_query=state.get('user_query', ''), target=', '.join(state.get('targets', [])) or '일반')\n",
        "        ref_preview = build_ref_docs_preview(docs, ref_key=ref_key)\n",
        "        if not ref_preview:\n",
        "            checked.append({**r, 'follow': False, 'priority': 0, 'check_reason': 'ref_docs_not_found', 'ref_docs_preview': []})\n",
        "            continue\n",
        "        try:\n",
        "            follow, pri, why = llm_should_follow_ref_by_content(\n",
        "                get_runtime_llm(runtime, 'ref_expander'),\n",
        "                user_query=state.get('user_query', ''),\n",
        "                target=', '.join(state.get('targets', [])) or '일반',\n",
        "                parent_chunk_preview='',\n",
        "                ref_key=ref_key,\n",
        "                ref_docs_preview=ref_preview,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            follow, pri, why = False, 0, f'llm_error:{e}'\n",
        "        checked.append({**r, 'follow': bool(follow), 'priority': int(pri or 0), 'check_reason': why, 'ref_docs_preview': ref_preview})\n",
        "\n",
        "    checked.sort(key=lambda x: (1 if x.get('follow') else 0, int(x.get('priority',0))), reverse=True)\n",
        "    follow_candidates = [x for x in checked if x.get('follow')]\n",
        "    candidates = follow_candidates + [x for x in checked if not x.get('follow')] if follow_candidates else checked\n",
        "\n",
        "    try:\n",
        "        (expand_count, batch_reason), batch_debug = llm_select_ref_batch_size(\n",
        "            get_runtime_llm(runtime, 'ref_batch_planner'),\n",
        "            user_query=state.get('user_query', ''),\n",
        "            targets=state.get('targets', []),\n",
        "            hop_count=hop_count,\n",
        "            max_hops=max_hops,\n",
        "            pending_refs=candidates,\n",
        "            max_batch=4,\n",
        "            return_debug=True,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        expand_count, batch_reason = 1, f'llm_error:{e}'\n",
        "        batch_debug = {'raw_text': '', 'parsed_obj': {}, 'parse_error': str(e), 'prompt_preview': ''}\n",
        "\n",
        "    picked = candidates[:max(1, min(expand_count, len(candidates)))]\n",
        "    picked_keys = [f\"{r.get('law_id')}:{r.get('article') or '__law__'}\" for r in picked]\n",
        "\n",
        "    picked_counter = {}\n",
        "    for r in picked:\n",
        "        rk = (str(r.get('law_id')), str(r.get('article')))\n",
        "        picked_counter[rk] = picked_counter.get(rk, 0) + 1\n",
        "\n",
        "    remaining = []\n",
        "    for r in pending:\n",
        "        rk = (str(r.get('law_id')), str(r.get('article')))\n",
        "        if picked_counter.get(rk, 0) > 0:\n",
        "            picked_counter[rk] -= 1\n",
        "            continue\n",
        "        remaining.append(r)\n",
        "    pending = remaining\n",
        "\n",
        "    add_refs = []\n",
        "    merged_contexts = list(state.get('contexts', []))\n",
        "    target_hint = ', '.join(state.get('targets', [])) or '일반'\n",
        "    fetched_chunks = []\n",
        "\n",
        "    for ref in picked:\n",
        "        ref_key = f\"{ref.get('law_id')}:{ref.get('article') or '__law__'}\"\n",
        "        seen_set.add(ref_key)\n",
        "\n",
        "        law_id = str(ref.get('law_id', '')).strip()\n",
        "        article = str(ref.get('article', '')).strip()\n",
        "        if not law_id:\n",
        "            fetched_chunks.append({'ref_key': ref_key, 'fetched_count': 0, 'fetched_keys': [], 'fetched_previews': [], 'note': 'invalid law_id'})\n",
        "            continue\n",
        "\n",
        "        docs = get_ref_docs_for_candidate(runtime, law_id=law_id, article=article, user_query=state.get('user_query',''), target=target_hint)\n",
        "\n",
        "        row = {\n",
        "            'ref_key': ref_key,\n",
        "            'fetched_count': len(docs or []),\n",
        "            'fetched_keys': [],\n",
        "            'fetched_previews': [],\n",
        "            'source': ref.get('source', ''),\n",
        "            'priority': ref.get('priority', 0),\n",
        "            'decision_reason': str(ref.get('decision_reason', ''))[:200],\n",
        "        }\n",
        "\n",
        "        if not docs:\n",
        "            fetched_chunks.append(row)\n",
        "            continue\n",
        "\n",
        "        for d in docs:\n",
        "            m = d.get('metadata', {}) or {}\n",
        "            ckey = f\"{m.get('law_id')}:{m.get('article_num')}:{m.get('article_sub', '0')}\"\n",
        "            row['fetched_keys'].append(ckey)\n",
        "            row['fetched_previews'].append((d.get('content') or '').replace('\\n', ' ')[:180])\n",
        "\n",
        "        fetched_chunks.append(row)\n",
        "        merged_contexts = dedup_contexts(merged_contexts + list(docs))\n",
        "\n",
        "        for d in docs:\n",
        "            add_refs.extend(\n",
        "                propose_refs_from_chunk(\n",
        "                    runtime,\n",
        "                    user_query=state.get('user_query', ''),\n",
        "                    target=target_hint,\n",
        "                    doc=d,\n",
        "                    max_refs_per_chunk=6,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    exist = {(x.get('law_id'), x.get('article')) for x in pending}\n",
        "    exist |= {(k.split(':', 1)[0], k.split(':', 1)[1] if ':' in k else '') for k in seen_set}\n",
        "    for r in add_refs:\n",
        "        rk = (r.get('law_id'), r.get('article'))\n",
        "        if rk in exist:\n",
        "            continue\n",
        "        pending.append(r)\n",
        "        exist.add(rk)\n",
        "\n",
        "    pending.sort(key=lambda x: int(x.get('priority', 0)), reverse=True)\n",
        "\n",
        "    state['contexts'] = merged_contexts\n",
        "    state['pending_refs'] = pending\n",
        "    state['seen_ref_keys'] = list(seen_set)\n",
        "    state['hop_count'] = hop_count + 1\n",
        "\n",
        "    logs = list(state.get('ref_batch_decisions', []))\n",
        "    logs.append({\n",
        "        'hop': hop_count,\n",
        "        'expand_count': len(picked),\n",
        "        'batch_reason': batch_reason,\n",
        "        'picked_refs': picked_keys,\n",
        "        'checked_refs': [\n",
        "            {\n",
        "                'ref_key': f\"{x.get('law_id')}:{x.get('article') or '__law__'}\",\n",
        "                'follow': x.get('follow', False),\n",
        "                'priority': x.get('priority', 0),\n",
        "                'check_reason': x.get('check_reason', ''),\n",
        "                'source': x.get('source', ''),\n",
        "                'ref_docs_preview': x.get('ref_docs_preview', []),\n",
        "            }\n",
        "            for x in checked\n",
        "        ],\n",
        "        'fetched_chunks': fetched_chunks,\n",
        "        'llm_raw_text': batch_debug.get('raw_text', ''),\n",
        "        'llm_parsed_obj': batch_debug.get('parsed_obj', {}),\n",
        "        'llm_parse_error': batch_debug.get('parse_error', ''),\n",
        "        'need_ref_reason': need_reason,\n",
        "    })\n",
        "    state['ref_batch_decisions'] = logs\n",
        "\n",
        "    return state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "061233da",
      "metadata": {},
      "outputs": [],
      "source": [
        "def route_ref_loop(state: GraphState) -> str:\n",
        "    if state.get('pending_refs') and state.get('hop_count', 0) < state.get('max_hops', 3):\n",
        "        return 'reference_tracker'\n",
        "    return 'appendix'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40a793ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "def node_appendix(state: GraphState, runtime) -> GraphState:\n",
        "    merged = []\n",
        "    seen = set()\n",
        "    for t in state.get('targets', []):\n",
        "        for x in lookup_appendix1_term(t, runtime['terms'])[:3]:\n",
        "            key = (x.get('category'), x.get('subcategory'))\n",
        "            if key in seen:\n",
        "                continue\n",
        "            seen.add(key)\n",
        "            merged.append(x)\n",
        "    state['appendix'] = merged\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f98afec",
      "metadata": {},
      "outputs": [],
      "source": [
        "def node_generate(state: GraphState, runtime) -> GraphState:\n",
        "    contexts = state.get('contexts', [])\n",
        "    targets = state.get('targets', [])\n",
        "    target_hits = {k: len(v) for k, v in (state.get('target_hits', {}) or {}).items()}\n",
        "    user_query = state.get('user_query', '')\n",
        "\n",
        "    # query 조건에 맞는 답변 생성을 위해 근거 원문 일부를 함께 전달\n",
        "    evidence = []\n",
        "    for c in contexts[:14]:\n",
        "        m = c.get('metadata', {})\n",
        "        txt = (c.get('content') or '').replace('\\n', ' ').strip()\n",
        "        evidence.append({\n",
        "            'law_name': m.get('law_name'),\n",
        "            'law_id': m.get('law_id'),\n",
        "            'article_num': m.get('article_num'),\n",
        "            'article_sub': m.get('article_sub', '0'),\n",
        "            'article_title': m.get('article_title'),\n",
        "            'excerpt': txt[:420],\n",
        "        })\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "당신은 건축법률 답변 생성기다.\n",
        "반드시 사용자 질문(query)의 조건을 기준으로 답하라.\n",
        "\n",
        "출력 규칙:\n",
        "1) 먼저 query에서 조건을 명시적으로 정리한다.\n",
        "2) 근거 조항을 최소 1개 제시하고, 각 조항에 대해 근거 excerpt를 함께 인용한다.\n",
        "3) 수치/거리(예: 3m)를 말할 때는 반드시 evidence excerpt에 같은 값이 있어야 한다.\n",
        "4) evidence에 없는 수치/조건은 추정하지 말고 '자료상 확인 불가'로 표시한다.\n",
        "5) 마지막에 '추가 필요 입력값'을 bullet로 제시한다.\n",
        "6) 한국어로 답변한다.\n",
        "\n",
        "query: {user_query}\n",
        "target_hits_count: {target_hits}\n",
        "appendix: {state.get('appendix', [])}\n",
        "evidence: {evidence}\n",
        "\"\"\".strip()\n",
        "\n",
        "    ans = get_runtime_llm(runtime, 'answer_generator').invoke(prompt)\n",
        "    state['answer'] = getattr(ans, 'content', str(ans))\n",
        "    return state\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "589b4271",
      "metadata": {},
      "source": [
        "## Step H. Graph 조립/실행\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9693a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_graph_rag(runtime):\n",
        "    if StateGraph is None:\n",
        "        raise ImportError('langgraph가 필요합니다. conda run -n natna pip install langgraph')\n",
        "\n",
        "    g = StateGraph(GraphState)\n",
        "\n",
        "    g.add_node('extract_targets', lambda s: node_extract_targets(s, runtime))\n",
        "    g.add_node('retrieve_multi_agent', lambda s: node_retrieve_multi_agent(s, runtime))\n",
        "    g.add_node('reference_tracker', lambda s: node_reference_tracker(s, runtime))\n",
        "    g.add_node('appendix', lambda s: node_appendix(s, runtime))\n",
        "    g.add_node('generate', lambda s: node_generate(s, runtime))\n",
        "\n",
        "    g.set_entry_point('extract_targets')\n",
        "    g.add_edge('extract_targets', 'retrieve_multi_agent')\n",
        "    g.add_edge('retrieve_multi_agent', 'reference_tracker')\n",
        "    g.add_conditional_edges(\n",
        "        'reference_tracker',\n",
        "        route_ref_loop,\n",
        "        {\n",
        "            'reference_tracker': 'reference_tracker',\n",
        "            'appendix': 'appendix',\n",
        "        },\n",
        "    )\n",
        "    g.add_edge('appendix', 'generate')\n",
        "    g.add_edge('generate', END)\n",
        "\n",
        "    return g.compile()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e0c4dbb",
      "metadata": {},
      "source": [
        "## Step H-1. 수동 실행 모드 (노드별 한 단계씩)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c480217",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_graph_once(user_query: str, max_hops: int = 3):\n",
        "    runtime = init_runtime()\n",
        "    app = build_graph_rag(runtime)\n",
        "    out = app.invoke({'user_query': user_query, 'max_hops': max_hops})\n",
        "    return {\n",
        "        'targets': out.get('targets', []),\n",
        "        'target_hits_count': {k: len(v) for k, v in (out.get('target_hits', {}) or {}).items()},\n",
        "        'retrieved': len(out.get('contexts', [])),\n",
        "        'appendix': out.get('appendix', []),\n",
        "        'answer': out.get('answer', ''),\n",
        "        'contexts': out.get('contexts', []),\n",
        "        'pending_refs': out.get('pending_refs', []),\n",
        "        'seen_ref_keys': out.get('seen_ref_keys', []),\n",
        "        'hop_count': out.get('hop_count', 0),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec74e9b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_hop_debug(runtime, user_query: str, targets: list[str], max_hops: int = 5, k: int = 6) -> dict:\n",
        "    # 초기 fan-out/fan-in\n",
        "    init = run_multi_target_agents(runtime, user_query=user_query, targets=targets, k=k)\n",
        "    state = {\n",
        "        'user_query': user_query,\n",
        "        'targets': targets,\n",
        "        'target_hits': init['target_hits'],\n",
        "        'hits': init['contexts'],\n",
        "        'contexts': list(init['contexts']),\n",
        "        'pending_refs': list(init['pending_refs']),\n",
        "        'seen_ref_keys': [],\n",
        "        'hop_count': 0,\n",
        "        'max_hops': max_hops,\n",
        "    }\n",
        "\n",
        "    hop_logs = []\n",
        "    for hop in range(max_hops):\n",
        "        before_pending = len(state.get('pending_refs', []))\n",
        "        before_ctx = len(state.get('contexts', []))\n",
        "\n",
        "        if before_pending == 0:\n",
        "            hop_logs.append({\n",
        "                'hop': hop,\n",
        "                'before_pending': before_pending,\n",
        "                'after_pending': before_pending,\n",
        "                'before_contexts': before_ctx,\n",
        "                'after_contexts': before_ctx,\n",
        "                'picked_ref': None,\n",
        "                'note': 'no pending refs, stop',\n",
        "            })\n",
        "            break\n",
        "\n",
        "        next_ref = state['pending_refs'][0]\n",
        "        picked_key = f\"{next_ref.get('law_id')}:{next_ref.get('article')}\"\n",
        "\n",
        "        state = node_reference_tracker(state, runtime)\n",
        "\n",
        "        after_pending = len(state.get('pending_refs', []))\n",
        "        after_ctx = len(state.get('contexts', []))\n",
        "\n",
        "        hop_logs.append({\n",
        "            'hop': hop,\n",
        "            'before_pending': before_pending,\n",
        "            'after_pending': after_pending,\n",
        "            'before_contexts': before_ctx,\n",
        "            'after_contexts': after_ctx,\n",
        "            'picked_ref': picked_key,\n",
        "            'new_contexts': after_ctx - before_ctx,\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        'state': state,\n",
        "        'hop_logs': hop_logs,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9d4f6d7",
      "metadata": {},
      "source": [
        "### 디버깅 실행 예시 3: hop별로 ref를 어떻게 타는지 확인\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8341538b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 인터랙티브 reference 그래프 (PyVis, notebook inline srcdoc)\n",
        "import hashlib\n",
        "import html as _html\n",
        "\n",
        "\n",
        "def build_reference_graph_data(contexts):\n",
        "    nodes = {}\n",
        "    edges = []\n",
        "\n",
        "    in_scope = {\n",
        "        '건축법': '001823',\n",
        "        '건축법 시행령': '002118',\n",
        "    }\n",
        "    law_id_to_name = {v: k for k, v in in_scope.items()}\n",
        "\n",
        "    circled_to_num = {\n",
        "        '①': '1', '②': '2', '③': '3', '④': '4', '⑤': '5',\n",
        "        '⑥': '6', '⑦': '7', '⑧': '8', '⑨': '9', '⑩': '10',\n",
        "        '⑪': '11', '⑫': '12', '⑬': '13', '⑭': '14', '⑮': '15',\n",
        "        '⑯': '16', '⑰': '17', '⑱': '18', '⑲': '19', '⑳': '20',\n",
        "    }\n",
        "\n",
        "    def norm_para(v: str) -> str:\n",
        "        t = str(v or '').strip()\n",
        "        if not t:\n",
        "            return ''\n",
        "        m = re.search(r'제\\s*(\\d+)\\s*항', t)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "        if t in circled_to_num:\n",
        "            return circled_to_num[t]\n",
        "        m = re.search(r'(\\d+)', t)\n",
        "        return m.group(1) if m else ''\n",
        "\n",
        "    def safe_meta(item):\n",
        "        if isinstance(item, dict):\n",
        "            return item.get('metadata', {}) or {}\n",
        "        return {}\n",
        "\n",
        "    def safe_content(item):\n",
        "        if isinstance(item, dict):\n",
        "            return str(item.get('content', ''))\n",
        "        return ''\n",
        "\n",
        "    def add_article_placeholder_if_missing(chunk_key: str):\n",
        "        if chunk_key in nodes:\n",
        "            return\n",
        "        parts = chunk_key.split(':')\n",
        "        if len(parts) < 3:\n",
        "            return\n",
        "        lid, anum, asub = parts[0], parts[1], parts[2]\n",
        "        lname = law_id_to_name.get(lid, lid)\n",
        "        sub_txt = f\"의{asub}\" if asub not in ['', '0'] else ''\n",
        "        nodes[chunk_key] = {\n",
        "            'id': chunk_key,\n",
        "            'label': f\"{lname}\\n제{anum}조{sub_txt}\\n(미조회)\",\n",
        "            'title': f\"[{chunk_key}] {lname} 제{anum}조{sub_txt} - 아직 contexts에 없음\",\n",
        "            'group': 'unfetched',\n",
        "        }\n",
        "\n",
        "    for idx, item in enumerate(contexts):\n",
        "        meta = safe_meta(item)\n",
        "        content = safe_content(item)\n",
        "\n",
        "        law_id = str(meta.get('law_id', '') or '').zfill(6)\n",
        "        article_num = str(meta.get('article_num', '') or '')\n",
        "        article_sub = str(meta.get('article_sub', '') or '0')\n",
        "\n",
        "        if law_id and article_num:\n",
        "            article_key = f\"{law_id}:{article_num}:{article_sub if article_sub else '0'}\"\n",
        "        else:\n",
        "            article_key = f\"doc:{idx}\"\n",
        "\n",
        "        law_name = str(meta.get('law_name', law_id_to_name.get(law_id, 'unknown')))\n",
        "        title = str(meta.get('article_title', ''))\n",
        "        sub_txt = f\"의{article_sub}\" if article_sub not in ['', '0'] else ''\n",
        "\n",
        "        paragraphs = meta.get('paragraphs', []) or []\n",
        "        para_nodes = []\n",
        "        para_content_by_num = {}\n",
        "\n",
        "        if isinstance(paragraphs, list) and paragraphs:\n",
        "            for p in paragraphs:\n",
        "                if not isinstance(p, dict):\n",
        "                    continue\n",
        "                pnum = norm_para(p.get('num', ''))\n",
        "                pcontent = str(p.get('content', '') or '').strip()\n",
        "                if not pnum:\n",
        "                    continue\n",
        "                nid = f\"{article_key}:p{pnum}\"\n",
        "                para_nodes.append((pnum, nid))\n",
        "                para_content_by_num[pnum] = pcontent\n",
        "                preview = pcontent.replace('\\n', ' ')[:220]\n",
        "                nodes[nid] = {\n",
        "                    'id': nid,\n",
        "                    'label': f\"{law_name}\\n제{article_num}조{sub_txt} 제{pnum}항\",\n",
        "                    'title': f\"[{nid}] {law_name} 제{article_num}조{sub_txt} 제{pnum}항 {title}\\n{preview}\",\n",
        "                    'group': f\"{law_name}-paragraph\",\n",
        "                }\n",
        "\n",
        "            preview = content[:220].replace('\\n', ' ')\n",
        "            nodes[article_key] = {\n",
        "                'id': article_key,\n",
        "                'label': f\"{law_name}\\n제{article_num}조{sub_txt}\",\n",
        "                'title': f\"[{article_key}] {law_name} 제{article_num}조{sub_txt} {title}\\n{preview}\",\n",
        "                'group': law_name,\n",
        "            }\n",
        "            for _, nid in para_nodes:\n",
        "                edges.append((article_key, nid, 'contains-paragraph'))\n",
        "        else:\n",
        "            preview = content[:220].replace('\\n', ' ')\n",
        "            nodes[article_key] = {\n",
        "                'id': article_key,\n",
        "                'label': f\"{law_name}\\n제{article_num}조{sub_txt}\" if article_num else article_key,\n",
        "                'title': f\"[{article_key}] {law_name} 제{article_num}조{sub_txt} {title}\\n{preview}\",\n",
        "                'group': law_name,\n",
        "            }\n",
        "\n",
        "        def source_node_for_ref(r: dict) -> str:\n",
        "            rp = norm_para(r.get('paragraph', ''))\n",
        "            if rp and f\"{article_key}:p{rp}\" in nodes:\n",
        "                return f\"{article_key}:p{rp}\"\n",
        "\n",
        "            raw = str(r.get('raw', '') or '').strip()\n",
        "            if raw and para_nodes:\n",
        "                for pnum, nid in para_nodes:\n",
        "                    ptxt = para_content_by_num.get(pnum, '')\n",
        "                    if ptxt and raw in ptxt:\n",
        "                        return nid\n",
        "\n",
        "            if para_nodes:\n",
        "                return para_nodes[0][1]\n",
        "            return article_key\n",
        "\n",
        "        def target_node_key_for_ref_law_article(ref_law_id: str, ref_article: str, ref_paragraph: str = '') -> str:\n",
        "            t_main, t_sub = parse_article_token(ref_article)\n",
        "            base = make_chunk_key(ref_law_id, t_main or ref_article, t_sub)\n",
        "            rp = norm_para(ref_paragraph)\n",
        "            return f\"{base}:p{rp}\" if rp else base\n",
        "\n",
        "        for r in meta.get('internal_refs', []) or []:\n",
        "            if not isinstance(r, dict):\n",
        "                continue\n",
        "            tgt_article = str(r.get('article', '') or '')\n",
        "            if not (tgt_article and law_id):\n",
        "                continue\n",
        "            src = source_node_for_ref(r)\n",
        "            tgt = target_node_key_for_ref_law_article(law_id, tgt_article, str(r.get('paragraph', '') or ''))\n",
        "            edges.append((src, tgt, 'internal'))\n",
        "\n",
        "        for r in meta.get('external_refs', []) or []:\n",
        "            if not isinstance(r, dict):\n",
        "                continue\n",
        "            lname = str(r.get('law_name', '') or '')\n",
        "            art = str(r.get('article', '') or '')\n",
        "            mapped = in_scope.get(lname)\n",
        "            src = source_node_for_ref(r)\n",
        "            if mapped and art:\n",
        "                tgt = target_node_key_for_ref_law_article(mapped, art, str(r.get('paragraph', '') or ''))\n",
        "                edges.append((src, tgt, 'external-in-scope'))\n",
        "            else:\n",
        "                tgt = f\"external:{lname}:{art}\"\n",
        "                if tgt not in nodes:\n",
        "                    nodes[tgt] = {\n",
        "                        'id': tgt,\n",
        "                        'label': f\"외부\\n{lname} 제{art}조\",\n",
        "                        'title': f\"external ref: {lname} 제{art}\",\n",
        "                        'group': 'external',\n",
        "                    }\n",
        "                edges.append((src, tgt, 'external-out-of-scope'))\n",
        "\n",
        "    for src, tgt, etype in list(edges):\n",
        "        if tgt in nodes or tgt.startswith('external:'):\n",
        "            continue\n",
        "        if ':p' in tgt:\n",
        "            base, ptag = tgt.rsplit(':p', 1)\n",
        "            add_article_placeholder_if_missing(base)\n",
        "            parts = base.split(':')\n",
        "            lid = parts[0] if len(parts) > 0 else ''\n",
        "            anum = parts[1] if len(parts) > 1 else '?'\n",
        "            asub = parts[2] if len(parts) > 2 else '0'\n",
        "            lname = law_id_to_name.get(lid, lid)\n",
        "            sub_txt = f\"의{asub}\" if asub not in ['', '0'] else ''\n",
        "            nodes[tgt] = {\n",
        "                'id': tgt,\n",
        "                'label': f\"{lname}\\n제{anum}조{sub_txt} 제{ptag}항\\n(미조회)\",\n",
        "                'title': f\"[{tgt}] {lname} 제{anum}조{sub_txt} 제{ptag}항 - 아직 contexts에 없음\",\n",
        "                'group': 'unfetched',\n",
        "            }\n",
        "            edges.append((base, tgt, 'contains-paragraph'))\n",
        "        elif ':' in tgt:\n",
        "            add_article_placeholder_if_missing(tgt)\n",
        "\n",
        "    edge_seen = set()\n",
        "    uniq_edges = []\n",
        "    for e in edges:\n",
        "        if e in edge_seen:\n",
        "            continue\n",
        "        edge_seen.add(e)\n",
        "        uniq_edges.append(e)\n",
        "\n",
        "    return nodes, uniq_edges\n",
        "\n",
        "\n",
        "def render_reference_graph(contexts):\n",
        "    try:\n",
        "        from pyvis.network import Network\n",
        "        from IPython.display import HTML, display\n",
        "    except Exception:\n",
        "        raise ImportError('pyvis가 필요합니다. conda run -n natna pip install pyvis')\n",
        "\n",
        "    nodes, edges = build_reference_graph_data(contexts)\n",
        "    print(f'graph data -> nodes: {len(nodes)}, edges: {len(edges)}')\n",
        "\n",
        "    if not nodes:\n",
        "        print('[warn] contexts가 비어 있어 그래프를 그릴 노드가 없습니다.')\n",
        "        return None\n",
        "\n",
        "    net = Network(height='760px', width='100%', directed=True, notebook=True, cdn_resources='in_line')\n",
        "    net.force_atlas_2based(gravity=-50, central_gravity=0.01, spring_length=120, spring_strength=0.08)\n",
        "\n",
        "    for n in nodes.values():\n",
        "        net.add_node(n['id'], label=n['label'], title=n['title'], group=n['group'])\n",
        "\n",
        "    color_by_type = {\n",
        "        'internal': '#2E86DE',\n",
        "        'external-in-scope': '#16A085',\n",
        "        'external-out-of-scope': '#7F8C8D',\n",
        "        'contains-paragraph': '#BDC3C7',\n",
        "    }\n",
        "    for src, tgt, etype in edges:\n",
        "        if tgt not in nodes:\n",
        "            net.add_node(tgt, label=tgt, title=tgt, group='unknown')\n",
        "        net.add_edge(\n",
        "            src,\n",
        "            tgt,\n",
        "            title=etype,\n",
        "            color=color_by_type.get(etype, '#999999'),\n",
        "            dashes=(etype == 'external-out-of-scope'),\n",
        "        )\n",
        "\n",
        "    html_doc = net.generate_html(notebook=True)\n",
        "    iframe = f'<iframe style=\"width:100%;height:800px;border:1px solid #ddd;\" srcdoc=\"{_html.escape(html_doc)}\"></iframe>'\n",
        "    display(HTML(iframe))\n",
        "    return html_doc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "360a8d5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 시각화 헬퍼: 단계별 스냅샷 (그래프 + 내용 + ref 로그)\n",
        "def show_state_snapshot(state: dict, stage: str, context_limit: int = 12, show_graph: bool = True):\n",
        "    from IPython.display import display\n",
        "    try:\n",
        "        import pandas as pd\n",
        "    except Exception:\n",
        "        pd = None\n",
        "\n",
        "    contexts = list(state.get('contexts', []) or [])\n",
        "    pending = list(state.get('pending_refs', []) or [])\n",
        "    print(f'[{stage}] contexts={len(contexts)}, pending_refs={len(pending)}, hop_count={state.get(\"hop_count\", 0)}')\n",
        "\n",
        "    trd = state.get('target_retrieve_debug', {}) or {}\n",
        "    if trd:\n",
        "        print('target_retrieve_debug:')\n",
        "        for k, v in trd.items():\n",
        "            print(f\"  - {k}: requested={v.get('requested_k')}, returned={v.get('returned_k')}\")\n",
        "\n",
        "    if show_graph and contexts:\n",
        "        render_reference_graph(contexts[:context_limit])\n",
        "\n",
        "    rows = []\n",
        "    for c in contexts[:context_limit]:\n",
        "        m = c.get('metadata', {}) or {}\n",
        "        article_sub = str(m.get('article_sub', '0'))\n",
        "        article_suffix = ('의' + article_sub) if article_sub not in ['', '0'] else ''\n",
        "        _internal_keys = [\n",
        "            f\"{m.get('law_id')}:{str(r.get('article','')).strip()}\"\n",
        "            for r in (m.get('internal_refs', []) or []) if isinstance(r, dict) and str(r.get('article','')).strip()\n",
        "        ]\n",
        "        _external_keys = []\n",
        "        for r in (m.get('external_refs', []) or []):\n",
        "            if not isinstance(r, dict):\n",
        "                continue\n",
        "            _lname = str(r.get('law_name', '')).strip()\n",
        "            _article = str(r.get('article', '')).strip()\n",
        "            _mapped = IN_SCOPE_LAW_NAME_TO_ID.get(_lname, '')\n",
        "            if _mapped and _article:\n",
        "                _external_keys.append(f\"{_mapped}:{_article}\")\n",
        "            elif _lname or _article:\n",
        "                _external_keys.append(f\"external:{_lname}:{_article}\")\n",
        "\n",
        "        rows.append({\n",
        "            'chunk_key': f\"{m.get('law_id')}:{m.get('article_num')}:{m.get('article_sub', '0')}\",\n",
        "            'law_name': m.get('law_name'),\n",
        "            'article': f\"제{m.get('article_num')}조{article_suffix}\",\n",
        "            'title': m.get('article_title', ''),\n",
        "            'content_preview': (c.get('content') or '').replace('\\n', ' ')[:220],\n",
        "            'internal_ref_cnt': len(_internal_keys),\n",
        "            'internal_ref_keys': ', '.join(_internal_keys[:8]),\n",
        "            'external_ref_cnt': len(_external_keys),\n",
        "            'external_ref_keys': ', '.join(_external_keys[:8]),\n",
        "        })\n",
        "\n",
        "    if pd is not None and rows:\n",
        "        display(pd.DataFrame(rows))\n",
        "    else:\n",
        "        print('context rows:', rows[:3])\n",
        "\n",
        "    pref = []\n",
        "    for r in pending[:10]:\n",
        "        pref.append({\n",
        "            'law_id': r.get('law_id'),\n",
        "            'article': r.get('article'),\n",
        "            'priority': r.get('priority'),\n",
        "            'source': r.get('source'),\n",
        "            'reason_preview': str(r.get('decision_reason', ''))[:120],\n",
        "        })\n",
        "\n",
        "    if pd is not None and pref:\n",
        "        display(pd.DataFrame(pref))\n",
        "    else:\n",
        "        print('pending refs head:', pref)\n",
        "\n",
        "    batch = list(state.get('ref_batch_decisions', []) or [])\n",
        "    if batch:\n",
        "        last = batch[-1]\n",
        "        fetched = last.get('fetched_chunks', []) or []\n",
        "        checked = last.get('checked_refs', []) or []\n",
        "        if pd is not None:\n",
        "            display(pd.DataFrame(batch))\n",
        "\n",
        "            if checked:\n",
        "                checked_rows = []\n",
        "                for x in checked:\n",
        "                    checked_rows.append({\n",
        "                        'ref_key': x.get('ref_key'),\n",
        "                        'follow': x.get('follow', False),\n",
        "                        'priority': x.get('priority', 0),\n",
        "                        'source': x.get('source', ''),\n",
        "                        'check_reason': str(x.get('check_reason', ''))[:220],\n",
        "                        'ref_preview': ' | '.join([(p.get('content_preview') or '')[:90] for p in (x.get('ref_docs_preview') or [])[:2]]),\n",
        "                    })\n",
        "                display(pd.DataFrame(checked_rows))\n",
        "\n",
        "            if fetched:\n",
        "                fetched_rows = []\n",
        "                for x in fetched:\n",
        "                    fetched_rows.append({\n",
        "                        'ref_key': x.get('ref_key'),\n",
        "                        'fetched_count': x.get('fetched_count', 0),\n",
        "                        'fetched_keys': ', '.join(x.get('fetched_keys', [])),\n",
        "                        'fetched_preview': ' | '.join(x.get('fetched_previews', [])[:2]),\n",
        "                        'source': x.get('source', ''),\n",
        "                        'priority': x.get('priority', 0),\n",
        "                    })\n",
        "                display(pd.DataFrame(fetched_rows))\n",
        "        else:\n",
        "            print('batch decisions:', batch)\n",
        "            print('checked refs:', checked)\n",
        "            print('latest fetched:', fetched)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_ref_audit_rows(runtime, state: dict, target: str | None = None, hit_limit: int = 12) -> list[dict]:\n",
        "    tgt = target or ((state.get('targets') or ['일반'])[0])\n",
        "    docs = (state.get('target_hits', {}) or {}).get(tgt, [])\n",
        "    rows = inspect_expand_decisions(\n",
        "        runtime,\n",
        "        user_query=state.get('user_query', ''),\n",
        "        target=tgt,\n",
        "        docs=docs,\n",
        "        limit=hit_limit,\n",
        "    )\n",
        "\n",
        "    flat = []\n",
        "    for r in rows:\n",
        "        proposed = {f\"{x.get('law_id')}:{x.get('article')}\": x for x in (r.get('proposed_refs') or [])}\n",
        "        for c in (r.get('ref_candidates_preview') or []):\n",
        "            key = f\"{c.get('law_id')}:{c.get('article')}\"\n",
        "            p = proposed.get(key, {})\n",
        "            selected = key in proposed\n",
        "\n",
        "            previews = c.get('ref_docs_preview') or p.get('ref_docs_preview') or []\n",
        "            preview_text = ' | '.join([(x.get('content_preview') or '')[:140] for x in previews[:2]])\n",
        "            preview_keys = ', '.join([x.get('chunk_key', '') for x in previews[:2]])\n",
        "            paragraph_match = any(bool(x.get('paragraph_match', False)) for x in previews) if previews else False\n",
        "\n",
        "\n",
        "            # fallback: preview가 비면 DB 직접 재조회로 보강\n",
        "            if not preview_text.strip() and ':' in key:\n",
        "                try:\n",
        "                    _lid, _art = key.split(':', 1)\n",
        "                    _docs = get_article(runtime['client'], 'building_law', _lid, _art)\n",
        "                    _pv = build_ref_docs_preview(\n",
        "                        _docs,\n",
        "                        paragraph=str(c.get('paragraph', '') or ''),\n",
        "                        item=str(c.get('item', '') or ''),\n",
        "                        ref_key=key,\n",
        "                        max_docs=2,\n",
        "                        max_chars=260,\n",
        "                    )\n",
        "                    if _pv:\n",
        "                        preview_text = ' | '.join([(x.get('content_preview') or '')[:140] for x in _pv[:2]])\n",
        "                        preview_keys = ', '.join([x.get('chunk_key', '') for x in _pv[:2]])\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            flat.append({\n",
        "                'target': tgt,\n",
        "                'from_chunk': r.get('chunk_key', ''),\n",
        "                'ref_key': key,\n",
        "                'paragraph': c.get('paragraph', ''),\n",
        "                'item': c.get('item', ''),\n",
        "                'source': c.get('source', ''),\n",
        "                'raw_ref': c.get('raw', ''),\n",
        "                'selected': selected,\n",
        "                'priority': p.get('priority', 0) if selected else 0,\n",
        "                'llm_expand': r.get('llm_expand', None),\n",
        "                'llm_priority': r.get('llm_priority', None),\n",
        "                'reason': (p.get('decision_reason') or c.get('reason') or r.get('llm_reason') or '')[:220],\n",
        "                'ref_preview_keys': preview_keys,\n",
        "                'ref_preview': preview_text,\n",
        "                'ref_preview_len': len(preview_text.strip()),\n",
        "                'paragraph_match': paragraph_match,\n",
        "            })\n",
        "\n",
        "    seen=set()\n",
        "    out=[]\n",
        "    for x in flat:\n",
        "        k=(x['from_chunk'], x['ref_key'])\n",
        "        if k in seen:\n",
        "            continue\n",
        "        seen.add(k)\n",
        "        out.append(x)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def display_ref_audit(runtime, state: dict, target: str | None = None, hit_limit: int = 12):\n",
        "    from IPython.display import display\n",
        "    try:\n",
        "        import pandas as pd\n",
        "    except Exception:\n",
        "        rows = build_ref_audit_rows(runtime, state, target=target, hit_limit=hit_limit)\n",
        "        print('rows:', len(rows))\n",
        "        return rows\n",
        "\n",
        "    rows = build_ref_audit_rows(runtime, state, target=target, hit_limit=hit_limit)\n",
        "    df = pd.DataFrame(rows)\n",
        "    if df.empty:\n",
        "        print('ref audit: empty')\n",
        "        return df\n",
        "\n",
        "    cols = [\n",
        "        'target', 'from_chunk', 'ref_key', 'paragraph', 'item', 'source', 'raw_ref',\n",
        "        'selected', 'priority', 'reason',\n",
        "        'ref_preview_keys', 'ref_preview_len', 'paragraph_match', 'ref_preview',\n",
        "    ]\n",
        "    df = df[cols]\n",
        "\n",
        "    def row_style(r):\n",
        "        if bool(r.get('selected')):\n",
        "            return ['background-color: #e9f7ef'] * len(r)\n",
        "        return ['background-color: #f0f0f0; color: #666'] * len(r)\n",
        "\n",
        "    try:\n",
        "        display(df.style.apply(row_style, axis=1))\n",
        "    except Exception:\n",
        "        display(df)\n",
        "\n",
        "    print('selected refs:', int(df['selected'].sum()), '/', len(df))\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def show_ref_fulltext(state: dict, ref_key: str, max_chars: int = 6000):\n",
        "    # ref_key: '001823:58' 또는 '001823:58:0'\n",
        "    parts = str(ref_key).split(':')\n",
        "    if len(parts) < 2:\n",
        "        print('invalid ref_key, expected law_id:article or law_id:article:sub')\n",
        "        return\n",
        "\n",
        "    law_id = parts[0]\n",
        "    article = parts[1]\n",
        "    sub = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "    hits = []\n",
        "    for c in (state.get('contexts') or []):\n",
        "        m = c.get('metadata', {}) or {}\n",
        "        if str(m.get('law_id', '')) != law_id:\n",
        "            continue\n",
        "        if str(m.get('article_num', '')) != article:\n",
        "            continue\n",
        "        if sub is not None and str(m.get('article_sub', '0')) != str(sub):\n",
        "            continue\n",
        "        hits.append(c)\n",
        "\n",
        "    if not hits:\n",
        "        # sub 없는 키로 재시도\n",
        "        if sub is not None:\n",
        "            for c in (state.get('contexts') or []):\n",
        "                m = c.get('metadata', {}) or {}\n",
        "                if str(m.get('law_id', '')) == law_id and str(m.get('article_num', '')) == article:\n",
        "                    hits.append(c)\n",
        "\n",
        "    if not hits:\n",
        "        print(f'not found in contexts: {ref_key}')\n",
        "        return\n",
        "\n",
        "    for i, c in enumerate(hits, 1):\n",
        "        m = c.get('metadata', {}) or {}\n",
        "        ckey = f\"{m.get('law_id')}:{m.get('article_num')}:{m.get('article_sub', '0')}\"\n",
        "        print(f\"\\n[{i}] {ckey} | {m.get('law_name')} 제{m.get('article_num')}조 | {m.get('article_title','')}\")\n",
        "        txt = (c.get('content') or '')[:max_chars]\n",
        "        print(txt)\n",
        "\n",
        "\n",
        "\n",
        "def debug_db_fetch_for_refs(runtime, ref_keys: list[str], collection: str = 'building_law', max_preview: int = 160):\n",
        "    rows = []\n",
        "    client = runtime['client']\n",
        "    for rk in ref_keys:\n",
        "        parts = str(rk).split(':')\n",
        "        if len(parts) < 2:\n",
        "            rows.append({'ref_key': rk, 'ok': False, 'count': 0, 'note': 'invalid ref_key'})\n",
        "            continue\n",
        "        law_id = parts[0]\n",
        "        article = parts[1]\n",
        "        docs = get_article(client, collection, law_id, article)\n",
        "\n",
        "        # 본문 추출 우선순위: content -> page_content -> metadata.content_original -> metadata.page_content\n",
        "        previews = []\n",
        "        meta_keys = []\n",
        "        for d in (docs or [])[:2]:\n",
        "            m = (d.get('metadata', {}) or {}) if isinstance(d, dict) else {}\n",
        "            txt = (\n",
        "                (d.get('content') if isinstance(d, dict) else '')\n",
        "                or (d.get('page_content') if isinstance(d, dict) else '')\n",
        "                or m.get('content_original', '')\n",
        "                or m.get('page_content', '')\n",
        "                or ''\n",
        "            )\n",
        "            previews.append(str(txt).replace('\\n', ' ')[:max_preview])\n",
        "            meta_keys.append(sorted(list(m.keys()))[:8])\n",
        "\n",
        "        rows.append({\n",
        "            'ref_key': rk,\n",
        "            'ok': bool(docs),\n",
        "            'count': len(docs or []),\n",
        "            'preview': ' | '.join(previews),\n",
        "            'meta_keys_head': meta_keys,\n",
        "        })\n",
        "\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        from IPython.display import display\n",
        "        display(pd.DataFrame(rows))\n",
        "    except Exception:\n",
        "        print(rows)\n",
        "    return rows\n",
        "\n",
        "\n",
        "\n",
        "def precheck_refs_between_step1_step2(state: dict, runtime, k: int = 6, limit_per_target: int = 10):\n",
        "    \"\"\"STEP1~2 사이 디버깅용: 초기 retrieve 후 ref를 DB조회+LLM판단까지 미리 수행\"\"\"\n",
        "    user_query = state.get('user_query', '')\n",
        "    targets = state.get('targets', []) or ['일반']\n",
        "\n",
        "    base = run_multi_target_agents(runtime, user_query=user_query, targets=targets, k=k)\n",
        "\n",
        "    ref_reviews = {}\n",
        "    pending = []\n",
        "    seen = set()\n",
        "\n",
        "    for t in targets:\n",
        "        docs = (base.get('target_hits', {}) or {}).get(t, [])\n",
        "        rows = inspect_expand_decisions(\n",
        "            runtime,\n",
        "            user_query=user_query,\n",
        "            target=t,\n",
        "            docs=docs,\n",
        "            limit=limit_per_target,\n",
        "        )\n",
        "        ref_reviews[t] = rows\n",
        "\n",
        "        for r in rows:\n",
        "            for p in (r.get('proposed_refs') or []):\n",
        "                key = (str(p.get('law_id','')), str(p.get('article','')))\n",
        "                if key in seen:\n",
        "                    continue\n",
        "                seen.add(key)\n",
        "                pending.append({\n",
        "                    'law_id': p.get('law_id'),\n",
        "                    'article': p.get('article'),\n",
        "                    'paragraph': p.get('paragraph', ''),\n",
        "                    'item': p.get('item', ''),\n",
        "                    'source': p.get('source', ''),\n",
        "                    'source_chunk_key': r.get('chunk_key', ''),\n",
        "                    'priority': int(p.get('priority', 0) or 0),\n",
        "                    'decision_reason': p.get('decision_reason', ''),\n",
        "                    'llm_reason': p.get('decision_reason', ''),\n",
        "                    'ref_docs_preview': p.get('ref_docs_preview', []),\n",
        "                    'raw_ref': p.get('raw_ref', ''),\n",
        "                })\n",
        "\n",
        "    pending.sort(key=lambda x: int(x.get('priority', 0)), reverse=True)\n",
        "\n",
        "    return {\n",
        "        'target_hits': base.get('target_hits', {}),\n",
        "        'target_decisions': base.get('target_decisions', {}),\n",
        "        'contexts': base.get('contexts', []),\n",
        "        'pending_refs': pending,\n",
        "        'ref_reviews': ref_reviews,\n",
        "        'targets': targets,\n",
        "    }\n",
        "\n",
        "\n",
        "def apply_precheck_to_state(state: dict, precheck: dict):\n",
        "    state['targets'] = precheck.get('targets', state.get('targets', []))\n",
        "    state['target_hits'] = precheck.get('target_hits', {})\n",
        "    state['target_decisions'] = precheck.get('target_decisions', {})\n",
        "    state['hits'] = list(precheck.get('contexts', []))\n",
        "    state['contexts'] = list(precheck.get('contexts', []))\n",
        "    state['pending_refs'] = list(precheck.get('pending_refs', []))\n",
        "    state['seen_ref_keys'] = []\n",
        "    state['precheck_ref_reviews'] = precheck.get('ref_reviews', {})\n",
        "    return state\n",
        "\n",
        "\n",
        "\n",
        "def prefetch_refs_between_step1_step2(state: dict, runtime, k: int = 6):\n",
        "    user_query = state.get('user_query', '')\n",
        "    targets = state.get('targets', []) or ['일반']\n",
        "    base = run_multi_target_agents(runtime, user_query=user_query, targets=targets, k=k)\n",
        "\n",
        "    prefetched = []\n",
        "    seen = set()\n",
        "\n",
        "    def parse_para_item_from_raw(raw: str) -> tuple[str, str]:\n",
        "        txt = str(raw or '')\n",
        "        m_para = re.search(r'제\\s*(\\d+)\\s*항', txt)\n",
        "        m_item = re.search(r'제\\s*(\\d+)\\s*호', txt)\n",
        "        return (m_para.group(1) if m_para else ''), (m_item.group(1) if m_item else '')\n",
        "\n",
        "    for t in targets:\n",
        "        docs = (base.get('target_hits', {}) or {}).get(t, [])\n",
        "        for d in docs:\n",
        "            m = d.get('metadata', {}) or {}\n",
        "            src_key = f\"{m.get('law_id')}:{m.get('article_num')}:{m.get('article_sub','0')}\"\n",
        "            for r in extract_ref_candidates(m)[:12]:\n",
        "                law_id = str(r.get('law_id','')).strip()\n",
        "                article = str(r.get('article','')).strip()\n",
        "                paragraph = str(r.get('paragraph','') or '').strip()\n",
        "                item = str(r.get('item','') or '').strip()\n",
        "                raw_ref = str((r.get('source_ref') or {}).get('raw','') or '')\n",
        "\n",
        "                if not paragraph and not item and raw_ref:\n",
        "                    p2, i2 = parse_para_item_from_raw(raw_ref)\n",
        "                    paragraph = paragraph or p2\n",
        "                    item = item or i2\n",
        "\n",
        "                k2 = (law_id, article, paragraph, item, src_key)\n",
        "                if k2 in seen:\n",
        "                    continue\n",
        "                seen.add(k2)\n",
        "\n",
        "                ref_key = f\"{law_id}:{article or '__law__'}\"\n",
        "                ref_docs = get_ref_docs_for_candidate(runtime, law_id, article, user_query=user_query, target=t) if law_id else []\n",
        "                ref_preview = build_ref_docs_preview(ref_docs, paragraph=paragraph, item=item, ref_key=ref_key, max_docs=2, max_chars=260)\n",
        "\n",
        "                prefetched.append({\n",
        "                    'target': t,\n",
        "                    'source_chunk_key': src_key,\n",
        "                    'law_id': law_id,\n",
        "                    'article': article,\n",
        "                    'paragraph': paragraph,\n",
        "                    'item': item,\n",
        "                    'source': r.get('source',''),\n",
        "                    'raw_ref': raw_ref,\n",
        "                    'ref_docs_count': len(ref_docs or []),\n",
        "                    'ref_docs_preview': ref_preview,\n",
        "                })\n",
        "\n",
        "    state['targets'] = targets\n",
        "    state['target_hits'] = base.get('target_hits', {})\n",
        "    state['target_decisions'] = base.get('target_decisions', {})\n",
        "    state['hits'] = list(base.get('contexts', []))\n",
        "    state['contexts'] = list(base.get('contexts', []))\n",
        "    state['prefetched_refs'] = prefetched\n",
        "    return state\n",
        "\n",
        "\n",
        "\n",
        "def llm_judge_prefetched_refs(state: dict, runtime):\n",
        "    user_query = state.get('user_query', '')\n",
        "    judged = []\n",
        "    pending = []\n",
        "    seen = set()\n",
        "\n",
        "    try:\n",
        "        need_ref, need_reason = llm_need_refs_for_state_context(runtime, state)\n",
        "    except Exception as e:\n",
        "        need_ref, need_reason = True, f'llm_error:{e}'\n",
        "\n",
        "    if not need_ref:\n",
        "        for r in (state.get('prefetched_refs') or []):\n",
        "            judged.append({**r, 'follow': False, 'priority': 0, 'decision_reason': f'chunk_context_sufficient: {need_reason}'})\n",
        "        state['judged_refs'] = judged\n",
        "        state['pending_refs'] = []\n",
        "        state['seen_ref_keys'] = []\n",
        "        return state\n",
        "\n",
        "    for r in (state.get('prefetched_refs') or []):\n",
        "        ref_key = f\"{r.get('law_id')}:{r.get('article') or '__law__'}\"\n",
        "        preview = r.get('ref_docs_preview', []) or []\n",
        "        target = str(r.get('target') or ((state.get('targets') or ['일반'])[0]))\n",
        "\n",
        "        if not preview:\n",
        "            follow, pri, reason = False, 0, 'ref_docs_not_found'\n",
        "        else:\n",
        "            try:\n",
        "                follow, pri, reason = llm_should_follow_ref_by_content(\n",
        "                    get_runtime_llm(runtime, 'ref_expander'),\n",
        "                    user_query=user_query,\n",
        "                    target=target,\n",
        "                    parent_chunk_preview='',\n",
        "                    ref_key=ref_key,\n",
        "                    ref_docs_preview=preview,\n",
        "                )\n",
        "            except Exception as e:\n",
        "                follow, pri, reason = False, 0, f'llm_error:{e}'\n",
        "\n",
        "        row = {**r, 'follow': bool(follow), 'priority': int(pri or 0), 'decision_reason': str(reason or '')}\n",
        "        judged.append(row)\n",
        "\n",
        "        dedup_key = (str(r.get('law_id','')), str(r.get('article','')))\n",
        "        if follow and dedup_key not in seen:\n",
        "            seen.add(dedup_key)\n",
        "            pending.append({\n",
        "                'law_id': r.get('law_id'),\n",
        "                'article': r.get('article'),\n",
        "                'paragraph': r.get('paragraph',''),\n",
        "                'item': r.get('item',''),\n",
        "                'source': r.get('source',''),\n",
        "                'source_chunk_key': r.get('source_chunk_key',''),\n",
        "                'priority': int(pri or 0),\n",
        "                'decision_reason': str(reason or ''),\n",
        "                'llm_reason': str(reason or ''),\n",
        "                'ref_docs_preview': preview,\n",
        "                'raw_ref': r.get('raw_ref',''),\n",
        "            })\n",
        "\n",
        "    pending.sort(key=lambda x: int(x.get('priority', 0)), reverse=True)\n",
        "    state['judged_refs'] = judged\n",
        "    state['pending_refs'] = pending\n",
        "    state['seen_ref_keys'] = []\n",
        "    return state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34eeeaa3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 0) 단일 예시 쿼리/상태 초기화\n",
        "example_query = '서울특별시 종로구 송현동 48-24번지, 49-4번지, 대지면적 9787m2, 연면적 25676m2, 층수 지하 2층 지상3층, 건폐율 60%이하, 용적률 150%이하, 면적표 25696m2, 높이 16m 이하, 도시지역, 제1종일반주고, 고도지구(16m) 건축선을 알려줘'\n",
        "manual_state = {\n",
        "    'user_query': example_query,\n",
        "    'max_hops': 3,\n",
        "}\n",
        "manual_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21c8c152",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 1) target 추출\n",
        "manual_state = node_extract_targets(manual_state, runtime)\n",
        "{\n",
        "    'query': manual_state.get('user_query'),\n",
        "    'targets': manual_state.get('targets', []),\n",
        "    'hop_count': manual_state.get('hop_count'),\n",
        "    'max_hops': manual_state.get('max_hops'),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5d92d2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 1.5-A) ref 사전조회 (DB 조회만)\n",
        "manual_state = prefetch_refs_between_step1_step2(manual_state, runtime, k=5)\n",
        "{\n",
        "    'targets': manual_state.get('targets', []),\n",
        "    'contexts_count': len(manual_state.get('contexts', [])),\n",
        "    'prefetched_refs_count': len(manual_state.get('prefetched_refs', [])),\n",
        "}\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "    from IPython.display import display\n",
        "    _pre = pd.DataFrame(manual_state.get('prefetched_refs', []))\n",
        "    if not _pre.empty:\n",
        "        cols = ['target','source_chunk_key','law_id','article','paragraph','item','source','raw_ref','ref_docs_count']\n",
        "        display(_pre[cols])\n",
        "        _pre['_pv_cnt'] = _pre['ref_docs_preview'].apply(lambda x: len(x or []))\n",
        "        print('prefetch preview missing:', int((_pre['_pv_cnt']==0).sum()), '/', len(_pre))\n",
        "except Exception as _e:\n",
        "    print('prefetch render failed:', _e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db9f155f",
      "metadata": {},
      "outputs": [],
      "source": [
        "manual_state[\"target_hits\"][\"건축선\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0250e29c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 1.5-B) 사전조회 ref에 대한 LLM 판단\n",
        "manual_state = llm_judge_prefetched_refs(manual_state, runtime)\n",
        "{\n",
        "    'judged_refs_count': len(manual_state.get('judged_refs', [])),\n",
        "    'pending_refs_count': len(manual_state.get('pending_refs', [])),\n",
        "    'pending_refs_head': manual_state.get('pending_refs', [])[:5],\n",
        "}\n",
        "\n",
        "show_state_snapshot(manual_state, stage='STEP1.5B-after-judge', context_limit=10, show_graph=True)\n",
        "\n",
        "inspect_target = (manual_state.get('targets') or ['일반'])[0]\n",
        "ref_audit_df = display_ref_audit(runtime, manual_state, target=inspect_target, hit_limit=10)\n",
        "ref_audit_df\n",
        "\n",
        "try:\n",
        "    _empty = int((ref_audit_df['ref_preview_len'] == 0).sum())\n",
        "    _all = len(ref_audit_df)\n",
        "    print('ref_preview_empty_ratio:', f'{_empty}/{_all}')\n",
        "except Exception as _e:\n",
        "    print('ref preview diag failed:', _e)\n",
        "\n",
        "try:\n",
        "    _keys = list(ref_audit_df['ref_key'].dropna().astype(str).unique())[:8]\n",
        "except Exception:\n",
        "    _keys = []\n",
        "print('db_fetch_check_keys:', _keys)\n",
        "_ = debug_db_fetch_for_refs(runtime, _keys)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b739e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "_keys = list(ref_audit_df['ref_key'].dropna().astype(str).unique())[:8]\n",
        "_keys, ref_audit_df['ref_key']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4514ad1",
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = debug_db_fetch_for_refs(runtime, _keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ea58e69",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3) 특정 target의 초기 hits 확인 + ref 본문 열람\n",
        "inspect_target = (manual_state.get('targets') or ['일반'])[0]\n",
        "inspect_target_hits(manual_state.get('target_hits', {}), inspect_target, limit=8)\n",
        "\n",
        "# 예시: 아래 키를 ref_audit_df의 ref_key 값으로 바꿔서 본문 확인\n",
        "# show_ref_fulltext(manual_state, '001823:58')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3768575",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 4) 확장 판단 상세 확인 (LLM reason 포함)\n",
        "rows = inspect_expand_decisions(\n",
        "    runtime,\n",
        "    user_query=manual_state.get('user_query', ''),\n",
        "    target=inspect_target,\n",
        "    docs=(manual_state.get('target_hits', {}) or {}).get(inspect_target, []),\n",
        "    limit=5,\n",
        ")\n",
        "show_expand_detail(rows, idx=0) if rows else {'msg': 'rows 없음'}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "594f46bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9217dec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 5) hop 1회 실행\n",
        "before_ctx = len(manual_state.get('contexts', []))\n",
        "before_pending = len(manual_state.get('pending_refs', []))\n",
        "manual_state = node_reference_tracker(manual_state, runtime)\n",
        "{\n",
        "    'hop_count': manual_state.get('hop_count', 0),\n",
        "    'contexts_before': before_ctx,\n",
        "    'contexts_after': len(manual_state.get('contexts', [])),\n",
        "    'pending_before': before_pending,\n",
        "    'pending_after': len(manual_state.get('pending_refs', [])),\n",
        "    'ref_batch_decision_last': (manual_state.get('ref_batch_decisions', []) or [None])[-1],\n",
        "}\n",
        "\n",
        "show_state_snapshot(manual_state, stage='STEP5-after-hop1', context_limit=12, show_graph=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ad6d94f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 6) 남은 hop 자동 실행 (원하면 횟수 조절)\n",
        "while manual_state.get('pending_refs') and manual_state.get('hop_count', 0) < manual_state.get('max_hops', 3):\n",
        "    manual_state = node_reference_tracker(manual_state, runtime)\n",
        "{\n",
        "    'hop_count': manual_state.get('hop_count', 0),\n",
        "    'contexts_count': len(manual_state.get('contexts', [])),\n",
        "    'pending_refs_count': len(manual_state.get('pending_refs', [])),\n",
        "    'ref_batch_decisions': manual_state.get('ref_batch_decisions', []),\n",
        "}\n",
        "\n",
        "show_state_snapshot(manual_state, stage='STEP6-after-loop', context_limit=14, show_graph=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac95783d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 7) appendix + 최종 답변 생성\n",
        "manual_state = node_appendix(manual_state, runtime)\n",
        "manual_state = node_generate(manual_state, runtime)\n",
        "{\n",
        "    'targets': manual_state.get('targets', []),\n",
        "    'appendix_count': len(manual_state.get('appendix', [])),\n",
        "    'answer_preview': manual_state.get('answer', '')[:1200],\n",
        "}\n",
        "\n",
        "show_state_snapshot(manual_state, stage='STEP7-final', context_limit=14, show_graph=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a443ca96",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(manual_state[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34c125ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 8) (선택) 최종 컨텍스트 그래프 시각화\n",
        "contexts = manual_state.get('contexts', [])\n",
        "print('contexts:', len(contexts))\n",
        "if contexts:\n",
        "    render_reference_graph(contexts)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "natna",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}