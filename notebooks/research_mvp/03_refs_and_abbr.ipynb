{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f68805d7",
      "metadata": {},
      "source": [
        "# 03. 축약어 추출 (제공 코드 기반 최종본)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "886ae4ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_naver import ChatClovaX\n",
        "from langchain.agents import create_agent\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71faff0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 1) raw JSON -> all_chunks 생성 (02_parse_law.ipynb 의존 제거)\n",
        "\n",
        "@dataclass\n",
        "class ArticleChunk:\n",
        "    law_name: str\n",
        "    law_id: str\n",
        "    article_num: str\n",
        "    article_title: str\n",
        "    content: str\n",
        "\n",
        "\n",
        "def normalize_to_list(v):\n",
        "    if v is None:\n",
        "        return []\n",
        "    if isinstance(v, dict):\n",
        "        return [v]\n",
        "    if isinstance(v, list):\n",
        "        return v\n",
        "    return []\n",
        "\n",
        "\n",
        "def parse_law_data(data: dict) -> list[ArticleChunk]:\n",
        "    law_name = data['법령']['기본정보']['법령명_한글']\n",
        "    law_id = str(data['법령']['기본정보']['법령ID'])\n",
        "    out = []\n",
        "\n",
        "    for article in normalize_to_list(data['법령']['조문'].get('조문단위')):\n",
        "        if article.get('조문여부') != '조문':\n",
        "            continue\n",
        "\n",
        "        parts = []\n",
        "        header = str(article.get('조문내용', '')).strip()\n",
        "        if header:\n",
        "            parts.append(header)\n",
        "\n",
        "        for para in normalize_to_list(article.get('항')):\n",
        "            para_text = str(para.get('항내용', '')).strip()\n",
        "            if para_text:\n",
        "                parts.append(para_text)\n",
        "            for ho in normalize_to_list(para.get('호')):\n",
        "                ho_text = str(ho.get('호내용', '')).strip()\n",
        "                if ho_text:\n",
        "                    parts.append(ho_text)\n",
        "                for mok in normalize_to_list(ho.get('목')):\n",
        "                    mok_text = str(mok.get('목내용', '')).strip()\n",
        "                    if mok_text:\n",
        "                        parts.append(mok_text)\n",
        "\n",
        "        out.append(\n",
        "            ArticleChunk(\n",
        "                law_name=law_name,\n",
        "                law_id=law_id,\n",
        "                article_num=str(article.get('조문번호', '')).strip(),\n",
        "                article_title=str(article.get('조문제목', '')).strip(),\n",
        "                content='\\n'.join(parts),\n",
        "            )\n",
        "        )\n",
        "    return out\n",
        "\n",
        "\n",
        "# raw 경로 자동 탐색: notebooks/research_mvp/data/processed/raw 우선, 없으면 data/processed/raw\n",
        "candidates = [\n",
        "    Path('notebooks/research_mvp/data/processed/raw'),\n",
        "    Path('data/processed/raw'),\n",
        "]\n",
        "raw_dir = next((d for d in candidates if d.exists()), None)\n",
        "if raw_dir is None:\n",
        "    raise FileNotFoundError('raw JSON 경로를 찾지 못했습니다. 01_fetch_law.ipynb를 먼저 실행하세요.')\n",
        "\n",
        "raw_files = sorted(raw_dir.glob('*.json'))\n",
        "if not raw_files:\n",
        "    raise FileNotFoundError(f'raw JSON 파일이 없습니다: {raw_dir}')\n",
        "\n",
        "all_chunks = []\n",
        "for fp in raw_files:\n",
        "    payload = json.loads(fp.read_text(encoding='utf-8'))\n",
        "    all_chunks.extend(parse_law_data(payload))\n",
        "\n",
        "print('raw_dir:', raw_dir)\n",
        "print('raw_files:', len(raw_files))\n",
        "print('all_chunks:', len(all_chunks))\n",
        "print('sample:', all_chunks[0].law_name, all_chunks[0].article_num, all_chunks[0].article_title)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4712732",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 2) 제공해주신 코드 기반 축약어 추출 체인\n",
        "\n",
        "class AbbrOutput(BaseModel):\n",
        "    abbreviations: dict[str, str] = Field(default_factory=dict)\n",
        "\n",
        "llm = ChatClovaX(\n",
        "    model=\"HCX-005\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=2048,\n",
        ")\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"당신은 법률 문서 전문을 분석하여, 그 안에서 약어(또는 축약어)의 **원래 의미**를 추출해야 합니다.\n",
        "\n",
        "#아래의 원칙을 반드시 지키세요:\n",
        "1. 반드시 문서 내용에 **명시적으로 등장한 약어 또는 축약어만** 추출합니다.\n",
        "2. 약어가 정의되지 않은 경우, 절대 추측하지 않습니다.\n",
        "3. 약어가 정의된 문장은 보통 “(이하 ‘~’이라 한다)” 또는 “(이하 ‘~’라 한다)” 형태로 나타납니다.\n",
        "4. 약어의 원래 의미에는 다음 요소들이 포함될 수 있습니다:\n",
        "   - 관련 법령, 조항 번호, 시행령/시행규칙\n",
        "   - “에 따라”, “에 의한”, “이 정하여 고시하는”, “으로 정하는”, “에서 규정한” 등의 조건문\n",
        "   - 문장 속 수식어, 제약 조건 등\n",
        "   이러한 조건은 **절대 생략하지 말고 그대로 포함하세요.**\n",
        "5. 정의 구문에 \"다만\", \"단서\", \"예외\" 등이 이어지는 경우, 해당 조건도 실제 명칭에 반드시 포함합니다.\n",
        "6. 출력은 반드시 **파싱 가능한 JSON 형식**으로 반환해야 합니다.\n",
        "\n",
        "#출력 규칙:\n",
        "- 반드시 유효한 \"JSON\" 형식으로만 출력하세요.\n",
        "- 약어가 전혀 없을 경우에는 정확히 빈 JSON 객체만 출력하세요.\n",
        "- '(이하 \"X\"이라 한다)/(이하 \"X\"라 한다)' 패턴이 없는 일반 정의 항목은 제외하세요.\n",
        "\n",
        "# 문서 내용:\n",
        "{chunk}\n",
        "\"\"\"\n",
        "\n",
        "# agent = create_agent(\n",
        "#     model=llm,\n",
        "#     tools=[],\n",
        "#     system_prompt=SYSTEM_PROMPT,\n",
        "#     # response_format=AbbrOutput,\n",
        "# )\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", SYSTEM_PROMPT),\n",
        "])\n",
        "parser = JsonOutputParser()\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "\n",
        "def extract_abbr_from_chunk(chunk: str) -> dict[str, str]:\n",
        "    # 여기서는 예외를 삼키지 않고 상위 루프로 전달\n",
        "    result = chain.invoke({\"chunk\": chunk})\n",
        "\n",
        "    if not isinstance(result, dict):\n",
        "        return {}\n",
        "\n",
        "    # 1) 표준 키\n",
        "    if isinstance(result.get('abbreviations'), dict):\n",
        "        return {k: v for k, v in result['abbreviations'].items() if isinstance(k, str) and isinstance(v, str)}\n",
        "\n",
        "    # 2) 한글 키 대응\n",
        "    if isinstance(result.get('약어'), dict):\n",
        "        return {k: v for k, v in result['약어'].items() if isinstance(k, str) and isinstance(v, str)}\n",
        "\n",
        "    # 3) 래핑된 dict 대응\n",
        "    for v in result.values():\n",
        "        if isinstance(v, dict):\n",
        "            return {k: vv for k, vv in v.items() if isinstance(k, str) and isinstance(vv, str)}\n",
        "\n",
        "    # 4) 평면 dict 대응\n",
        "    return {k: v for k, v in result.items() if isinstance(k, str) and isinstance(v, str)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12745dbb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3) chunk별 추출 + law별 집계 + JSON 저장 (resume + retry 지원)\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import json\n",
        "import time\n",
        "\n",
        "\n",
        "def chunk_key(c: ArticleChunk) -> str:\n",
        "    return f\"{c.law_id}:{c.article_num}\"\n",
        "\n",
        "\n",
        "def load_existing_chunk_maps(path: Path) -> dict[str, dict[str, str]]:\n",
        "    if not path.exists():\n",
        "        return {}\n",
        "    try:\n",
        "        obj = json.loads(path.read_text(encoding='utf-8'))\n",
        "        return obj if isinstance(obj, dict) else {}\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "\n",
        "def aggregate_by_law(chunks: list[ArticleChunk], chunk_abbr_maps: dict[str, dict[str, str]]):\n",
        "    idx = {chunk_key(c): c for c in chunks}\n",
        "    by_law = defaultdict(dict)\n",
        "\n",
        "    for ckey, amap in chunk_abbr_maps.items():\n",
        "        c = idx.get(ckey)\n",
        "        if c is None:\n",
        "            continue\n",
        "        for k, v in (amap or {}).items():\n",
        "            by_law[c.law_name][k] = v\n",
        "\n",
        "    return dict(by_law)\n",
        "\n",
        "\n",
        "def save_abbr_outputs(chunk_abbr_maps, law_abbr_maps):\n",
        "    out_dir = Path('data/processed')\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    p_chunk = out_dir / 'abbr_maps_by_chunk.json'\n",
        "    p_law = out_dir / 'abbr_maps_by_law.json'\n",
        "\n",
        "    p_chunk.write_text(json.dumps(chunk_abbr_maps, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "    p_law.write_text(json.dumps(law_abbr_maps, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "\n",
        "    return p_chunk, p_law\n",
        "\n",
        "\n",
        "def is_rate_limit_error(e: Exception) -> bool:\n",
        "    msg = str(e)\n",
        "    return ('429' in msg) or ('rate exceeded' in msg.lower()) or ('RateLimitError' in msg)\n",
        "\n",
        "\n",
        "def run_abbr_extraction_for_chunks(\n",
        "    chunks: list[ArticleChunk],\n",
        "    limit: int | None = None,\n",
        "    resume: bool = True,\n",
        "    retry_empty: bool = True,\n",
        "    checkpoint_every: int = 5,\n",
        "    max_retries: int = 3,\n",
        "    base_sleep: float = 1.0,\n",
        "    stop_on_error: bool = False,\n",
        "):\n",
        "    out_dir = Path('data/processed')\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    chunk_path = out_dir / 'abbr_maps_by_chunk.json'\n",
        "    failed_path = out_dir / 'abbr_extract_failed.json'\n",
        "\n",
        "    existing = load_existing_chunk_maps(chunk_path) if resume else {}\n",
        "    failed = []\n",
        "\n",
        "    targets = chunks[:limit] if limit else chunks\n",
        "    total = len(targets)\n",
        "\n",
        "    print('resume mode:', resume)\n",
        "    print('retry_empty:', retry_empty)\n",
        "    print('already done:', len(existing))\n",
        "    print('targets:', total)\n",
        "\n",
        "    processed_new = 0\n",
        "    for i, c in enumerate(targets, 1):\n",
        "        ckey = chunk_key(c)\n",
        "\n",
        "        if resume and ckey in existing:\n",
        "            # 기존 값이 비어있다면 재시도\n",
        "            if retry_empty and (existing.get(ckey) == {} or existing.get(ckey) is None):\n",
        "                pass\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        success = False\n",
        "        last_err = None\n",
        "        for attempt in range(max_retries + 1):\n",
        "            try:\n",
        "                abbr = extract_abbr_from_chunk(c.content)\n",
        "                # 성공 시에만 저장 (빈 dict도 \"정상 결과\"일 수 있으므로 저장)\n",
        "                existing[ckey] = abbr if isinstance(abbr, dict) else {}\n",
        "                success = True\n",
        "                break\n",
        "            except KeyboardInterrupt:\n",
        "                # 중단 시 현재까지 저장하고 종료\n",
        "                law_map = aggregate_by_law(chunks, existing)\n",
        "                save_abbr_outputs(existing, law_map)\n",
        "                failed_path.write_text(json.dumps(failed, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "                print('[interrupt] checkpoint saved. rerun with resume=True to continue.')\n",
        "                raise\n",
        "            except Exception as e:\n",
        "                last_err = e\n",
        "                if is_rate_limit_error(e) and attempt < max_retries:\n",
        "                    wait = min(base_sleep * (2 ** attempt), 30.0)\n",
        "                    print(f'[warn] 429 on {ckey}, retry {attempt+1}/{max_retries}, sleep={wait:.1f}s')\n",
        "                    time.sleep(wait)\n",
        "                    continue\n",
        "                break\n",
        "\n",
        "        if not success:\n",
        "            failed.append({\n",
        "                'chunk_key': ckey,\n",
        "                'law_name': c.law_name,\n",
        "                'article_num': c.article_num,\n",
        "                'error': str(last_err)[:500] if last_err else 'unknown',\n",
        "            })\n",
        "            # 실패는 existing에 기록하지 않음 -> 다음 run에서 다시 시도됨\n",
        "            print(f'[error] failed: {ckey} -> {last_err}')\n",
        "            if stop_on_error:\n",
        "                law_map = aggregate_by_law(chunks, existing)\n",
        "                save_abbr_outputs(existing, law_map)\n",
        "                failed_path.write_text(json.dumps(failed, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "                print('stop_on_error=True: checkpoint saved and stopped.')\n",
        "                break\n",
        "            continue\n",
        "\n",
        "        processed_new += 1\n",
        "        if processed_new % checkpoint_every == 0:\n",
        "            law_map = aggregate_by_law(chunks, existing)\n",
        "            save_abbr_outputs(existing, law_map)\n",
        "            failed_path.write_text(json.dumps(failed, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "            print(f'checkpoint saved: +{processed_new} new')\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f'progress: seen {i}/{total}, new {processed_new}, total saved {len(existing)}, failed {len(failed)}')\n",
        "\n",
        "    law_abbr_maps = aggregate_by_law(chunks, existing)\n",
        "    p_chunk, p_law = save_abbr_outputs(existing, law_abbr_maps)\n",
        "\n",
        "    failed_path.write_text(json.dumps(failed, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "\n",
        "    print('saved chunk:', p_chunk)\n",
        "    print('saved law:', p_law)\n",
        "    print('failed log:', failed_path)\n",
        "    print('failed count:', len(failed))\n",
        "    return existing, law_abbr_maps, failed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f6dc74",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 4) 실행\n",
        "# 중간 에러/중단 후 재실행하면 abbr_maps_by_chunk.json 기준으로 이어서 진행됩니다.\n",
        "\n",
        "chunk_abbr_maps, law_abbr_maps, failed = run_abbr_extraction_for_chunks(\n",
        "    all_chunks,\n",
        "    limit=None,\n",
        "    resume=True,\n",
        "    retry_empty=False,\n",
        "    checkpoint_every=5,\n",
        "    max_retries=3,\n",
        "    base_sleep=1.0,\n",
        "    stop_on_error=False,\n",
        ")\n",
        "\n",
        "print('chunk_abbr_maps:', len(chunk_abbr_maps))\n",
        "print('law_abbr_maps:', {k: len(v) for k, v in law_abbr_maps.items()})\n",
        "print('failed:', len(failed))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "natna",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}