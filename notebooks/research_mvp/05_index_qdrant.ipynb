{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "125d3bfc",
      "metadata": {},
      "source": [
        "# 05. Qdrant 인덱싱 (chunk별 abbr_map JSON 사용)\n",
        "\n",
        "`03`에서 생성한 `abbr_maps_by_law.json`을 읽어 인덱싱에 반영합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfb8915d",
      "metadata": {},
      "source": [
        "## Qdrant Cloud 설정\n",
        "\n",
        "클라우드 사용 시 `QDRANT_URL`, `QDRANT_API_KEY`를 설정하세요. 미설정이면 로컬 경로를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e985d90",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# 예시 (실제 값으로 변경)\n",
        "# os.environ[\"QDRANT_URL\"] = \"https://<cluster>.cloud.qdrant.io:6333\"\n",
        "# os.environ[\"QDRANT_API_KEY\"] = \"<api-key>\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a68b9be1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import uuid\n",
        "import hashlib\n",
        "from dataclasses import asdict, dataclass, field\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "def normalize_to_list(v):\n",
        "    if v is None:\n",
        "        return []\n",
        "    if isinstance(v, dict):\n",
        "        return [v]\n",
        "    if isinstance(v, list):\n",
        "        return v\n",
        "    return []\n",
        "\n",
        "\n",
        "def classify_law_type(name):\n",
        "    if '시행령' in name:\n",
        "        return '시행령'\n",
        "    if '시행규칙' in name:\n",
        "        return '시행규칙'\n",
        "    return '법률'\n",
        "\n",
        "\n",
        "def parse_article_numbers(article: dict, header: str) -> tuple[str, str]:\n",
        "    main = str(article.get('조문번호', '') or '').strip()\n",
        "    sub = str(article.get('조문가지번호', '') or '').strip()\n",
        "\n",
        "    if not main or (not sub and '의' in header):\n",
        "        m = re.search(r'제\\s*(\\d+)\\s*조(?:\\s*의\\s*(\\d+))?', header or '')\n",
        "        if m:\n",
        "            if not main:\n",
        "                main = m.group(1) or ''\n",
        "            if not sub:\n",
        "                sub = m.group(2) or ''\n",
        "\n",
        "    return main, sub\n",
        "\n",
        "\n",
        "def make_chunk_key(law_id: str, article_num: str, article_sub: str | int | None = '') -> str:\n",
        "    lid = str(law_id).strip().zfill(6)\n",
        "    sub = str(article_sub or '').strip()\n",
        "    sub_num = sub if sub else '0'\n",
        "    return f\"{lid}:{str(article_num).strip()}:{sub_num}\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Reference:\n",
        "    law_name: str | None\n",
        "    article: str\n",
        "    paragraph: str | None = None\n",
        "    item: str | None = None\n",
        "    raw: str = ''\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ArticleChunk:\n",
        "    law_name: str\n",
        "    law_id: str\n",
        "    law_type: str\n",
        "    article_num: str\n",
        "    article_sub: str = ''\n",
        "    article_title: str = ''\n",
        "    content: str = ''\n",
        "    paragraphs: list[dict] = field(default_factory=list)\n",
        "    internal_refs: list[Reference] = field(default_factory=list)\n",
        "    external_refs: list[Reference] = field(default_factory=list)\n",
        "    abbreviations: dict[str, str] = field(default_factory=dict)\n",
        "    effective_date: str = ''\n",
        "    change_type: str = ''\n",
        "\n",
        "    def payload(self):\n",
        "        return {\n",
        "            'law_name': self.law_name,\n",
        "            'law_id': self.law_id,\n",
        "            'law_type': self.law_type,\n",
        "            'article_num': self.article_num,\n",
        "            'article_sub': self.article_sub,\n",
        "            'article_title': self.article_title,\n",
        "            'content_original': self.content,\n",
        "            'paragraphs': self.paragraphs,\n",
        "            'internal_refs': [asdict(r) for r in self.internal_refs],\n",
        "            'external_refs': [asdict(r) for r in self.external_refs],\n",
        "            'abbreviations': self.abbreviations,\n",
        "            'effective_date': self.effective_date,\n",
        "            'change_type': self.change_type,\n",
        "        }\n",
        "\n",
        "\n",
        "def fetch_law_json(law_id):\n",
        "    oc = os.getenv('OC', '')\n",
        "    if not oc:\n",
        "        raise ValueError('Missing OC in .env')\n",
        "    url = 'http://www.law.go.kr/DRF/lawService.do'\n",
        "    params = {'OC': oc, 'target': 'eflaw', 'ID': law_id, 'type': 'JSON'}\n",
        "    resp = requests.get(url, params=params, timeout=30)\n",
        "    resp.raise_for_status()\n",
        "    return resp.json()\n",
        "\n",
        "\n",
        "def parse_law_data(data):\n",
        "    law_name = data['법령']['기본정보']['법령명_한글']\n",
        "    law_id = str(data['법령']['기본정보']['법령ID']).zfill(6)\n",
        "    out = []\n",
        "    for article in normalize_to_list(data['법령']['조문'].get('조문단위')):\n",
        "        if article.get('조문여부') != '조문':\n",
        "            continue\n",
        "        header = str(article.get('조문내용', '')).strip()\n",
        "        article_num, article_sub = parse_article_numbers(article, header)\n",
        "\n",
        "        content_parts = [header] if header else []\n",
        "        paragraphs = []\n",
        "        for para in normalize_to_list(article.get('항')):\n",
        "            para_content = str(para.get('항내용', '')).strip()\n",
        "            if para_content:\n",
        "                content_parts.append(para_content)\n",
        "            paragraphs.append({'num': str(para.get('항번호', '')).strip(), 'content': para_content})\n",
        "\n",
        "        out.append(ArticleChunk(\n",
        "            law_name=law_name,\n",
        "            law_id=law_id,\n",
        "            law_type=classify_law_type(law_name),\n",
        "            article_num=article_num,\n",
        "            article_sub=article_sub,\n",
        "            article_title=str(article.get('조문제목', '')).strip(),\n",
        "            content='\\n'.join([x for x in content_parts if x]),\n",
        "            paragraphs=paragraphs,\n",
        "            effective_date=str(article.get('조문시행일자', '')),\n",
        "            change_type=str(article.get('조문제개정유형', '')),\n",
        "        ))\n",
        "    return out\n",
        "\n",
        "\n",
        "# LEGACY fallback (현재 파이프라인에서는 사용하지 않음)\n",
        "INTERNAL_PATTERN = re.compile(r'제(\\d+(?:의\\d+)?)조(?:제(\\d+)항)?(?:제(\\d+)호)?')\n",
        "EXTERNAL_PATTERN = re.compile(r'「([^」]+)」\\s*제(\\d+(?:의\\d+)?)조(?:\\s*제(\\d+)항)?(?:\\s*제(\\d+)호)?')\n",
        "\n",
        "\n",
        "def extract_refs(chunks):\n",
        "    for c in chunks:\n",
        "        for m in EXTERNAL_PATTERN.finditer(c.content):\n",
        "            if m.group(1) != c.law_name:\n",
        "                c.external_refs.append(Reference(m.group(1), m.group(2), m.group(3), m.group(4), m.group(0)))\n",
        "        clean = EXTERNAL_PATTERN.sub('', c.content)\n",
        "        for m in INTERNAL_PATTERN.finditer(clean):\n",
        "            c.internal_refs.append(Reference(c.law_name, m.group(1), m.group(2), m.group(3), m.group(0)))\n",
        "\n",
        "\n",
        "def apply_chunk_abbr_map(chunks, chunk_abbr_maps):\n",
        "    for c in chunks:\n",
        "        key_new = make_chunk_key(c.law_id, c.article_num, c.article_sub)\n",
        "        key_old = f\"{c.law_id}:{c.article_num}\"\n",
        "        amap = chunk_abbr_maps.get(key_new)\n",
        "        if amap is None:\n",
        "            amap = chunk_abbr_maps.get(key_old, {})\n",
        "        c.abbreviations = amap if isinstance(amap, dict) else {}\n",
        "\n",
        "\n",
        "def index_qdrant(\n",
        "    chunks,\n",
        "    collection_name='building_law',\n",
        "    qdrant_path='./qdrant_data',\n",
        "    qdrant_url=None,\n",
        "    qdrant_api_key=None,\n",
        "    prefer_grpc=False,\n",
        "    batch_size=32,\n",
        "    max_retries=6,\n",
        "    base_sleep=1.0,\n",
        "    failed_log_path='data/processed/qdrant_failed_chunks.json',\n",
        "):\n",
        "    import math\n",
        "    import time\n",
        "\n",
        "    from langchain_core.documents import Document\n",
        "    from langchain_naver import ClovaXEmbeddings\n",
        "    from langchain_qdrant import QdrantVectorStore\n",
        "    from qdrant_client import QdrantClient\n",
        "    from qdrant_client.http.exceptions import UnexpectedResponse\n",
        "    from qdrant_client.http.models import Distance, VectorParams\n",
        "\n",
        "    def is_rate_limit_error(err: Exception) -> bool:\n",
        "        msg = str(err)\n",
        "        return ('429' in msg) or ('RateLimitError' in msg) or ('rate exceeded' in msg.lower())\n",
        "\n",
        "    def compute_content_hash(chunk) -> str:\n",
        "        raw = f\"{chunk.law_id}|{chunk.article_num}|{chunk.article_sub}|{chunk.article_title}|{chunk.content}\"\n",
        "        return hashlib.sha256(raw.encode('utf-8')).hexdigest()\n",
        "\n",
        "    def save_failed(failed_items):\n",
        "        path = Path(failed_log_path)\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        path.write_text(json.dumps(failed_items, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "        return str(path)\n",
        "\n",
        "    url = (qdrant_url or os.getenv('QDRANT_URL') or '').strip()\n",
        "    api_key = (qdrant_api_key or os.getenv('QDRANT_API_KEY') or '').strip()\n",
        "\n",
        "    if url:\n",
        "        if '<' in url or 'your-' in url:\n",
        "            raise ValueError('QDRANT_URL 예시값이 그대로 들어가 있습니다. 실제 클러스터 URL로 바꾸세요.')\n",
        "        if not api_key:\n",
        "            raise ValueError('QDRANT_URL이 설정된 경우 QDRANT_API_KEY도 필요합니다.')\n",
        "        client = QdrantClient(url=url, api_key=api_key, prefer_grpc=prefer_grpc)\n",
        "    else:\n",
        "        client = QdrantClient(path=qdrant_path)\n",
        "\n",
        "    try:\n",
        "        exists = client.collection_exists(collection_name)\n",
        "    except UnexpectedResponse as e:\n",
        "        if '403' in str(e):\n",
        "            raise RuntimeError(\n",
        "                'Qdrant 403 Forbidden: API Key 권한 또는 URL이 잘못됐습니다. '\n",
        "                'Cloud 콘솔 API Key(Write)와 endpoint(:6333) 확인하세요.'\n",
        "            ) from e\n",
        "        raise\n",
        "\n",
        "    if not exists:\n",
        "        client.create_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n",
        "        )\n",
        "\n",
        "    embeddings = ClovaXEmbeddings(model='bge-m3')\n",
        "    store = QdrantVectorStore(client=client, collection_name=collection_name, embedding=embeddings)\n",
        "\n",
        "    existing_hash_by_source = {}\n",
        "    next_offset = None\n",
        "    while True:\n",
        "        points, next_offset = client.scroll(\n",
        "            collection_name=collection_name,\n",
        "            offset=next_offset,\n",
        "            limit=1000,\n",
        "            with_payload=True,\n",
        "            with_vectors=False,\n",
        "        )\n",
        "        for pt in points:\n",
        "            payload = pt.payload or {}\n",
        "            sk = payload.get('source_key')\n",
        "            if sk:\n",
        "                existing_hash_by_source[str(sk)] = payload.get('content_hash')\n",
        "        if next_offset is None:\n",
        "            break\n",
        "\n",
        "    candidates = []\n",
        "    for c in chunks:\n",
        "        source_key = make_chunk_key(c.law_id, c.article_num, c.article_sub)\n",
        "        point_id = str(uuid.uuid5(uuid.NAMESPACE_URL, source_key))\n",
        "        content_hash = compute_content_hash(c)\n",
        "\n",
        "        payload = c.payload()\n",
        "        payload['content_hash'] = content_hash\n",
        "        payload['source_key'] = source_key\n",
        "        doc = Document(page_content=c.content, metadata=payload)\n",
        "        candidates.append((source_key, point_id, content_hash, doc))\n",
        "\n",
        "    total = len(candidates)\n",
        "    if total == 0:\n",
        "        return {'created': 0, 'updated': 0, 'skipped': 0, 'written': 0, 'failed': 0, 'total': 0, 'failed_log': ''}\n",
        "\n",
        "    created = 0\n",
        "    updated = 0\n",
        "    skipped = 0\n",
        "    failed_items = []\n",
        "    to_upsert = []\n",
        "\n",
        "    for source_key, point_id, h, doc in candidates:\n",
        "        old_hash = existing_hash_by_source.get(source_key)\n",
        "        if old_hash is None:\n",
        "            created += 1\n",
        "            to_upsert.append((source_key, point_id, h, doc))\n",
        "        elif old_hash != h:\n",
        "            updated += 1\n",
        "            to_upsert.append((source_key, point_id, h, doc))\n",
        "        else:\n",
        "            skipped += 1\n",
        "\n",
        "    total_upsert = len(to_upsert)\n",
        "    if total_upsert == 0:\n",
        "        print(f'no changes: skipped={skipped}, total={total}')\n",
        "        return {\n",
        "            'created': created,\n",
        "            'updated': updated,\n",
        "            'skipped': skipped,\n",
        "            'written': 0,\n",
        "            'failed': 0,\n",
        "            'total': total,\n",
        "            'failed_log': '',\n",
        "        }\n",
        "\n",
        "    num_batches = math.ceil(total_upsert / batch_size)\n",
        "    written = 0\n",
        "\n",
        "    def upsert_one(source_key, point_id, doc):\n",
        "        for attempt in range(max_retries + 1):\n",
        "            try:\n",
        "                store.add_documents(documents=[doc], ids=[point_id])\n",
        "                return True, ''\n",
        "            except Exception as e:\n",
        "                if is_rate_limit_error(e) and attempt < max_retries:\n",
        "                    wait = min(base_sleep * (2 ** attempt), 30.0)\n",
        "                    time.sleep(wait)\n",
        "                    continue\n",
        "                return False, str(e)\n",
        "\n",
        "    for bi in range(num_batches):\n",
        "        s = bi * batch_size\n",
        "        e = min((bi + 1) * batch_size, total_upsert)\n",
        "        batch = to_upsert[s:e]\n",
        "\n",
        "        d_batch = [x[3] for x in batch]\n",
        "        i_batch = [x[1] for x in batch]\n",
        "\n",
        "        batch_done = False\n",
        "        for attempt in range(max_retries + 1):\n",
        "            try:\n",
        "                store.add_documents(documents=d_batch, ids=i_batch)\n",
        "                written += len(d_batch)\n",
        "                print(f\"batch {bi+1}/{num_batches} ok: +{len(d_batch)} (written={written})\")\n",
        "                time.sleep(0.25)\n",
        "                batch_done = True\n",
        "                break\n",
        "            except Exception as err:\n",
        "                if is_rate_limit_error(err) and attempt < max_retries:\n",
        "                    wait = min(base_sleep * (2 ** attempt), 30.0)\n",
        "                    print(f\"rate limited on batch {bi+1}, retry {attempt+1}/{max_retries}, sleep={wait:.1f}s\")\n",
        "                    time.sleep(wait)\n",
        "                    continue\n",
        "                print(f\"batch {bi+1} failed, fallback to per-point retry: {err}\")\n",
        "                break\n",
        "\n",
        "        if batch_done:\n",
        "            continue\n",
        "\n",
        "        for source_key, point_id, _h, doc in batch:\n",
        "            ok, msg = upsert_one(source_key, point_id, doc)\n",
        "            if ok:\n",
        "                written += 1\n",
        "            else:\n",
        "                failed_items.append({\n",
        "                    'source_key': source_key,\n",
        "                    'point_id': point_id,\n",
        "                    'error': msg[:500],\n",
        "                })\n",
        "\n",
        "    failed_log = ''\n",
        "    if failed_items:\n",
        "        failed_log = save_failed(failed_items)\n",
        "        print(f\"failed chunks: {len(failed_items)} -> {failed_log}\")\n",
        "\n",
        "    return {\n",
        "        'created': created,\n",
        "        'updated': updated,\n",
        "        'skipped': skipped,\n",
        "        'written': written,\n",
        "        'failed': len(failed_items),\n",
        "        'total': total,\n",
        "        'failed_log': failed_log,\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "def sanitize_filename_component(name: str) -> str:\n",
        "    return str(name).replace(' ', '_')\n",
        "\n",
        "\n",
        "def _normalize_ref_list(refs):\n",
        "    out = []\n",
        "    for r in refs or []:\n",
        "        if not isinstance(r, dict):\n",
        "            continue\n",
        "        out.append(\n",
        "            Reference(\n",
        "                law_name=r.get('law_name'),\n",
        "                article=str(r.get('article', '')).strip(),\n",
        "                paragraph=(str(r.get('paragraph')).strip() if r.get('paragraph') not in [None, ''] else None),\n",
        "                item=(str(r.get('item')).strip() if r.get('item') not in [None, ''] else None),\n",
        "                raw=str(r.get('raw', '')).strip(),\n",
        "            )\n",
        "        )\n",
        "    return out\n",
        "\n",
        "\n",
        "def apply_ref_map_to_chunks(chunks, ref_map_by_law):\n",
        "    applied = 0\n",
        "    for c in chunks:\n",
        "        law_id = str(c.law_id).strip().zfill(6)\n",
        "        law_map = ref_map_by_law.get(law_id, {})\n",
        "        if not isinstance(law_map, dict):\n",
        "            continue\n",
        "\n",
        "        key = make_chunk_key(c.law_id, c.article_num, c.article_sub)\n",
        "        row = law_map.get(key)\n",
        "\n",
        "        # backward compatibility (old key format)\n",
        "        if row is None:\n",
        "            row = law_map.get(f\"{c.law_id}:{c.article_num}\")\n",
        "\n",
        "        if not isinstance(row, dict):\n",
        "            continue\n",
        "\n",
        "        c.internal_refs = _normalize_ref_list(row.get('internal_refs', []))\n",
        "        c.external_refs = _normalize_ref_list(row.get('external_refs', []))\n",
        "        applied += 1\n",
        "\n",
        "    return applied\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _split_article_token(token: str) -> tuple[str, str]:\n",
        "    t = str(token or '').strip()\n",
        "    if not t:\n",
        "        return '', ''\n",
        "    m = re.fullmatch(r'(\\d+)(?:의(\\d+))?', t)\n",
        "    if m:\n",
        "        return m.group(1) or '', m.group(2) or ''\n",
        "    return t, ''\n",
        "\n",
        "\n",
        "def _normalize_chunk_ref_map_keys(raw_map: dict, target_law_id: str) -> dict:\n",
        "    norm = {}\n",
        "    if not isinstance(raw_map, dict):\n",
        "        return norm\n",
        "\n",
        "    for k, v in raw_map.items():\n",
        "        if not isinstance(v, dict):\n",
        "            continue\n",
        "\n",
        "        parts = str(k).split(':')\n",
        "        article_num = ''\n",
        "        article_sub = ''\n",
        "\n",
        "        if len(parts) >= 3:\n",
        "            # law:article:sub\n",
        "            article_num = str(parts[1]).strip()\n",
        "            article_sub = str(parts[2]).strip()\n",
        "        elif len(parts) == 2:\n",
        "            # law:article  (article may be 4 or 4의2)\n",
        "            article_token = str(parts[1]).strip()\n",
        "            article_num, article_sub = _split_article_token(article_token)\n",
        "        elif len(parts) == 1:\n",
        "            # article only\n",
        "            article_num, article_sub = _split_article_token(parts[0])\n",
        "\n",
        "        if not article_num:\n",
        "            continue\n",
        "\n",
        "        nk = make_chunk_key(target_law_id, article_num, article_sub)\n",
        "        norm[nk] = v\n",
        "\n",
        "    return norm\n",
        "\n",
        "\n",
        "def load_golden_ref_map_by_law(base_dir='data/processed/chunks_golden/chunk_ref_map'):\n",
        "    candidates = [\n",
        "        Path(base_dir),\n",
        "        Path('notebooks/research_mvp') / base_dir,\n",
        "    ]\n",
        "    base = next((x for x in candidates if x.exists()), None)\n",
        "    if base is None:\n",
        "        print('[warn] golden ref map dir not found:', candidates)\n",
        "        return {}\n",
        "\n",
        "    out = {}\n",
        "    for p in sorted(base.glob('*.json')):\n",
        "        law_id = p.stem.strip().zfill(6)  # file name is source of truth: 001823 / 002118\n",
        "        try:\n",
        "            data = json.loads(p.read_text(encoding='utf-8'))\n",
        "            out[law_id] = _normalize_chunk_ref_map_keys(data, law_id)\n",
        "        except Exception as e:\n",
        "            print('[warn] failed to load golden ref map:', p, e)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_ordin_chunks_from_export(chunks_ordin_dir: str | None = None):\n",
        "    candidates = [\n",
        "        Path(chunks_ordin_dir) if chunks_ordin_dir else None,\n",
        "        Path('data/processed/chunks_ordin'),\n",
        "        Path('data/processed/chunks_ordin'),\n",
        "        Path('notebooks/research_mvp/data/processed/chunks_ordin'),\n",
        "    ]\n",
        "    candidates = [p for p in candidates if p is not None]\n",
        "\n",
        "    src = next((p for p in candidates if p.exists()), None)\n",
        "    if src is None:\n",
        "        return [], None\n",
        "\n",
        "    all_file = src / 'all_ordin_chunks.json'\n",
        "    if not all_file.exists():\n",
        "        return [], src\n",
        "\n",
        "    rows = json.loads(all_file.read_text(encoding='utf-8'))\n",
        "    out = []\n",
        "    for r in rows:\n",
        "        try:\n",
        "            out.append(ArticleChunk(\n",
        "                law_name=str(r.get('law_name', '')).strip(),\n",
        "                law_id=str(r.get('law_id', '')).strip(),\n",
        "                law_type='조례',\n",
        "                article_num=str(r.get('article_num', '')).strip(),\n",
        "                article_sub=str(r.get('article_sub', '') or '').strip(),\n",
        "                article_title=str(r.get('article_title', '')).strip(),\n",
        "                content=str(r.get('content', '')).strip(),\n",
        "                paragraphs=r.get('paragraphs', []) if isinstance(r.get('paragraphs', []), list) else [],\n",
        "                effective_date=str(r.get('effective_date', '')).strip(),\n",
        "                change_type=str(r.get('change_type', '')).strip(),\n",
        "            ))\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    return out, src\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d9ea7a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 0: Qdrant 연결 사전 점검\n",
        "import os\n",
        "import requests\n",
        "\n",
        "url = (os.getenv('QDRANT_URL') or '').strip()\n",
        "api_key = (os.getenv('QDRANT_API_KEY') or '').strip()\n",
        "\n",
        "if not url:\n",
        "    print('Mode: LOCAL (QDRANT_URL 미설정)')\n",
        "else:\n",
        "    print('Mode: CLOUD')\n",
        "    print('API key set =', bool(api_key))\n",
        "\n",
        "    headers = {'api-key': api_key} if api_key else {}\n",
        "    try:\n",
        "        r = requests.get(f\"{url}/collections\", headers=headers, timeout=15)\n",
        "        print('GET /collections status =', r.status_code)\n",
        "        if r.status_code == 403:\n",
        "            print('-> 403: API key 권한 또는 URL이 잘못됐습니다.')\n",
        "            print('   1) Cloud 콘솔 API Key(Write 권한) 확인')\n",
        "            print('   2) endpoint가 API endpoint인지 확인 (:6333 포함 권장)')\n",
        "        elif r.status_code >= 400:\n",
        "            print('-> error body:', r.text[:300])\n",
        "        else:\n",
        "            data = r.json()\n",
        "            print('collections:', [c.get('name') for c in data.get('result', {}).get('collections', [])][:10])\n",
        "    except Exception as e:\n",
        "        print('request failed:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "034542a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 1: raw 데이터 수집/로드 (법령 + 조례)\n",
        "all_chunks = []\n",
        "\n",
        "# 1) 법령(건축법/시행령) raw\n",
        "raw_dir_candidates = [\n",
        "    Path('data/processed/raw'),\n",
        "    Path('notebooks/research_mvp/data/processed/raw'),\n",
        "]\n",
        "raw_dir = next((p for p in raw_dir_candidates if p.exists()), None)\n",
        "\n",
        "if raw_dir is not None:\n",
        "    files = sorted(raw_dir.glob('*.json'))\n",
        "    print('raw source: local', raw_dir)\n",
        "    for fp in files:\n",
        "        data = json.loads(fp.read_text(encoding='utf-8'))\n",
        "        all_chunks.extend(parse_law_data(data))\n",
        "else:\n",
        "    print('raw source: law api')\n",
        "    for law_id in ('1823', '2118'):\n",
        "        data = fetch_law_json(law_id)\n",
        "        all_chunks.extend(parse_law_data(data))\n",
        "\n",
        "# 2) 자치법규(조례) parsed export\n",
        "ordin_chunks, ordin_src = load_ordin_chunks_from_export()\n",
        "if ordin_src is not None:\n",
        "    print('ordin source:', ordin_src)\n",
        "print('ordin chunks loaded:', len(ordin_chunks))\n",
        "all_chunks.extend(ordin_chunks)\n",
        "\n",
        "print('loaded chunks total:', len(all_chunks))\n",
        "print('law ids sample:', sorted({str(c.law_id) for c in all_chunks})[:10])\n",
        "print('law_type counts:')\n",
        "from collections import Counter\n",
        "print(dict(Counter([str(c.law_type) for c in all_chunks])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17dc82d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b2a2250",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 2: 03 노트북에서 만든 chunk별 abbr_map JSON 로드\n",
        "abbr_candidates = [\n",
        "    Path('data/processed/abbr_maps_by_chunk.json'),\n",
        "    Path('notebooks/research_mvp/data/processed/abbr_maps_by_chunk.json'),\n",
        "    Path('data/processed/abbr_maps_by_chunk.agent.json'),\n",
        "    Path('notebooks/research_mvp/data/processed/abbr_maps_by_chunk.agent.json'),\n",
        "]\n",
        "abbr_json_path = next((p for p in abbr_candidates if p.exists()), None)\n",
        "\n",
        "if abbr_json_path is None:\n",
        "    raise FileNotFoundError('먼저 03_refs_and_abbr.ipynb를 실행해서 chunk별 abbr_map JSON을 생성하세요.')\n",
        "\n",
        "chunk_abbr_maps = json.loads(abbr_json_path.read_text(encoding='utf-8'))\n",
        "print('abbr source:', abbr_json_path)\n",
        "print('chunk abbr count:', len(chunk_abbr_maps))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "863fa1fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3: golden ref map 반영 + chunk별 축약어 매핑\n",
        "# 정규식 ref 추출은 사용하지 않음 (golden ref map만 신뢰)\n",
        "ref_map_by_law = load_golden_ref_map_by_law('data/processed/chunks_golden/chunk_ref_map')\n",
        "applied = apply_ref_map_to_chunks(all_chunks, ref_map_by_law)\n",
        "print('golden refs applied chunks:', applied)\n",
        "\n",
        "cov = {}\n",
        "unmatched_by_law = {}\n",
        "for c in all_chunks:\n",
        "    lid = str(c.law_id).strip().zfill(6)\n",
        "    k = make_chunk_key(c.law_id, c.article_num, c.article_sub)\n",
        "\n",
        "    cov.setdefault(lid, {'total': 0, 'with_ref': 0, 'without_ref': 0, 'mapped_key': 0})\n",
        "    cov[lid]['total'] += 1\n",
        "\n",
        "    if c.internal_refs or c.external_refs:\n",
        "        cov[lid]['with_ref'] += 1\n",
        "    else:\n",
        "        cov[lid]['without_ref'] += 1\n",
        "\n",
        "    law_map = ref_map_by_law.get(lid, {})\n",
        "    if isinstance(law_map, dict) and k in law_map:\n",
        "        cov[lid]['mapped_key'] += 1\n",
        "    else:\n",
        "        unmatched_by_law.setdefault(lid, []).append(k)\n",
        "\n",
        "print('coverage by law:', cov)\n",
        "for lid, keys in unmatched_by_law.items():\n",
        "    print(f'unmatched sample [{lid}] ({len(keys)}):', keys[:10])\n",
        "\n",
        "apply_chunk_abbr_map(all_chunks, chunk_abbr_maps)\n",
        "\n",
        "sample = next(c for c in all_chunks if c.law_name == '건축법 시행령')\n",
        "print('sample law:', sample.law_name)\n",
        "print('chunk key:', make_chunk_key(sample.law_id, sample.article_num, sample.article_sub))\n",
        "print('article num/sub:', sample.article_num, sample.article_sub or '0')\n",
        "print('refs internal/external:', len(sample.internal_refs), len(sample.external_refs))\n",
        "print('abbr size:', len(sample.abbreviations))\n",
        "print('content head:', sample.content[:180])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b69f2e68",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3-1: chunk 품질 점검 (abbr / refs)\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def chunk_key(c):\n",
        "    return make_chunk_key(c.law_id, c.article_num, c.article_sub)\n",
        "\n",
        "def abbr_size(c):\n",
        "    amap = c.abbreviations if isinstance(c.abbreviations, dict) else {}\n",
        "    if '약어' in amap and isinstance(amap.get('약어'), dict):\n",
        "        amap = amap['약어']\n",
        "    return len(amap)\n",
        "\n",
        "n_total = len(all_chunks)\n",
        "n_abbr_nonempty = sum(1 for c in all_chunks if abbr_size(c) > 0)\n",
        "n_internal = sum(1 for c in all_chunks if len(c.internal_refs) > 0)\n",
        "n_external = sum(1 for c in all_chunks if len(c.external_refs) > 0)\n",
        "\n",
        "print('total chunks:', n_total)\n",
        "print('abbr non-empty:', n_abbr_nonempty)\n",
        "print('internal refs non-empty:', n_internal)\n",
        "print('external refs non-empty:', n_external)\n",
        "\n",
        "suspect_abbr_miss = [\n",
        "    c for c in all_chunks\n",
        "    if ('이하' in c.content and abbr_size(c) == 0)\n",
        "]\n",
        "print('suspect abbr missing:', len(suspect_abbr_miss))\n",
        "\n",
        "article_ref_pat = re.compile(r'제\\d+(?:의\\d+)?조')\n",
        "suspect_ref_miss = [\n",
        "    c for c in all_chunks\n",
        "    if article_ref_pat.search(c.content) and len(c.internal_refs) == 0 and len(c.external_refs) == 0\n",
        "]\n",
        "print('suspect ref missing:', len(suspect_ref_miss))\n",
        "\n",
        "cnt_by_law = Counter(c.law_name for c in all_chunks)\n",
        "print('chunks by law:', dict(cnt_by_law))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "944af36f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3-2: 의심 청크 샘플 확인\n",
        "def preview_chunk(c):\n",
        "    print('---')\n",
        "    print('key:', chunk_key(c))\n",
        "    print('law:', c.law_name, '| article:', c.article_num, c.article_sub or '0', c.article_title)\n",
        "    print('abbr size:', abbr_size(c))\n",
        "    print('internal/external:', len(c.internal_refs), len(c.external_refs))\n",
        "    print('head:', (c.content or '')[:260].replace('\\n', ' / '))\n",
        "\n",
        "print('[suspect_abbr_miss sample]')\n",
        "for c in suspect_abbr_miss[:5]:\n",
        "    preview_chunk(c)\n",
        "\n",
        "print('[suspect_ref_miss sample]')\n",
        "for c in suspect_ref_miss[:5]:\n",
        "    preview_chunk(c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d53880",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3-3: 단건 상세 점검 (원하는 key로 확인)\n",
        "inspect_key = chunk_key(all_chunks[4])  # 예: '001823:4:2'\n",
        "target = next((c for c in all_chunks if chunk_key(c) == inspect_key), None)\n",
        "\n",
        "if not target:\n",
        "    print('not found:', inspect_key)\n",
        "else:\n",
        "    print('key:', inspect_key)\n",
        "    print('law/article:', target.law_name, target.article_num, target.article_sub or '0', target.article_title)\n",
        "    print('abbr:', target.abbreviations)\n",
        "    print('internal_refs:', [asdict(r) for r in target.internal_refs])\n",
        "    print('external_refs:', [asdict(r) for r in target.external_refs])\n",
        "    print('content\\n', target.content[:1200])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cc6b9ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3-4: refs/abbr 반영된 chunk 결과 저장\n",
        "from dataclasses import asdict\n",
        "\n",
        "out_dir = Path('data/processed/chunks_enriched')\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def chunk_to_enriched_dict(c):\n",
        "    return {\n",
        "        'chunk_key': make_chunk_key(c.law_id, c.article_num, c.article_sub),\n",
        "        'law_name': c.law_name,\n",
        "        'law_id': c.law_id,\n",
        "        'law_type': c.law_type,\n",
        "        'article_num': c.article_num,\n",
        "        'article_sub': c.article_sub,\n",
        "        'article_title': c.article_title,\n",
        "        'content': c.content,\n",
        "        'abbreviations': c.abbreviations,\n",
        "        'internal_refs': [asdict(r) for r in c.internal_refs],\n",
        "        'external_refs': [asdict(r) for r in c.external_refs],\n",
        "        'effective_date': c.effective_date,\n",
        "        'change_type': c.change_type,\n",
        "    }\n",
        "\n",
        "enriched = [chunk_to_enriched_dict(c) for c in all_chunks]\n",
        "\n",
        "all_path = out_dir / 'all_chunks_enriched.json'\n",
        "all_path.write_text(json.dumps(enriched, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "\n",
        "by_law = {}\n",
        "for row in enriched:\n",
        "    by_law.setdefault(row['law_id'], []).append(row)\n",
        "\n",
        "for law_id, rows in by_law.items():\n",
        "    law_name = rows[0]['law_name']\n",
        "    p_law = out_dir / f\"{law_id}_{sanitize_filename_component(law_name)}_chunks_enriched.json\"\n",
        "    p_law.write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "\n",
        "ref_map = {\n",
        "    row['chunk_key']: {\n",
        "        'internal_refs': row['internal_refs'],\n",
        "        'external_refs': row['external_refs'],\n",
        "    }\n",
        "    for row in enriched\n",
        "}\n",
        "ref_path = out_dir / 'chunk_ref_map.json'\n",
        "ref_path.write_text(json.dumps(ref_map, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "\n",
        "print('saved all:', all_path)\n",
        "print('saved ref map:', ref_path)\n",
        "print('law files:', len(by_law))\n",
        "\n",
        "next((r for r in enriched if r['internal_refs'] or r['external_refs']), enriched[0] if enriched else {})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e413287",
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks[-20:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44d23e90",
      "metadata": {},
      "outputs": [],
      "source": [
        "len(all_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8da7b14e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 4: Qdrant 단일 컬렉션 인덱싱 (Cloud/Local 자동 + rate-limit + hash incremental)\n",
        "stats = index_qdrant(\n",
        "    all_chunks[476:],\n",
        "    batch_size=8,\n",
        "    max_retries=3,\n",
        "    base_sleep=20.0,\n",
        ")\n",
        "print('index stats:', stats)\n",
        "if stats.get('failed'):\n",
        "    print('failed log file:', stats.get('failed_log'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e543661",
      "metadata": {},
      "source": [
        "## Rate Limit 대응 팁\n",
        "\n",
        "- 429가 계속 나면 `batch_size`를 더 낮추세요(예: 24 -> 12 -> 8).\n",
        "- `base_sleep`를 1.0 -> 2.0으로 올리면 재시도 간격이 늘어납니다.\n",
        "- 재실행 시 이미 저장된 ID는 덮어쓰기(upsert) 동작입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2115986",
      "metadata": {},
      "source": [
        "## Hash Incremental Upsert\n",
        "\n",
        "- `content_hash`가 같으면 임베딩/업로드를 건너뜁니다(`skipped`).\n",
        "- 신규는 `created`, 내용 변경은 `updated`로 집계됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43d66922",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 5: 실패 chunk만 재시도\n",
        "failed_path = Path('data/processed/qdrant_failed_chunks.json')\n",
        "if not failed_path.exists():\n",
        "    print('no failed log found')\n",
        "else:\n",
        "    failed_items = json.loads(failed_path.read_text(encoding='utf-8'))\n",
        "    failed_keys = {x['source_key'] for x in failed_items}\n",
        "    retry_chunks = [c for c in all_chunks if make_chunk_key(c.law_id, c.article_num, c.article_sub) in failed_keys]\n",
        "    print('retry chunk count:', len(retry_chunks))\n",
        "    retry_stats = index_qdrant(\n",
        "        retry_chunks,\n",
        "        batch_size=8,\n",
        "        max_retries=5,\n",
        "        base_sleep=2.0,\n",
        "    )\n",
        "    print('retry stats:', retry_stats)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "natna",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}