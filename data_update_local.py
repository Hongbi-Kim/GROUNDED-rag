import os
import requests
import pandas as pd
from tqdm import tqdm
import json
from pathlib import Path

from langchain_core.documents import Document
from collections import defaultdict


oc = os.getenv('OC', '')

df = pd.read_csv("data/ë²•ë ¹ê²€ìƒ‰ëª©ë¡_ìì¹˜ë²•ê·œ_ê±´ì¶•.csv", skiprows=1)

def load_documents_from_jsonl(file_path):
    """
    JSONL íŒŒì¼ì—ì„œ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    
    Args:
        file_path: JSONL íŒŒì¼ ê²½ë¡œ
        
    Returns:
        List[Document]: Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    documents = []
    
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            # ë¹ˆ ì¤„ ê±´ë„ˆë›°ê¸°
            if not line.strip():
                continue
                
            # JSON íŒŒì‹±
            data = json.loads(line)
            
            # Document ê°ì²´ ìƒì„±
            doc = Document(
                page_content=data["page_content"],
                metadata=data["metadata"],
                id=data["id"]
            )
            documents.append(doc)
    
    print(f"âœ… ì´ {len(documents)}ê°œ Document ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ")
    return documents

def save_documents(documents, file_path):
    """
    Document ë¦¬ìŠ¤íŠ¸ë¥¼ JSONL íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    with open(file_path, "w", encoding="utf-8") as f:
        for doc in documents:
            json.dump({
                "id": doc.id,
                "page_content": doc.page_content,
                "metadata": doc.metadata
            }, f, ensure_ascii=False)
            f.write("\n")
    print(f"âœ… {len(documents)}ê°œ Documentë¥¼ {file_path}ì— ì €ì¥ ì™„ë£Œ")

def extract_clause_documents(df, oc, chain_abb, chain, checkpoint_dir="checkpoints"):
    """
    ì¡°í•­ë³„ë¡œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ Document ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ì¤‘ê°„ ì €ì¥ ê¸°ëŠ¥ í¬í•¨.
    """
    # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(checkpoint_dir).mkdir(exist_ok=True)
    
    # ì´ë¯¸ ì²˜ë¦¬ëœ ë²•ê·œ ID í™•ì¸
    processed_ids = set()
    checkpoint_file = os.path.join(checkpoint_dir, "processed_ids.txt")
    if os.path.exists(checkpoint_file):
        with open(checkpoint_file, "r", encoding="utf-8") as f:
            processed_ids = set(line.strip() for line in f)
        print(f"âœ… ì´ë¯¸ ì²˜ë¦¬ëœ ë²•ê·œ: {len(processed_ids)}ê°œ")
    
    documents = []
    
    # ê¸°ì¡´ documents.jsonlì´ ìˆìœ¼ë©´ ë¡œë“œ
    if os.path.exists("documents.jsonl"):
        print("ê¸°ì¡´ documents.jsonl ë¡œë“œ ì¤‘...")
        documents = load_documents_from_jsonl("documents.jsonl")
        print(f"âœ… ê¸°ì¡´ ë¬¸ì„œì˜ ì¡°í•­ ë¦¬ìŠ¤íŠ¸ {len(documents)}ê°œ ë¡œë“œ ì™„ë£Œ")
    
    # ì²˜ë¦¬í•  ë²•ê·œë§Œ í•„í„°ë§
    # total_laws = len(df["ìì¹˜ë²•ê·œID"])
    df_law_ids = [str(id) for id in df["ìì¹˜ë²•ê·œID"]]
    laws_to_process = [id for id in df_law_ids if id not in processed_ids]
    
    if not laws_to_process:
        print("âœ… ëª¨ë“  ë²•ê·œê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return documents

    print(f"ğŸ”„ ì²˜ë¦¬í•  ë²•ê·œ: {len(laws_to_process)}ê°œ (ì „ì²´ {len(df_law_ids)}ê°œ ì¤‘)")

    for id in tqdm(laws_to_process, desc="ë²•ê·œ ì²˜ë¦¬"):
        # try:
        url = f"http://www.law.go.kr/DRF/lawService.do?OC={oc}&target=ordin&ID={id}&type=JSON"
        response = requests.get(url)
        response.raise_for_status()

        data = response.json()
        meta = data["LawService"]["ìì¹˜ë²•ê·œê¸°ë³¸ì •ë³´"]
        content = data["LawService"]["ì¡°ë¬¸"]["ì¡°"]

        # --- ì¡°ë¬¸ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
        ctx_list = []
        ctx_without_tt = []
        header = ""
        
        for ctx in content:
            if ctx["ì¡°ë¬¸ì—¬ë¶€"] == "Y":
                txt = header + " " + ctx["ì¡°ë‚´ìš©"]
                txt = txt.strip()
                ctx_list.append(txt)
                
                if ctx["ì¡°ë‚´ìš©"].find(")") != -1:
                    ctx_without_tt.append(ctx["ì¡°ë‚´ìš©"][ctx["ì¡°ë‚´ìš©"].find(")")+1:].strip())
                else:
                    ctx_without_tt.append(ctx["ì¡°ë‚´ìš©"])
            else:
                header = "[" + ctx["ì¡°ë‚´ìš©"] + "]"
        # print(f"  í˜„ì¬ ë²•ë ¹ì˜ ì´ ì¡°í•­ ê°œìˆ˜: {len(ctx_list)}")
        # --- ì•½ì–´ ì¶”ì¶œ ---
        abb = {}
        clause_abb_list = []  # ê° ì¡°í•­ì˜ ì•½ì–´ë¥¼ ì €ì¥
        for i, item in enumerate(ctx_list):
            if "ì´í•˜" in item:
                print(f"  ğŸ”¤ ì•½ì–´ ì¶”ì¶œ ì¤‘... ({i+1}/{len(ctx_list)})", end='\r')
                q = ctx_without_tt[i]
                abb_output = chain_abb.invoke({"question": q})
                
                filtered_abb = {
                    k: v for k, v in abb_output.items()
                    if (k in ["ì•½ì–´", ""] and v) or (k not in ["ì•½ì–´", ""])
                }
                abb.update(filtered_abb)
            else:
                filtered_abb = {}
            # print(f"ì•½ì–´: {filtered_abb}")
            clause_abb_list.append(filtered_abb)

        # --- ì¡°í•­ë³„ Document ìƒì„± ---
        cnt = 1
        law_documents = []  # í˜„ì¬ ë²•ê·œì˜ ë¬¸ì„œë“¤
        
        for i, item in enumerate(ctx_list):
            print(f" ğŸ”— links ì¶”ì¶œ ì¤‘... ({i+1}/{len(ctx_list)})", end='\r')
            chunk = (
                item
            )

            q = "ë²•ë¥  ë¬¸ì„œì˜ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: " + ctx_without_tt[i]
            links_llm = chain.invoke({"abb": abb, "question": q})

            filtered_links = {
                k: v for k, v in links_llm.items()
                if (k in ["ê¸°íƒ€", "ë²•ë¥  ë¬¸ì„œ ì œëª©"] and v) or (k not in ["ê¸°íƒ€", "ë²•ë¥  ë¬¸ì„œ ì œëª©"])
            }

            # --- ë©”íƒ€ë°ì´í„° ìƒì„± ---
            doc_meta = meta.copy()
            doc_meta["ì•½ì–´"] = clause_abb_list[i]
            if filtered_links:
                doc_meta["links"] = [filtered_links]

            # --- Document ê°ì²´ ìƒì„± ---
            doc = Document(
                page_content=chunk,
                metadata=doc_meta,
                id=f"{meta['ìì¹˜ë²•ê·œID']}_clause_{cnt}"
            )
            cnt += 1
            law_documents.append(doc)
        
        # í˜„ì¬ ë²•ê·œ ë¬¸ì„œë“¤ì„ ì „ì²´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        documents.extend(law_documents)
        
        # ì²˜ë¦¬ ì™„ë£Œëœ ID ê¸°ë¡
        processed_ids.add(id)
        with open(checkpoint_file, "a", encoding="utf-8") as f:
            f.write(f"{id}\n")
        
        # 10ê°œ ë²•ê·œë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
        # if len(processed_ids) % 10 == 0:
        save_documents(documents, "documents.jsonl")
        # print(f"\nğŸ’¾ ì¤‘ê°„ ì €ì¥: {len(documents)}ê°œ ë¬¸ì„œ, {len(processed_ids)}ê°œ ë²•ê·œ ì²˜ë¦¬ ì™„ë£Œ")
    
        # except Exception as e:
        #     print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ (ë²•ê·œ ID: {id}): {str(e)}")
        #     # ì˜¤ë¥˜ ë¡œê·¸ ì €ì¥
        #     with open(os.path.join(checkpoint_dir, "error_log.txt"), "a", encoding="utf-8") as f:
        #         f.write(f"{id}: {str(e)}\n")
        #     continue

    # ìµœì¢… ì €ì¥
    save_documents(documents, "documents.jsonl")
    print(f"\nâœ… ìµœì¢… ì €ì¥: {len(documents)}ê°œ ë¬¸ì„œ ì™„ë£Œ")
    
    return documents

def merge_documents_by_chapter(documents, min_len=300, max_len=600, 
                               checkpoint_dir="checkpoints", batch_size=1):
    """
    ì¡°í•­ë³„ Documentë¥¼ ì¥ ë‹¨ìœ„ë¡œ ë³‘í•©í•˜ì—¬ ì ì ˆí•œ í¬ê¸°ì˜ ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ì¤‘ê°„ ì €ì¥ ê¸°ëŠ¥ í¬í•¨.
    """
    # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(checkpoint_dir).mkdir(exist_ok=True)
    
    # ì´ë¯¸ ë³‘í•©ëœ ë¬¸ì„œê°€ ìˆìœ¼ë©´ ë¡œë“œ
    merged_docs = []
    if os.path.exists("documents_merged.jsonl"):
        print("ê¸°ì¡´ documents_merged.jsonl ë¡œë“œ ì¤‘...")
        merged_docs = load_documents_from_jsonl("documents_merged.jsonl")
        print(f"âœ… ê¸°ì¡´ ë³‘í•© ë¬¸ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸{len(merged_docs)}ê°œ ë¡œë“œ ì™„ë£Œ")
    
    # ì´ë¯¸ ì²˜ë¦¬ëœ ë²•ê·œ ID í™•ì¸
    processed_law_ids = set()
    merge_checkpoint_file = os.path.join(checkpoint_dir, "merged_law_ids.txt")
    if os.path.exists(merge_checkpoint_file):
        with open(merge_checkpoint_file, "r", encoding="utf-8") as f:
            processed_law_ids = set(line.strip() for line in f if line.strip())
        print(f"âœ… ì´ë¯¸ ë³‘í•©ëœ ë²•ê·œ: {len(processed_law_ids)}ê°œ")
    
    # 1ï¸âƒ£ Documentë“¤ì„ ìì¹˜ë²•ê·œID ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
    docs_by_law = defaultdict(list)
    for doc in documents:
        law_id = str(doc.metadata.get("ìì¹˜ë²•ê·œID"))
        if law_id not in processed_law_ids:
            docs_by_law[law_id].append(doc)
    
    if not docs_by_law:
        print("âœ… ëª¨ë“  ë²•ê·œê°€ ì´ë¯¸ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return merged_docs

    # 2ï¸âƒ£ ê° ë²•ë ¹ ë¬¸ì„œë³„ë¡œ ì²˜ë¦¬
    total_laws = len(docs_by_law)
    processed_count = 0
    
    print(f"ğŸ”„ ë³‘í•©í•  ë²•ê·œ: {total_laws}ê°œ")
    
    for law_id, doc_list in tqdm(docs_by_law.items(), desc="ë²•ë ¹ë³„ ë³‘í•©"):
        try:
            # IDë¥¼ ìˆ«ì ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (2019610_clause_1, 2019610_clause_2, ...)
            def extract_clause_number(doc):
                # ID í˜•ì‹: "2019610_clause_1"
                parts = doc.id.split("_")
                if len(parts) >= 3:
                    try:
                        return int(parts[2])  # clause ë²ˆí˜¸
                    except:
                        return 0
                return 0
            
            doc_list = sorted(doc_list, key=extract_clause_number)
            base_meta = {k: v for k, v in doc_list[0].metadata.items() 
                         if k not in ["links", "ì•½ì–´"]}

            # 3ï¸âƒ£ ì¥ë³„ë¡œ ê·¸ë£¹í™”
            chapters = []
            current_chapter = []
            current_header = ""
            
            for doc in doc_list:
                content = doc.page_content
                
                # ì¥ í—¤ë” ì¶”ì¶œ: [ì œ1ì¥ ì´ì¹™] í˜•íƒœ
                header = ""
                if content.startswith("[") and "]" in content:
                    # ì²« ë²ˆì§¸ ']'ê¹Œì§€ê°€ ì¥ í—¤ë”
                    end_idx = content.find("]")
                    potential_header = content[:end_idx+1]
                    # "ì œ1ì¥", "ì œ2ì¥" ê°™ì€ íŒ¨í„´ì´ ìˆìœ¼ë©´ ì¥ í—¤ë”ë¡œ ì¸ì‹
                    if "ì¥" in potential_header or "í¸" in potential_header:
                        header = potential_header
                
                # í—¤ë”ê°€ ë°”ë€Œë©´ ìƒˆ ì¥ ì‹œì‘
                if header and header != current_header:
                    if current_chapter:
                        chapters.append({
                            "header": current_header,
                            "docs": current_chapter
                        })
                    current_header = header
                    current_chapter = [doc]
                else:
                    current_chapter.append(doc)
            
            # ë§ˆì§€ë§‰ ì¥ ì¶”ê°€
            if current_chapter:
                chapters.append({
                    "header": current_header,
                    "docs": current_chapter
                })

            # 4ï¸âƒ£ ê° ì¥ë³„ë¡œ ì²­í¬ ìƒì„±
            for chapter_idx, chapter in enumerate(chapters, start=1):
                ì§€ìì²´ê¸°ê´€ëª… = base_meta.get("ì§€ìì²´ê¸°ê´€ëª…", "")
                ìì¹˜ë²•ê·œëª… = base_meta.get("ìì¹˜ë²•ê·œëª…", "")
                
                # ì¥ í—¤ë”ê°€ ìˆìœ¼ë©´ í¬í•¨
                if chapter["header"]:
                    base_header = f"[{ì§€ìì²´ê¸°ê´€ëª…} | {ìì¹˜ë²•ê·œëª…}]\n{chapter['header']}"
                else:
                    base_header = f"[{ì§€ìì²´ê¸°ê´€ëª…} | {ìì¹˜ë²•ê·œëª…}]"
                
                buffer = base_header
                merged_links_dict = defaultdict(set)
                merged_abb = {}
                chunk_id = 1
                
                for doc in chapter["docs"]:
                    content = doc.page_content
                    
                    # ì¥ í—¤ë” ì œê±° (ì¡°ë¬¸ ë‚´ìš©ë§Œ ì¶”ì¶œ)
                    text = content
                    if content.startswith("[") and "]" in content:
                        # [ì œ1ì¥ ì´ì¹™] ì œ1ì¡°... í˜•íƒœì—ì„œ ì œ1ì¡°... ë¶€ë¶„ë§Œ ì¶”ì¶œ
                        end_idx = content.find("]")
                        potential_header = content[:end_idx+1]
                        # ì¥ í—¤ë”ì¸ ê²½ìš°ì—ë§Œ ì œê±°
                        if "ì¥" in potential_header or "í¸" in potential_header:
                            text = content[end_idx+1:].strip()
                    
                    # ë§í¬ ë³‘í•©
                    links_list = doc.metadata.get("links", [])
                    for links in links_list:
                        for key, values in links.items():
                            if isinstance(values, list):
                                merged_links_dict[key].update(values)
                            else:
                                merged_links_dict[key].add(values)
                    
                    # ì•½ì–´ ë³‘í•©
                    abb = doc.metadata.get("ì•½ì–´", {})
                    merged_abb.update(abb)
                    
                    # ë²„í¼ì— ì¶”ê°€í• ì§€ ê²°ì •
                    potential_length = len(buffer) + len(text) + 1
                    
                    if potential_length <= max_len:
                        buffer += f"\n{text}"
                    else:
                        # í˜„ì¬ ë²„í¼ê°€ min_len ë¯¸ë§Œì´ë©´ ê°•ì œë¡œ ì¶”ê°€
                        if len(buffer) < min_len:
                            buffer += f"\n{text}"
                        else:
                            # í˜„ì¬ ë²„í¼ë¡œ Document ìƒì„±
                            merged_doc = create_merged_document(
                                law_id=law_id,
                                chapter_idx=chapter_idx,
                                chunk_id=chunk_id,
                                content=buffer.strip(),
                                base_meta=base_meta,
                                links_dict=merged_links_dict,
                                abb=merged_abb
                            )
                            merged_docs.append(merged_doc)
                            
                            # ìƒˆ ë²„í¼ ì‹œì‘
                            chunk_id += 1
                            buffer = base_header + f"\n{text}"
                            merged_links_dict = defaultdict(set)
                            merged_abb = {}
                            
                            # í˜„ì¬ ì¡°í•­ì˜ ë§í¬ì™€ ì•½ì–´ ì¶”ê°€
                            for links in links_list:
                                for key, values in links.items():
                                    if isinstance(values, list):
                                        merged_links_dict[key].update(values)
                                    else:
                                        merged_links_dict[key].add(values)
                            merged_abb.update(abb)
                
                # ë§ˆì§€ë§‰ ë²„í¼ ì²˜ë¦¬
                if len(buffer.strip()) > len(base_header.strip()):
                    merged_doc = create_merged_document(
                        law_id=law_id,
                        chapter_idx=chapter_idx,
                        chunk_id=chunk_id,
                        content=buffer.strip(),
                        base_meta=base_meta,
                        links_dict=merged_links_dict,
                        abb=merged_abb
                    )
                    merged_docs.append(merged_doc)
            
            # ì²˜ë¦¬ ì™„ë£Œëœ ë²•ê·œ ID ê¸°ë¡
            processed_law_ids.add(str(law_id))
            with open(merge_checkpoint_file, "a", encoding="utf-8") as f:
                f.write(f"{str(law_id)}\n")
            
            processed_count += 1
            
            # batch_sizeê°œ ë²•ê·œë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
            if processed_count % batch_size == 0:
                save_documents(merged_docs, "documents_merged.jsonl")
                # print(f"\nğŸ’¾ ì¤‘ê°„ ì €ì¥: {len(merged_docs)}ê°œ ë³‘í•© ë¬¸ì„œ, {processed_count}/{total_laws}ê°œ ë²•ê·œ ì²˜ë¦¬ ì™„ë£Œ")
        
        except Exception as e:
            print(f"\nâŒ ë³‘í•© ì˜¤ë¥˜ (ë²•ê·œ ID: {law_id}): {str(e)}")
            import traceback
            print(traceback.format_exc())
            # ì˜¤ë¥˜ ë¡œê·¸ ì €ì¥
            with open(os.path.join(checkpoint_dir, "merge_error_log.txt"), "a", encoding="utf-8") as f:
                f.write(f"{law_id}: {str(e)}\n{traceback.format_exc()}\n")
            continue

    # ìµœì¢… ì €ì¥
    save_documents(merged_docs, "documents_merged.jsonl")
    print(f"\nâœ… ìµœì¢… ë³‘í•© ì €ì¥: {len(merged_docs)}ê°œ ë¬¸ì„œ ì™„ë£Œ")
    
    return merged_docs

def create_merged_document(law_id, chapter_idx, chunk_id, content, base_meta, links_dict, abb):
    """
    ë³‘í•©ëœ Document ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    metadata = base_meta.copy()
    
    # links ì •ë¦¬ (setì„ listë¡œ ë³€í™˜)
    if links_dict:
        metadata["links"] = [{k: sorted(list(v)) for k, v in links_dict.items()}]
    
    # ì•½ì–´ ì¶”ê°€
    if abb:
        metadata["ì•½ì–´"] = abb
    
    doc = Document(
        page_content=content,
        metadata=metadata,
        id=f"{law_id}_chapter{chapter_idx}_chunk{chunk_id}"
    )
    
    return doc

def save_documents(documents, file_path):
    """
    Document ë¦¬ìŠ¤íŠ¸ë¥¼ JSONL íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    with open(file_path, "w", encoding="utf-8") as f:
        for doc in documents:
            json.dump({
                "id": doc.id,
                "page_content": doc.page_content,
                "metadata": doc.metadata
            }, f, ensure_ascii=False)
            f.write("\n")

def load_documents_from_jsonl(file_path):
    """
    JSONL íŒŒì¼ì—ì„œ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    """
    documents = []
    
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
                
            data = json.loads(line)
            doc = Document(
                page_content=data["page_content"],
                metadata=data["metadata"],
                id=data["id"]
            )
            documents.append(doc)
    
    return documents

# ====== Prompt ==================================================================
# ================================================================================
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

model = ChatOllama(
    model="gpt-oss:120b-cloud",
    temperature=0.1,
    max_tokens = 1024,
    timeout=None,
    max_retries=2,
    reasoning = None,
)

system_message = """ë‹¹ì‹ ì€ ë²•ë¥  ë¬¸ì„œ ì „ë¬¸ì„ ë¶„ì„í•˜ì—¬, ê·¸ ì•ˆì—ì„œ ëª…ì‹œì ìœ¼ë¡œ **ì°¸ê³ í•´ì•¼ í•˜ëŠ” ë‹¤ë¥¸ ë²•ë¥  ë¬¸ì„œì™€ í•´ë‹¹ ì¡°í•­ ë²ˆí˜¸**ë¥¼ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.

ì•„ë˜ì˜ ì›ì¹™ì„ ë°˜ë“œì‹œ ì§€í‚¤ì„¸ìš”:
1. ë°˜ë“œì‹œ ë¬¸ì„œ ë‚´ìš©ì— ëª…ì‹œì ìœ¼ë¡œ ë“±ì¥í•œ ë²•ë¥ ëª… ë˜ëŠ” ì¡°í•­ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
2. ë²•ë¥ ëª…ì´ ëª…í™•íˆ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ê²½ìš°, ì ˆëŒ€ ì¶”ì¸¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
3. ì¡°í•­ ë²ˆí˜¸ë‚˜ ë³„í‘œë§Œ ì–¸ê¸‰ëœ ê²½ìš°, í•´ë‹¹ í•­ëª©ì€ "ê¸°íƒ€" keyì— ë„£ìŠµë‹ˆë‹¤.
4. ì¶œë ¥ì€ ë°˜ë“œì‹œ **ìœ íš¨í•œ JSON í˜•ì‹**ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤ (íŒŒì‹± ê°€ëŠ¥í•œ êµ¬ì¡°).

ë‹¤ìŒì€ ì•½ì–´ì™€ ì‹¤ì œ ëª…ì¹­ì…ë‹ˆë‹¤. ë²•ë¥  ë¬¸ì„œ ì œëª©ì´ë‚˜ ì¡°í•­ëª…ì´ ì•½ì–´ë¡œ ëª…ì‹œë˜ì–´ìˆëŠ” ê²½ìš°, ì‹¤ì œ ëª…ì¹­ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•˜ì„¸ìš”.
{abb}

ì¶œë ¥ í˜•ì‹:
  "ë²•ë¥  ë¬¸ì„œ ì œëª©": ["ê´€ë ¨ ì¡°í•­ ë²ˆí˜¸", "..."],
  "ê¸°íƒ€": ["ë³„í‘œ", "ì¡°í•­ ë²ˆí˜¸", "..."]

ì£¼ì˜:
- ê° ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆë‹¤ë©´ ë¹ˆ ë°°ì—´([])ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
- ì„¤ëª… ë¬¸êµ¬, í•´ì„, ìì—°ì–´ ë¬¸ì¥ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
- "ê°™ì€ ë²•" ì´ë‚˜ "ë™ë²•" ë“±ì˜ í‘œí˜„ì€ ì‹¤ì œ ë²•ë¥ ëª…ìœ¼ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.
"""

prompt = ChatPromptTemplate.from_messages([
    ("system", system_message),
    ("user", "{question}")
])

# ì¼ë°˜ Chain ìƒì„±
chain = prompt | model | JsonOutputParser()

model_abb = ChatOllama(
    model="gpt-oss:120b-cloud",
    temperature=0.1,
    max_tokens = 1024,
    timeout=None,
    max_retries=2,
    reasoning = None,
)

system_message_abb = """ë‹¹ì‹ ì€ ë²•ë¥  ë¬¸ì„œ ì „ë¬¸ì„ ë¶„ì„í•˜ì—¬, ê·¸ ì•ˆì—ì„œ ì•½ì–´(ë˜ëŠ” ì¶•ì•½ì–´)ì˜ **ì›ë˜ ì˜ë¯¸**ë¥¼ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.

ì•„ë˜ì˜ ì›ì¹™ì„ ë°˜ë“œì‹œ ì§€í‚¤ì„¸ìš”:
1. ë°˜ë“œì‹œ ë¬¸ì„œ ë‚´ìš©ì— **ëª…ì‹œì ìœ¼ë¡œ ë“±ì¥í•œ ì•½ì–´ ë˜ëŠ” ì¶•ì•½ì–´ë§Œ** ì¶”ì¶œí•©ë‹ˆë‹¤.
2. ì•½ì–´ê°€ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš°, ì ˆëŒ€ ì¶”ì¸¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
3. ì•½ì–´ê°€ ì •ì˜ëœ ë¬¸ì¥ì€ ë³´í†µ â€œ(ì´í•˜ â€˜~â€™ì´ë¼ í•œë‹¤)â€ ë˜ëŠ” â€œ(ì´í•˜ â€˜~â€™ë¼ í•œë‹¤)â€ í˜•íƒœë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.
4. ì•½ì–´ì˜ ì›ë˜ ì˜ë¯¸ì—ëŠ” ë‹¤ìŒ ìš”ì†Œë“¤ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
   - ê´€ë ¨ ë²•ë ¹, ì¡°í•­ ë²ˆí˜¸, ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™
   - â€œì— ë”°ë¼â€, â€œì— ì˜í•œâ€, â€œì´ ì •í•˜ì—¬ ê³ ì‹œí•˜ëŠ”â€, â€œìœ¼ë¡œ ì •í•˜ëŠ”â€, â€œì—ì„œ ê·œì •í•œâ€ ë“±ì˜ ì¡°ê±´ë¬¸
   - ë¬¸ì¥ ì† ìˆ˜ì‹ì–´, ì œì•½ ì¡°ê±´ ë“±
   ì´ëŸ¬í•œ ì¡°ê±´ì€ **ì ˆëŒ€ ìƒëµí•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ í¬í•¨í•˜ì„¸ìš”.**
5. ì •ì˜ êµ¬ë¬¸ì— "ë‹¤ë§Œ", "ë‹¨ì„œ", "ì˜ˆì™¸" ë“±ì´ ì´ì–´ì§€ëŠ” ê²½ìš°, í•´ë‹¹ ì¡°ê±´ë„ ì‹¤ì œ ëª…ì¹­ì— ë°˜ë“œì‹œ í¬í•¨í•©ë‹ˆë‹¤.
6. ì¶œë ¥ì€ ë°˜ë“œì‹œ **íŒŒì‹± ê°€ëŠ¥í•œ JSON í˜•ì‹**ìœ¼ë¡œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.

ì¶œë ¥ í˜•ì‹:
  "ì•½ì–´": "ì›ë˜ ì˜ë¯¸"

- ì˜ˆì‹œ 1:
ì…ë ¥:
"ì œ3ì¡°(ì •ì˜) ì´ ì¡°ë¡€ì—ì„œ ì‚¬ìš©í•˜ëŠ” ìš©ì–´ì˜ ëœ»ì€ ã€Œê±´ì¶•ë¬¼ê´€ë¦¬ë²•ã€(ì´í•˜ â€œë²•â€ì´ë¼ í•œë‹¤) ë° ã€Œê±´ì¶•ë²•ã€ì—ì„œ ì •í•˜ëŠ” ë°”ì— ë”°ë¥¸ë‹¤."

ì¶œë ¥:
  "ë²•": "ê±´ì¶•ë¬¼ê´€ë¦¬ë²•"

- ì˜ˆì‹œ 2:
ì…ë ¥:
"ë²• ì œ42ì¡°ì œ1í•­(ë‹¨ì„œ ë¶€ë¶„ì€ ì œì™¸í•œë‹¤)ì— ë”°ë¼ 200ì œê³±ë¯¸í„° ì´ìƒì¸ ëŒ€ì§€ì— ê±´ì¶•ì„ í•˜ëŠ” ê±´ì¶•ì£¼ëŠ” ë‹¤ìŒ ê° í˜¸ì˜ ì–´ëŠ í•˜ë‚˜ì— í•´ë‹¹í•˜ëŠ” ì¡°ê²½ë©´ì (ì´í•˜ â€œì¡°ê²½ì˜ë¬´ë©´ì â€ì´ë¼ í•œë‹¤)ì— ë²• ì œ42ì¡°ì œ2í•­ì— ë”°ë¼ êµ­í† êµí†µë¶€ì¥ê´€ì´ ê³ ì‹œí•˜ëŠ” ì¡°ê²½ê¸°ì¤€(ì´í•˜ â€œì¡°ê²½ê¸°ì¤€â€ì´ë¼ í•œë‹¤)ì— ë”°ë¼ ì¡°ê²½ì˜ ì¡°ì¹˜ë¥¼ í•˜ì—¬ì•¼ í•œë‹¤. ë‹¤ë§Œ, ë‹¤ìŒ ê° í˜¸ì˜ ê¸°ì¤€ë³´ë‹¤ ì˜ ì œ27ì¡°ì œ2í•­ ê° í˜¸ì˜ ê¸°ì¤€ì´ ë” ì™„í™”ëœ ê²½ìš°ì—ëŠ” ê·¸ ê¸°ì¤€ì— ë”°ë¥¸ë‹¤. <ê°œì • 2009.12.9., 2017.11.9.>1. ì—°ë©´ì (ëŒ€ì§€ì— ë‘˜ ì´ìƒì˜ ê±´ì¶•ë¬¼ì´ ìˆëŠ” ê²½ìš°ì—ëŠ” ì—°ë©´ì ì˜ í•©ê³„ë¡œ í•œë‹¤. ì´í•˜ ì´ ì¡°ì—ì„œ ê°™ë‹¤)ì´ 2ì²œì œê³±ë¯¸í„° ì´ìƒì¸ ê±´ì¶•ë¬¼ì˜ ì¡°ê²½ì˜ë¬´ë©´ì : ëŒ€ì§€ë©´ì ì˜ 15í¼ì„¼íŠ¸ ì´ìƒ2. ì—°ë©´ì ì´ 1ì²œì œê³±ë¯¸í„° ì´ìƒ 2ì²œì œê³±ë¯¸í„° ë¯¸ë§Œì¸ ê±´ì¶•ë¬¼ì˜ ì¡°ê²½ì˜ë¬´ë©´ì  : ëŒ€ì§€ë©´ì ì˜ 10í¼ì„¼íŠ¸ ì´ìƒ3. ì—°ë©´ì ì´ 1ì²œì œê³±ë¯¸í„° ë¯¸ë§Œì¸ ê±´ì¶•ë¬¼ì˜ ì¡°ê²½ì˜ë¬´ë©´ì  : ëŒ€ì§€ë©´ì ì˜ 5í¼ì„¼íŠ¸ ì´ìƒâ‘¡ ì¡°ê²½ê¸°ì¤€ ì œ4ì¡°ì™€ ì œ5ì¡°ì œ1í•­ ë° ì œ12ì¡°ì—ì„œ ì •í•˜ëŠ” ì¡°ê²½ë©´ì  ì‚°ì •ê¸°ì¤€ì´ë‚˜ ì¡°ê²½ë©´ì ì˜ ë°°ì¹˜ê¸°ì¤€ì— ë”°ë¼ ì‚°ì •í•œ ë©´ì ì„ ì œ1í•­ì— ë”°ë¥¸ ì¡°ê²½ì˜ë¬´ë©´ì ìœ¼ë¡œ ì‚°ì •í•œë‹¤. ë‹¤ë§Œ, ë‹¤ìŒ ê° í˜¸ì—ì„œ ì •í•˜ëŠ” ê²½ìš°ì—ëŠ” ê·¸ ê¸°ì¤€ì— ë”°ë¼ ì‚°ì •í•œ ë©´ì ë§Œì„ ì¡°ê²½ì˜ë¬´ë©´ì ìœ¼ë¡œ ì‚°ì •í•˜ë˜ ì¡°ê²½ì˜ë¬´ë©´ì ì˜ 2ë¶„ì˜ 1ì„ ì´ˆê³¼í•  ìˆ˜ ì—†ë‹¤."

ì¶œë ¥:
  "ì¡°ê²½ì˜ë¬´ë©´ì ": "ë²• ì œ42ì¡°ì œ1í•­(ë‹¨ì„œ ë¶€ë¶„ì€ ì œì™¸í•œë‹¤)ì— ë”°ë¼ 200ì œê³±ë¯¸í„° ì´ìƒì¸ ëŒ€ì§€ì— ê±´ì¶•ì„ í•˜ëŠ” ê±´ì¶•ì£¼ëŠ” ë‹¤ìŒ ê° í˜¸ì˜ ì–´ëŠ í•˜ë‚˜ì— í•´ë‹¹í•˜ëŠ” ì¡°ê²½ë©´ì ì— ë²• ì œ42ì¡°ì œ2í•­ì— ë”°ë¼ êµ­í† êµí†µë¶€ì¥ê´€ì´ ê³ ì‹œí•˜ëŠ” ì¡°ê²½ê¸°ì¤€ì— ë”°ë¼ ì¡°ê²½ì˜ ì¡°ì¹˜ë¥¼ í•˜ì—¬ì•¼ í•˜ë‚˜, ë‹¤ë§Œ ë‹¤ìŒ ê° í˜¸ì˜ ê¸°ì¤€ë³´ë‹¤ ì˜ ì œ27ì¡°ì œ2í•­ ê° í˜¸ì˜ ê¸°ì¤€ì´ ë” ì™„í™”ëœ ê²½ìš°ì—ëŠ” ê·¸ ê¸°ì¤€ì— ë”°ë¥¸ë‹¤.",
  "ì¡°ê²½ê¸°ì¤€": "ë²• ì œ42ì¡°ì œ2í•­ì— ë”°ë¼ êµ­í† êµí†µë¶€ì¥ê´€ì´ ê³ ì‹œí•˜ëŠ” ì¡°ê²½ê¸°ì¤€"

ì¶œë ¥ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”):
- ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•©ë‹ˆë‹¤. ì½”ë“œë¸”ë¡(````), ì£¼ì„(`//`, `/* */`), ì„¤ëª…ë¬¸, ê¸°íƒ€ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
- ì•½ì–´ê°€ ì „í˜€ ì—†ì„ ê²½ìš°ì—ëŠ” ì •í™•íˆ ë¹ˆ JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
 """

prompt_abb = ChatPromptTemplate.from_messages([
    ("system", system_message_abb),
    ("user", "{question}")
])

# ì¼ë°˜ Chain ìƒì„±
chain_abb = prompt_abb | model_abb | JsonOutputParser()


# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    # 1ë‹¨ê³„: ì¡°í•­ë³„ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    print("=" * 50)
    print("1ë‹¨ê³„: ì¡°í•­ë³„ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘...")
    print("=" * 50)
    clause_documents = extract_clause_documents(df, oc, chain_abb, chain)
    print(f"\nâœ… ì¡°í•­ë³„ ë¬¸ì„œ {len(clause_documents)}ê°œ ì™„ë£Œ")

    # 2ë‹¨ê³„: ì¥ë³„ ë³‘í•©
    print("\n" + "=" * 50)
    print("2ë‹¨ê³„: ì¥ë³„ ë³‘í•© ì¤‘...")
    print("=" * 50)
    merged_documents = merge_documents_by_chapter(
        clause_documents, 
        min_len=300, 
        max_len=600,
        batch_size=100  # 100ê°œ ë²•ê·œë§ˆë‹¤ ì €ì¥
    )
    print(f"\nâœ… ë³‘í•© ë¬¸ì„œ {len(merged_documents)}ê°œ ì™„ë£Œ")

    print("\n" + "=" * 50)
    print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print(f"ì¡°í•­ë³„ ë¬¸ì„œ: {len(clause_documents)}ê°œ â†’ documents.jsonl")
    print(f"ë³‘í•© ë¬¸ì„œ: {len(merged_documents)}ê°œ â†’ documents_merged.jsonl")
    print("=" * 50)