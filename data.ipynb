{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "oc = os.getenv('OC', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ìˆœë²ˆ</th>\n",
       "      <th>ë²•ë ¹MST</th>\n",
       "      <th>ì†Œê´€ë¶€ì²˜ì½”ë“œ</th>\n",
       "      <th>ì†Œê´€ë¶€ì²˜ëª…</th>\n",
       "      <th>ë²•ë ¹ID</th>\n",
       "      <th>ë²•ë ¹ëª…</th>\n",
       "      <th>ê³µí¬ì¼ì</th>\n",
       "      <th>ê³µí¬ë²ˆí˜¸</th>\n",
       "      <th>ì‹œí–‰ì¼ì</th>\n",
       "      <th>ë²•ë ¹êµ¬ë¶„ì½”ë“œ</th>\n",
       "      <th>ë²•ë ¹êµ¬ë¶„ëª…</th>\n",
       "      <th>ë²•ë ¹ë¶„ì•¼ì½”ë“œ</th>\n",
       "      <th>ë²•ë ¹ë¶„ì•¼ëª…</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>276923</td>\n",
       "      <td>1613000</td>\n",
       "      <td>êµ­í† êµí†µë¶€</td>\n",
       "      <td>10594</td>\n",
       "      <td>ê±´ì¶•ê¸°ë³¸ë²•</td>\n",
       "      <td>20251001</td>\n",
       "      <td>ì œ21065í˜¸</td>\n",
       "      <td>20251001</td>\n",
       "      <td>A0002</td>\n",
       "      <td>ë²•ë¥ </td>\n",
       "      <td>34030000</td>\n",
       "      <td>ç¬¬3ç«  å»º ç¯‰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>277915</td>\n",
       "      <td>1613000</td>\n",
       "      <td>êµ­í† êµí†µë¶€</td>\n",
       "      <td>10787</td>\n",
       "      <td>ê±´ì¶•ê¸°ë³¸ë²• ì‹œí–‰ë ¹</td>\n",
       "      <td>20251001</td>\n",
       "      <td>ì œ35811í˜¸</td>\n",
       "      <td>20251001</td>\n",
       "      <td>A0007</td>\n",
       "      <td>ëŒ€í†µë ¹ë ¹</td>\n",
       "      <td>34030000</td>\n",
       "      <td>ç¬¬3ç«  å»º ç¯‰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>266691</td>\n",
       "      <td>1613000</td>\n",
       "      <td>êµ­í† êµí†µë¶€</td>\n",
       "      <td>13478</td>\n",
       "      <td>ê±´ì¶•ë¬¼ê´€ë¦¬ë²•</td>\n",
       "      <td>20241203</td>\n",
       "      <td>ì œ20549í˜¸</td>\n",
       "      <td>20250604</td>\n",
       "      <td>A0002</td>\n",
       "      <td>ë²•ë¥ </td>\n",
       "      <td>34030000</td>\n",
       "      <td>ç¬¬3ç«  å»º ç¯‰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>271337</td>\n",
       "      <td>1613000</td>\n",
       "      <td>êµ­í† êµí†µë¶€</td>\n",
       "      <td>13742</td>\n",
       "      <td>ê±´ì¶•ë¬¼ê´€ë¦¬ë²• ì‹œí–‰ë ¹</td>\n",
       "      <td>20250527</td>\n",
       "      <td>ì œ35549í˜¸</td>\n",
       "      <td>20250604</td>\n",
       "      <td>A0007</td>\n",
       "      <td>ëŒ€í†µë ¹ë ¹</td>\n",
       "      <td>34030000</td>\n",
       "      <td>ç¬¬3ç«  å»º ç¯‰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>271531</td>\n",
       "      <td>1613000</td>\n",
       "      <td>êµ­í† êµí†µë¶€</td>\n",
       "      <td>13747</td>\n",
       "      <td>ê±´ì¶•ë¬¼ê´€ë¦¬ë²• ì‹œí–‰ê·œì¹™</td>\n",
       "      <td>20250602</td>\n",
       "      <td>ì œ1495í˜¸</td>\n",
       "      <td>20250602</td>\n",
       "      <td>A0103</td>\n",
       "      <td>êµ­í† êµí†µë¶€ë ¹</td>\n",
       "      <td>34030000</td>\n",
       "      <td>ç¬¬3ç«  å»º ç¯‰</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ìˆœë²ˆ   ë²•ë ¹MST   ì†Œê´€ë¶€ì²˜ì½”ë“œ  ì†Œê´€ë¶€ì²˜ëª…   ë²•ë ¹ID          ë²•ë ¹ëª…      ê³µí¬ì¼ì     ê³µí¬ë²ˆí˜¸  \\\n",
       "0   1  276923  1613000  êµ­í† êµí†µë¶€  10594        ê±´ì¶•ê¸°ë³¸ë²•  20251001  ì œ21065í˜¸   \n",
       "1   2  277915  1613000  êµ­í† êµí†µë¶€  10787    ê±´ì¶•ê¸°ë³¸ë²• ì‹œí–‰ë ¹  20251001  ì œ35811í˜¸   \n",
       "2   3  266691  1613000  êµ­í† êµí†µë¶€  13478       ê±´ì¶•ë¬¼ê´€ë¦¬ë²•  20241203  ì œ20549í˜¸   \n",
       "3   4  271337  1613000  êµ­í† êµí†µë¶€  13742   ê±´ì¶•ë¬¼ê´€ë¦¬ë²• ì‹œí–‰ë ¹  20250527  ì œ35549í˜¸   \n",
       "4   5  271531  1613000  êµ­í† êµí†µë¶€  13747  ê±´ì¶•ë¬¼ê´€ë¦¬ë²• ì‹œí–‰ê·œì¹™  20250602   ì œ1495í˜¸   \n",
       "\n",
       "       ì‹œí–‰ì¼ì ë²•ë ¹êµ¬ë¶„ì½”ë“œ   ë²•ë ¹êµ¬ë¶„ëª…    ë²•ë ¹ë¶„ì•¼ì½”ë“œ    ë²•ë ¹ë¶„ì•¼ëª…  \n",
       "0  20251001  A0002      ë²•ë¥   34030000  ç¬¬3ç«  å»º ç¯‰  \n",
       "1  20251001  A0007    ëŒ€í†µë ¹ë ¹  34030000  ç¬¬3ç«  å»º ç¯‰  \n",
       "2  20250604  A0002      ë²•ë¥   34030000  ç¬¬3ç«  å»º ç¯‰  \n",
       "3  20250604  A0007    ëŒ€í†µë ¹ë ¹  34030000  ç¬¬3ç«  å»º ç¯‰  \n",
       "4  20250602  A0103  êµ­í† êµí†µë¶€ë ¹  34030000  ç¬¬3ç«  å»º ç¯‰  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ë²•ë ¹ê²€ìƒ‰ëª©ë¡_ë²•ë ¹_ê±´ì¶•.csv\", skiprows=1)\n",
    "print(len(df))\n",
    "dê±´ì¶•ë²• ì‹œí–‰ë ¹ê±´ì¶•ë²• ì‹œí–‰ë ¹ê±´ì¶•ë²• ì‹œfrom architecture_agent.ingestion.fetch_law import fetch_and_save_lawsfrom architecture_agent.ingestion.fetch_law import fetch_and_save_lawsfrom architecture_agent.ingestion.fetch_law import fetch_and_save_lawsfrom architecture_agent.ingestion.fetch_law import fetch_and_save_lawsfrom architecture_agent.ingestion.fetch_law import fetch_and_save_lafrom architecture_agent.ingestion.fetch_law import fetch_and_save_lawsfrom architecture_agent.ingestion.fetch_law import fetch_and_save_lawsfrom architecture_agent.ingestion.fetch_law import fetch_and_save_lawsfrom architecture_agent.ingestion.fetch_law import fetch_and_save_lawsfrom architecture_agent.ingestion.fetch_law import fetch_and_save_law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = \"001823\"\n",
    "url = f\"http://www.law.go.kr/DRF/lawService.do?OC={oc}&target=eflaw&ID=001823&type=JSON\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¡°ë¬¸ íŒŒì‹±\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ArticleChunk:\n",
    "    \"\"\"ì¡° ë‹¨ìœ„ chunk\"\"\"\n",
    "    # ì‹ë³„\n",
    "    law_name: str                    # \"ê±´ì¶•ë²•\"\n",
    "    law_id: str                      # \"001823\"\n",
    "    article_num: str                 # \"46\"\n",
    "    article_title: str               # \"ê±´ì¶•ì„ ì˜ ì§€ì •\"\n",
    "    \n",
    "    # ë³¸ë¬¸\n",
    "    content: str                     # í•­+í˜¸+ëª© í”Œë« ê²°í•© í…ìŠ¤íŠ¸\n",
    "    content_resolved: str = \"\"       # ì¶•ì•½ì–´ ì¹˜í™˜ ë²„ì „ (STEP 1-5ì—ì„œ ì±„ì›€)\n",
    "    \n",
    "    # ê³„ì¸µ êµ¬ì¡° ë³´ì¡´ (metadataìš©)\n",
    "    paragraphs: list = field(default_factory=list)\n",
    "    # [\n",
    "    #   {\"num\": \"1\", \"content\": \"...\", \n",
    "    #    \"subs\": [{\"num\": \"1\", \"content\": \"...\", \n",
    "    #              \"items\": [{\"num\": \"ê°€\", \"content\": \"...\"}]}]}\n",
    "    # ]\n",
    "    \n",
    "    # ì°¸ì¡° (STEP 1-4ì—ì„œ ì±„ì›€)\n",
    "    internal_refs: list = field(default_factory=list)\n",
    "    external_refs: list = field(default_factory=list)\n",
    "    parent_law_refs: list = field(default_factory=list)\n",
    "    \n",
    "    # ì¶•ì•½ì–´ë§µ (STEP 1-3ì—ì„œ ì±„ì›€)\n",
    "    abbreviations: dict = field(default_factory=dict)\n",
    "    \n",
    "    # ë©”íƒ€\n",
    "    effective_date: str = \"\"\n",
    "    change_type: str = \"\"\n",
    "\n",
    "\n",
    "def parse_law_data(data: dict) -> list[ArticleChunk]:\n",
    "    \"\"\"\n",
    "    ë²•ë ¹ API ì‘ë‹µ ì „ì²´ë¥¼ íŒŒì‹±í•˜ì—¬ ArticleChunk ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    law_info = data[\"ë²•ë ¹\"][\"ê¸°ë³¸ì •ë³´\"]\n",
    "    law_name = law_info[\"ë²•ë ¹ëª…_í•œê¸€\"]\n",
    "    law_id = law_info[\"ë²•ë ¹ID\"]\n",
    "    \n",
    "    articles_raw = data[\"ë²•ë ¹\"][\"ì¡°ë¬¸\"][\"ì¡°ë¬¸ë‹¨ìœ„\"]\n",
    "    \n",
    "    chunks = []\n",
    "    for article in articles_raw:\n",
    "        chunk = parse_article(article, law_name, law_id)\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def parse_article(article: dict, law_name: str, law_id: str) -> ArticleChunk | None:\n",
    "    \"\"\"ë‹¨ì¼ ì¡°ë¬¸ì„ ArticleChunkë¡œ ë³€í™˜\"\"\"\n",
    "    \n",
    "    # ì¡°ë¬¸ ì—¬ë¶€ í™•ì¸ (ë¶€ì¹™ ë“± ì œì™¸)\n",
    "    if article.get(\"ì¡°ë¬¸ì—¬ë¶€\") != \"ì¡°ë¬¸\":\n",
    "        return None\n",
    "    \n",
    "    article_num = article.get(\"ì¡°ë¬¸ë²ˆí˜¸\", \"\")\n",
    "    article_title_raw = article.get(\"ì¡°ë¬¸ì œëª©\", \"\")\n",
    "    article_header = article.get(\"ì¡°ë¬¸ë‚´ìš©\", \"\")  # \"ì œ2ì¡°(ì •ì˜)\"\n",
    "    \n",
    "    # í•­ íŒŒì‹±\n",
    "    paragraphs_structured = []\n",
    "    content_parts = [article_header]  # í”Œë« í…ìŠ¤íŠ¸ ì‹œì‘\n",
    "    \n",
    "    paragraphs_raw = normalize_to_list(article.get('í•­'))\n",
    "\n",
    "    for para in paragraphs_raw:\n",
    "        para_num = normalize_paragraph_num(para.get(\"í•­ë²ˆí˜¸\", \"\"))\n",
    "        para_content = para.get(\"í•­ë‚´ìš©\", \"\")\n",
    "        \n",
    "        content_parts.append(para_content)\n",
    "        \n",
    "        subs_structured = []\n",
    "        \n",
    "        # í˜¸ íŒŒì‹±\n",
    "        for sub in para.get(\"í˜¸\", []):\n",
    "            sub_num = sub.get(\"í˜¸ë²ˆí˜¸\", \"\").strip().rstrip(\".\")\n",
    "            sub_content = sub.get(\"í˜¸ë‚´ìš©\", \"\")\n",
    "            \n",
    "            content_parts.append(sub_content)\n",
    "            \n",
    "            items_structured = []\n",
    "            \n",
    "            # ëª© íŒŒì‹±\n",
    "            for item in sub.get(\"ëª©\", []):\n",
    "                item_num = item.get(\"ëª©ë²ˆí˜¸\", \"\").strip().rstrip(\".\")\n",
    "                item_content = item.get(\"ëª©ë‚´ìš©\", \"\")\n",
    "                \n",
    "                content_parts.append(item_content)\n",
    "                items_structured.append({\n",
    "                    \"num\": item_num,\n",
    "                    \"content\": item_content\n",
    "                })\n",
    "            \n",
    "            subs_structured.append({\n",
    "                \"num\": sub_num,\n",
    "                \"content\": sub_content,\n",
    "                \"items\": items_structured\n",
    "            })\n",
    "        \n",
    "        paragraphs_structured.append({\n",
    "            \"num\": para_num,\n",
    "            \"content\": para_content,\n",
    "            \"subs\": subs_structured\n",
    "        })\n",
    "    \n",
    "    # í”Œë« í…ìŠ¤íŠ¸ ê²°í•©\n",
    "    content = \"\\n\".join(content_parts)\n",
    "    \n",
    "    return ArticleChunk(\n",
    "        law_name=law_name,\n",
    "        law_id=law_id,\n",
    "        article_num=article_num,\n",
    "        article_title=article_title_raw,\n",
    "        content=content,\n",
    "        paragraphs=paragraphs_structured,\n",
    "        effective_date=article.get(\"ì¡°ë¬¸ì‹œí–‰ì¼ì\", \"\"),\n",
    "        change_type=article.get(\"ì¡°ë¬¸ì œê°œì •ìœ í˜•\", \"\"),\n",
    "    )\n",
    "\n",
    "def normalize_to_list(value):\n",
    "    \"\"\"í•­/í˜¸/ëª©ì´ dictë¡œ ì˜¬ ìˆ˜ë„, listë¡œ ì˜¬ ìˆ˜ë„, ì—†ì„ ìˆ˜ë„ ìˆëŠ” ê²½ìš° ì²˜ë¦¬\"\"\"\n",
    "    if value is None:\n",
    "        return []\n",
    "    if isinstance(value, dict):\n",
    "        return [value]\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    return []\n",
    "\n",
    "\n",
    "def normalize_paragraph_num(raw: str) -> str:\n",
    "    \"\"\"â‘ â‘¡â‘¢... â†’ 1, 2, 3...\"\"\"\n",
    "    circled_map = {\n",
    "        \"â‘ \": \"1\", \"â‘¡\": \"2\", \"â‘¢\": \"3\", \"â‘£\": \"4\", \"â‘¤\": \"5\",\n",
    "        \"â‘¥\": \"6\", \"â‘¦\": \"7\", \"â‘§\": \"8\", \"â‘¨\": \"9\", \"â‘©\": \"10\",\n",
    "        \"â‘ª\": \"11\", \"â‘«\": \"12\", \"â‘¬\": \"13\", \"â‘­\": \"14\", \"â‘®\": \"15\",\n",
    "    }\n",
    "    raw = raw.strip()\n",
    "    return circled_map.get(raw, raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶•ì•½ì–´ ë§µ\n",
    "def extract_abbreviations(chunks: list[ArticleChunk]) -> dict:\n",
    "    \"\"\"\n",
    "    ì „ì²´ ì¡°ë¬¸ì—ì„œ ì¶•ì•½ì–´ ë§µ ì¶”ì¶œ\n",
    "    \"ì´í•˜ \"ê±´ì¶•ì„ \"ì´ë¼ í•œë‹¤\" ê°™ì€ íŒ¨í„´\n",
    "    \n",
    "    Returns:\n",
    "        {\"ê±´ì¶•ì„ \": \"ê±´ì¶•ë¬¼ì„ ê±´ì¶•í•  ìˆ˜ ìˆëŠ” ì„ \", \"ì‹œÂ·ë„ì§€ì‚¬\": \"íŠ¹ë³„ì‹œì¥Â·ê´‘ì—­ì‹œì¥...\", ...}\n",
    "    \"\"\"\n",
    "    \n",
    "    # ì „ì²´ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸° (ë²•ë¥  ì „ì²´ë¥¼ í•œ ë²ˆ ìŠ¤ìº”)\n",
    "    full_text = \"\\n\".join(chunk.content for chunk in chunks)\n",
    "    \n",
    "    abbr_map = {}\n",
    "    \n",
    "    patterns = [\n",
    "        # [ì´í•˜ \"ê±´ì¶•ì„ (å»ºç¯‰ç·š)\"ì´ë¼ í•œë‹¤]\n",
    "        r'([^[ã€\\n]+?)\\[ì´í•˜\\s*[\"\\u201C]([^\"\\u201D]+)[\"\\u201D]\\s*(?:ì´ë¼|ë¼)\\s*í•œë‹¤\\]',\n",
    "        # (ì´í•˜ \"ì‹œÂ·ë„ì§€ì‚¬\"ë¼ í•œë‹¤)\n",
    "        r'([^(ã€\\n]+?)\\(ì´í•˜\\s*[\"\\u201C]([^\"\\u201D]+)[\"\\u201D]\\s*(?:ì´ë¼|ë¼)\\s*í•œë‹¤\\)',\n",
    "        # ì´í•˜ \"ê±´ì¶•ë¬¼ì˜ ê±´ì¶•ë“±\"ì´ë¼ í•œë‹¤\n",
    "        r'([^,\\n]+?)\\s*ì´í•˜\\s*[\"\\u201C]([^\"\\u201D]+)[\"\\u201D]\\s*(?:ì´ë¼|ë¼)\\s*í•œë‹¤',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        for match in re.finditer(pattern, full_text):\n",
    "            full_name = match.group(1).strip()\n",
    "            short_name = match.group(2).strip()\n",
    "            \n",
    "            # ë„ˆë¬´ ê¸´ full_nameì€ ì •ë¦¬ (ì•ë¶€ë¶„ ì˜ë¼ëƒ„)\n",
    "            # \"ë„ë¡œì™€ ì ‘í•œ ë¶€ë¶„ì— ê±´ì¶•ë¬¼ì„ ê±´ì¶•í•  ìˆ˜ ìˆëŠ” ì„ \" â†’ ë§ˆì§€ë§‰ ì˜ë¯¸ ë‹¨ìœ„ë§Œ\n",
    "            if len(full_name) > 50:\n",
    "                full_name = full_name[-50:]\n",
    "            \n",
    "            abbr_map[short_name] = full_name\n",
    "    \n",
    "    # ëª¨ë“  chunkì— ì¶•ì•½ì–´ë§µ ì²¨ë¶€\n",
    "    for chunk in chunks:\n",
    "        chunk.abbreviations = abbr_map\n",
    "    \n",
    "    return abbr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì°¸ì¡° ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_references(chunks: list[ArticleChunk], law_name: str):\n",
    "    \"\"\"\n",
    "    ê° ì¡°ë¬¸ì—ì„œ ì°¸ì¡° ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì—¬ chunkì— ì±„ì›€\n",
    "    \n",
    "    3ì¢…ë¥˜:\n",
    "    - internal_refs: ê°™ì€ ë²• ë‚´ ì°¸ì¡° (\"ì œ2ì¡°ì œ1í•­ì œ11í˜¸\")\n",
    "    - external_refs: ë‹¤ë¥¸ ë²• ì°¸ì¡° (\"ã€Œêµ­í† ì˜ ê³„íš ë° ì´ìš©ì— ê´€í•œ ë²•ë¥ ã€ ì œ76ì¡°\")\n",
    "    - parent_law_refs: ì‹œí–‰ë ¹â†’ë³¸ë²• ì°¸ì¡° (\"ë²• ì œ46ì¡°ì œ1í•­ì— ë”°ë¼\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # ë‹¤ë¥¸ ë²•ë¥  ì°¸ì¡°: ã€Œë²•ë¥ ëª…ã€ + ì„ íƒì ìœ¼ë¡œ ì œXì¡°\n",
    "    external_pattern = r'ã€Œ([^ã€]+)ã€(?:\\s*ì œ(\\d+(?:ì˜\\d+)?)ì¡°)?(?:\\s*ì œ(\\d+)í•­)?'\n",
    "    \n",
    "    # ê°™ì€ ë²• ë‚´ë¶€ ì°¸ì¡°: ì œXì¡°(ì œYí•­)(ì œZí˜¸)\n",
    "    internal_pattern = r'ì œ(\\d+(?:ì˜\\d+)?)ì¡°(?:ì œ(\\d+)í•­)?(?:ì œ(\\d+)í˜¸)?'\n",
    "    \n",
    "    # ì‹œí–‰ë ¹ì—ì„œ ë³¸ë²• ì°¸ì¡°: \"ë²• ì œXì¡°\"\n",
    "    parent_pattern = r'(?<![ê°€-í£])ë²•\\s+ì œ(\\d+(?:ì˜\\d+)?)ì¡°(?:ì œ(\\d+)í•­)?'\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        text = chunk.content\n",
    "        \n",
    "        # â”€â”€ external_refs â”€â”€\n",
    "        for m in re.finditer(external_pattern, text):\n",
    "            ref_law = m.group(1)\n",
    "            # ìê¸° ìì‹  ë²•ë¥  ì°¸ì¡°ëŠ” ì œì™¸\n",
    "            if ref_law == law_name:\n",
    "                continue\n",
    "            chunk.external_refs.append({\n",
    "                \"law_name\": ref_law,\n",
    "                \"article\": m.group(2),\n",
    "                \"paragraph\": m.group(3),\n",
    "                \"raw\": m.group(0),\n",
    "            })\n",
    "        \n",
    "        # â”€â”€ internal_refs â”€â”€\n",
    "        # ì™¸ë¶€ ì°¸ì¡°ë¡œ ì´ë¯¸ ì¡íŒ ë¶€ë¶„ ì œê±°í•œ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ\n",
    "        text_no_external = re.sub(external_pattern, \"\", text)\n",
    "        # parent_ref íŒ¨í„´ë„ ì œê±° (ì‹œí–‰ë ¹ì¸ ê²½ìš°)\n",
    "        text_clean = re.sub(parent_pattern, \"\", text_no_external)\n",
    "        \n",
    "        for m in re.finditer(internal_pattern, text_clean):\n",
    "            ref_article = m.group(1)\n",
    "            # ìê¸° ìì‹  ì¡°ë¬¸ ì°¸ì¡°ëŠ” ì œì™¸\n",
    "            if ref_article == chunk.article_num:\n",
    "                continue\n",
    "            chunk.internal_refs.append({\n",
    "                \"article\": ref_article,\n",
    "                \"paragraph\": m.group(2),\n",
    "                \"subparagraph\": m.group(3),\n",
    "                \"raw\": m.group(0),\n",
    "            })\n",
    "        \n",
    "        # â”€â”€ parent_law_refs (ì‹œí–‰ë ¹ ì „ìš©) â”€â”€\n",
    "        for m in re.finditer(parent_pattern, text):\n",
    "            chunk.parent_law_refs.append({\n",
    "                \"article\": m.group(1),\n",
    "                \"paragraph\": m.group(2),\n",
    "                \"raw\": m.group(0),\n",
    "            })\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±°\n",
    "        chunk.internal_refs = deduplicate_refs(chunk.internal_refs)\n",
    "        chunk.external_refs = deduplicate_refs(chunk.external_refs)\n",
    "        chunk.parent_law_refs = deduplicate_refs(chunk.parent_law_refs)\n",
    "\n",
    "\n",
    "def deduplicate_refs(refs: list[dict]) -> list[dict]:\n",
    "    \"\"\"raw ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°\"\"\"\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for ref in refs:\n",
    "        key = ref.get(\"raw\", str(ref))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            result.append(ref)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶•ì•½ì–´ ì¹˜í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_abbreviations(chunks: list[ArticleChunk], abbr_map: dict):\n",
    "    \"\"\"ì¶•ì•½ì–´ë¥¼ ì›ë˜ ëª…ì¹­ìœ¼ë¡œ ì¹˜í™˜í•œ content_resolved ìƒì„±\"\"\"\n",
    "    \n",
    "    # ê¸´ ì¶•ì•½ì–´ë¶€í„° ì¹˜í™˜ (ë¶€ë¶„ ë§¤ì¹­ ë°©ì§€)\n",
    "    sorted_abbrs = sorted(abbr_map.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        resolved = chunk.content\n",
    "        \n",
    "        for short, full in sorted_abbrs:\n",
    "            if len(short) <= 2:\n",
    "                # \"ë²•\", \"ì˜\" ê°™ì€ ì§§ì€ ì¶•ì•½ì–´: ë‹¨ì–´ ê²½ê³„ ì²´í¬\n",
    "                pattern = rf'(?<![ê°€-í£a-zA-Z]){re.escape(short)}(?=\\s|ì œ|ì˜|ì—|ì„|ë¥¼|ì´|ê°€|ì€|ëŠ”|ê³¼|ì™€|ìœ¼ë¡œ|$)'\n",
    "            else:\n",
    "                pattern = re.escape(short)\n",
    "            \n",
    "            resolved = re.sub(pattern, full, resolved)\n",
    "        \n",
    "        chunk.content_resolved = resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_naver import ChatClovaX, ClovaXEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "\n",
    "def store_to_qdrant(chunks: list[ArticleChunk], \n",
    "                     collection_name: str = \"building_law\"):\n",
    "    \"\"\"ArticleChunk ë¦¬ìŠ¤íŠ¸ë¥¼ Qdrantì— ì €ì¥\"\"\"\n",
    "    \n",
    "    # Qdrant í´ë¼ì´ì–¸íŠ¸\n",
    "    client = QdrantClient(path=\"./qdrant_data\")  # ë¡œì»¬ ì €ì¥\n",
    "    \n",
    "    # ì»¬ë ‰ì…˜ ìƒì„± (ì—†ìœ¼ë©´)\n",
    "    if not client.collection_exists(collection_name):\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(\n",
    "                size=1024, \n",
    "                distance=Distance.COSINE,\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    # ì„ë² ë”© ëª¨ë¸\n",
    "    embeddings = ClovaXEmbeddings(model=\"bge-m3\")\n",
    "    \n",
    "    # VectorStore ì´ˆê¸°í™”\n",
    "    vector_store = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "    \n",
    "    # Document ë³€í™˜\n",
    "    documents = []\n",
    "    ids = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        doc = Document(\n",
    "            page_content=chunk.content_resolved or chunk.content,\n",
    "            metadata={\n",
    "                # ì‹ë³„\n",
    "                \"law_name\": chunk.law_name,\n",
    "                \"law_id\": chunk.law_id,\n",
    "                \"article_num\": chunk.article_num,\n",
    "                \"article_title\": chunk.article_title,\n",
    "                \n",
    "                # ì›ë³¸ ë³´ì¡´\n",
    "                \"content_original\": chunk.content,\n",
    "                \n",
    "                # ê³„ì¸µ êµ¬ì¡°\n",
    "                \"paragraphs\": chunk.paragraphs,\n",
    "                \n",
    "                # ì°¸ì¡° ê´€ê³„\n",
    "                \"internal_refs\": chunk.internal_refs,\n",
    "                \"external_refs\": chunk.external_refs,\n",
    "                \"parent_law_refs\": chunk.parent_law_refs,\n",
    "                \n",
    "                # ì¶•ì•½ì–´ë§µ\n",
    "                \"abbreviations\": chunk.abbreviations,\n",
    "                \n",
    "                # ë©”íƒ€\n",
    "                \"effective_date\": chunk.effective_date,\n",
    "                \"change_type\": chunk.change_type,\n",
    "                \"law_type\": classify_law_type(chunk.law_name),\n",
    "            },\n",
    "        )\n",
    "        documents.append(doc)\n",
    "        ids.append(f\"{chunk.law_name}_{chunk.article_num}\")\n",
    "    \n",
    "    # ì €ì¥\n",
    "    vector_store.add_documents(documents=documents, ids=ids)\n",
    "    \n",
    "    print(f\"âœ… {len(documents)}ê°œ ì¡°ë¬¸ ì €ì¥ ì™„ë£Œ â†’ {collection_name}\")\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def classify_law_type(law_name: str) -> str:\n",
    "    if \"ì‹œí–‰ê·œì¹™\" in law_name:\n",
    "        return \"ì‹œí–‰ê·œì¹™\"\n",
    "    elif \"ì‹œí–‰ë ¹\" in law_name:\n",
    "        return \"ì‹œí–‰ë ¹\"\n",
    "    else:\n",
    "        return \"ë²•ë¥ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– ê±´ì¶•ë²• ì²˜ë¦¬ ì‹œì‘...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 3: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mğŸ“– \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlaw_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ì²˜ë¦¬ ì‹œì‘...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# STEP 1-2: ì¡°ë¬¸ íŒŒì‹±\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m chunks = \u001b[43mparse_law_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mparse_law_data\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m     50\u001b[39m chunks = []\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles_raw:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     chunk = \u001b[43mparse_article\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlaw_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlaw_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk:\n\u001b[32m     54\u001b[39m         chunks.append(chunk)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mparse_article\u001b[39m\u001b[34m(article, law_name, law_id)\u001b[39m\n\u001b[32m    110\u001b[39m     paragraphs_structured.append({\n\u001b[32m    111\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnum\u001b[39m\u001b[33m\"\u001b[39m: para_num,\n\u001b[32m    112\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: para_content,\n\u001b[32m    113\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msubs\u001b[39m\u001b[33m\"\u001b[39m: subs_structured\n\u001b[32m    114\u001b[39m     })\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# í”Œë« í…ìŠ¤íŠ¸ ê²°í•©\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m content = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(content_parts)\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ArticleChunk(\n\u001b[32m    120\u001b[39m     law_name=law_name,\n\u001b[32m    121\u001b[39m     law_id=law_id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    127\u001b[39m     change_type=article.get(\u001b[33m\"\u001b[39m\u001b[33mì¡°ë¬¸ì œê°œì •ìœ í˜•\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    128\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: sequence item 3: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "law_name = data[\"ë²•ë ¹\"][\"ê¸°ë³¸ì •ë³´\"][\"ë²•ë ¹ëª…_í•œê¸€\"]\n",
    "print(f\"ğŸ“– {law_name} ì²˜ë¦¬ ì‹œì‘...\")\n",
    "\n",
    "# STEP 1-2: ì¡°ë¬¸ íŒŒì‹±\n",
    "chunks = parse_law_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_law_db(data: dict, collection_name: str = \"building_law\"):\n",
    "    \"\"\"\n",
    "    ë²•ë ¹ API ì‘ë‹µ â†’ íŒŒì‹± â†’ ì „ì²˜ë¦¬ â†’ Qdrant ì €ì¥\n",
    "    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "    \"\"\"\n",
    "    \n",
    "    law_name = data[\"ë²•ë ¹\"][\"ê¸°ë³¸ì •ë³´\"][\"ë²•ë ¹ëª…_í•œê¸€\"]\n",
    "    print(f\"ğŸ“– {law_name} ì²˜ë¦¬ ì‹œì‘...\")\n",
    "    \n",
    "    # STEP 1-2: ì¡°ë¬¸ íŒŒì‹±\n",
    "    chunks = parse_law_data(data)\n",
    "    print(f\"   íŒŒì‹± ì™„ë£Œ: {len(chunks)}ê°œ ì¡°ë¬¸\")\n",
    "    \n",
    "    # STEP 1-3: ì¶•ì•½ì–´ë§µ ì¶”ì¶œ\n",
    "    abbr_map = extract_abbreviations(chunks)\n",
    "    print(f\"   ì¶•ì•½ì–´ ì¶”ì¶œ: {len(abbr_map)}ê°œ\")\n",
    "    for short, full in abbr_map.items():\n",
    "        print(f\"     '{short}' â†’ '{full}'\")\n",
    "    \n",
    "    # STEP 1-4: ì°¸ì¡° ì¶”ì¶œ\n",
    "    extract_references(chunks, law_name)\n",
    "    total_refs = sum(\n",
    "        len(c.internal_refs) + len(c.external_refs) + len(c.parent_law_refs) \n",
    "        for c in chunks\n",
    "    )\n",
    "    print(f\"   ì°¸ì¡° ì¶”ì¶œ: ì´ {total_refs}ê°œ\")\n",
    "    \n",
    "    # STEP 1-5: ì¶•ì•½ì–´ ì¹˜í™˜\n",
    "    resolve_abbreviations(chunks, abbr_map)\n",
    "    print(f\"   ì¶•ì•½ì–´ ì¹˜í™˜ ì™„ë£Œ\")\n",
    "    \n",
    "    # STEP 1-6: Qdrant ì €ì¥\n",
    "    vector_store = store_to_qdrant(chunks, collection_name)\n",
    "    print(f\"âœ… {law_name} ì™„ë£Œ!\\n\")\n",
    "    \n",
    "    return vector_store, chunks\n",
    "\n",
    "\n",
    "# ì‹¤í–‰\n",
    "# ê±´ì¶•ë²•\n",
    "vs1, chunks1 = build_law_db(building_law_data)\n",
    "\n",
    "# ê±´ì¶•ë²• ì‹œí–‰ë ¹ (ë³„ë„ API í˜¸ì¶œ í›„)\n",
    "# vs2, chunks2 = build_law_db(building_decree_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
